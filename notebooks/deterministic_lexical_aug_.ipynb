{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXHuSu1K0l0Z"
      },
      "outputs": [],
      "source": [
        "!pip -q install \"transformers==4.41.2\" \"accelerate==0.31.0\" \"datasets==2.19.0\"\n",
        "\n",
        "import os, json, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE = Path(\"/content/drive/MyDrive/cs-senti\")\n",
        "DATA = BASE / \"data\"\n",
        "LABELING = BASE / \"labeling\"\n",
        "MODELS = BASE / \"models\"\n",
        "\n",
        "# EESA (ÿ¨ÿßŸáÿ≤ÿ© ÿ®ÿ≥plits)\n",
        "FP_EESA_TR = DATA / \"eesa_train.jsonl\"\n",
        "FP_EESA_DE = DATA / \"eesa_dev.jsonl\"\n",
        "FP_EESA_TE = DATA / \"eesa_test.jsonl\"\n",
        "\n",
        "# AMG Ÿà MR (final labels ÿ®ÿπÿØ majority vote)\n",
        "# ŸÖŸÑÿßÿ≠ÿ∏ÿ©: ŸÑŸà ÿπŸÜÿØŸÉ AMG ÿ®ÿµŸäÿ∫ÿ© jsonl ÿßÿ≥ÿ™ÿÆÿØŸÖ .jsonlÿå ŸàŸÑŸà CSV ÿßÿ≥ÿ™ÿÆÿØŸÖ read_csv ÿ™ÿ≠ÿ™\n",
        "FP_AMG_FINAL = DATA / \"amg_cs_final_adjudicated.jsonl\"   # ÿ∫ŸäŸëÿ±Ÿáÿß ŸÑŸà ÿπŸÜÿØŸÉ CSV\n",
        "FP_MR_FINAL  = LABELING / \"mr_cs_final_adjudicated.csv\"   # ÿ∫ÿßŸÑÿ®ÿßŸã CSV\n"
      ],
      "metadata": {
        "id": "wfb-ukt11lJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LABELS = [\"pos\",\"neu\",\"neg\"]\n",
        "label_fix = {\"positive\":\"pos\",\"negative\":\"neg\",\"negitive\":\"neg\",\"neutral\":\"neu\"}\n",
        "\n",
        "def read_jsonl(fp: Path):\n",
        "    rows=[]\n",
        "    with open(fp, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line=line.strip()\n",
        "            if not line: continue\n",
        "            rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "def normalize_row(obj, text_keys=(\"text\",\"sentence\",\"utterance\"),\n",
        "                  label_keys=(\"label\",\"final_label\",\"sentiment\",\"gold\",\"target\")):\n",
        "    # ÿßŸÑÿ™ŸÉÿ≥ŸÄŸÄÿ™\n",
        "    text = None\n",
        "    for k in text_keys:\n",
        "        if k in obj and isinstance(obj[k], str) and obj[k].strip():\n",
        "            text = obj[k].strip()\n",
        "            break\n",
        "    assert text is not None, f\"no text field in: {obj.keys()}\"\n",
        "\n",
        "    # ÿßŸÑŸÑŸäÿ®ŸÄŸÄŸÑ\n",
        "    lab = None\n",
        "    for k in label_keys:\n",
        "        if k in obj and isinstance(obj[k], str) and obj[k].strip():\n",
        "            lab = obj[k].strip().lower()\n",
        "            lab = label_fix.get(lab, lab)\n",
        "            break\n",
        "    assert lab in LABELS, f\"unknown label: {lab}\"\n",
        "\n",
        "    return {\"text\": text, \"label\": lab}\n",
        "\n",
        "def show_dist(name, rows):\n",
        "    c = Counter([r[\"label\"] for r in rows])\n",
        "    print(f\"{name} ‚Üí {len(rows)} samples\")\n",
        "    print(c); print(\"-\"*40)\n"
      ],
      "metadata": {
        "id": "P2fUP6JZ1ygE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) EESA (ÿ¨ÿßŸáÿ≤ÿ©)\n",
        "eesa_tr = [normalize_row(r) for r in read_jsonl(FP_EESA_TR)]\n",
        "eesa_de = [normalize_row(r) for r in read_jsonl(FP_EESA_DE)]\n",
        "eesa_te = [normalize_row(r) for r in read_jsonl(FP_EESA_TE)]\n",
        "\n",
        "show_dist(\"EESA train\", eesa_tr)\n",
        "show_dist(\"EESA dev\",   eesa_de)\n",
        "show_dist(\"EESA test\",  eesa_te)\n",
        "\n",
        "# 2) AMG (ÿ≠ÿßŸàŸÑ ŸÜŸÇÿ±ÿ£ JSONL ÿ£ŸàŸÑÿßŸãÿå ŸÑŸà ŸÅÿ¥ŸÑ ÿ¨ÿ±Ÿëÿ® CSV)\n",
        "amg_rows = None\n",
        "try:\n",
        "    amg_rows = [normalize_row(r) for r in read_jsonl(FP_AMG_FINAL)]\n",
        "except Exception as e:\n",
        "    print(\"AMG jsonl read failed, trying CSV...\", e)\n",
        "    df_amg = pd.read_csv(FP_AMG_FINAL)  # ŸÑŸà ÿßŸÑŸÖŸÑŸÅ ŸÅÿπŸÑÿßŸã CSV ÿ∫ŸäŸëÿ± ÿßŸÑÿßŸÖÿ™ÿØÿßÿØ ŸÅŸàŸÇ\n",
        "    # ÿ≠ÿØÿØ ÿ£ÿπŸÖÿØÿ© text/label ÿ™ŸÑŸÇÿßÿ¶Ÿä\n",
        "    tx_col = \"text\" if \"text\" in df_amg.columns else [c for c in df_amg.columns if \"text\" in c.lower()][0]\n",
        "    lb_col = \"final_label\" if \"final_label\" in df_amg.columns else [c for c in df_amg.columns if \"label\" in c.lower()][0]\n",
        "    df_amg = df_amg[[tx_col, lb_col]].rename(columns={tx_col:\"text\", lb_col:\"label\"})\n",
        "    df_amg[\"label\"] = df_amg[\"label\"].map(lambda x: label_fix.get(str(x).lower().strip(), str(x).lower().strip()))\n",
        "    df_amg = df_amg[df_amg[\"label\"].isin(LABELS)].dropna(subset=[\"text\",\"label\"])\n",
        "    amg_rows = df_amg.to_dict(orient=\"records\")\n",
        "\n",
        "# 80/20 stratified\n",
        "amg_df = pd.DataFrame(amg_rows)\n",
        "amg_tr_df, amg_de_df = train_test_split(\n",
        "    amg_df, test_size=0.20, random_state=SEED, stratify=amg_df[\"label\"]\n",
        ")\n",
        "amg_tr = amg_tr_df.to_dict(orient=\"records\")\n",
        "amg_de = amg_de_df.to_dict(orient=\"records\")\n",
        "\n",
        "show_dist(\"AMG train\", amg_tr)\n",
        "show_dist(\"AMG dev\",   amg_de)\n",
        "\n",
        "# 3) MR (ÿ∫ÿßŸÑÿ®ÿßŸã CSV ŸÜŸáÿßÿ¶Ÿä)\n",
        "mr_df_raw = pd.read_csv(FP_MR_FINAL)\n",
        "# ÿßŸÉÿ™ÿ¥ÿßŸÅ ÿ™ŸÑŸÇÿßÿ¶Ÿä ŸÑŸÑÿ£ÿπŸÖÿØÿ©\n",
        "mr_tx = \"text\" if \"text\" in mr_df_raw.columns else [c for c in mr_df_raw.columns if \"text\" in c.lower()][0]\n",
        "mr_lb = \"final_label\" if \"final_label\" in mr_df_raw.columns else [c for c in mr_df_raw.columns if \"label\" in c.lower()][0]\n",
        "\n",
        "mr_df = mr_df_raw[[mr_tx, mr_lb]].rename(columns={mr_tx:\"text\", mr_lb:\"label\"})\n",
        "mr_df[\"label\"] = mr_df[\"label\"].map(lambda x: label_fix.get(str(x).lower().strip(), str(x).lower().strip()))\n",
        "mr_df = mr_df[mr_df[\"label\"].isin(LABELS)].dropna(subset=[\"text\",\"label\"]).reset_index(drop=True)\n",
        "\n",
        "print(\"MR all (cleaned) ‚Üí\", len(mr_df), Counter(mr_df[\"label\"]))\n",
        "print(\"-\"*40)\n",
        "\n",
        "mr_tr_df, mr_de_df = train_test_split(\n",
        "    mr_df, test_size=0.20, random_state=SEED, stratify=mr_df[\"label\"]\n",
        ")\n",
        "mr_tr = mr_tr_df.to_dict(orient=\"records\")\n",
        "mr_de = mr_de_df.to_dict(orient=\"records\")\n",
        "\n",
        "print(\"MR train ‚Üí\", len(mr_tr), Counter([r[\"label\"] for r in mr_tr]))\n",
        "print(\"MR dev   ‚Üí\", len(mr_de), Counter([r[\"label\"] for r in mr_de]))\n",
        "print(\"-\"*40)\n"
      ],
      "metadata": {
        "id": "bXjgZVlJ137Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_jsonl(rows, fp: Path):\n",
        "    with open(fp, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in rows: f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "mix_train = eesa_tr + amg_tr + mr_tr\n",
        "mix_dev   = eesa_de + amg_de + mr_de\n",
        "mix_test  = eesa_te  # ÿ´ÿßÿ®ÿ™ ŸÑŸÑÿ™ŸÇŸäŸäŸÖ ÿßŸÑÿπÿßÿØŸÑ\n",
        "\n",
        "show_dist(\"MIXED train\", mix_train)\n",
        "show_dist(\"MIXED dev\",   mix_dev)\n",
        "show_dist(\"TEST (EESA)\", mix_test)\n",
        "\n",
        "OUT_MIX_TR = DATA / \"mixed_train.jsonl\"\n",
        "OUT_MIX_DE = DATA / \"mixed_dev.jsonl\"\n",
        "to_jsonl(mix_train, OUT_MIX_TR)\n",
        "to_jsonl(mix_dev,   OUT_MIX_DE)\n",
        "print(\"Saved mixed:\", OUT_MIX_TR, \"and\", OUT_MIX_DE)\n"
      ],
      "metadata": {
        "id": "ne4bdlrw1-DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LABELS = [\"pos\",\"neu\",\"neg\"]\n",
        "label2id = {l:i for i,l in enumerate(LABELS)}\n",
        "id2label = {i:l for l,i in label2id.items()}\n",
        "\n",
        "class JsonlDS(Dataset):\n",
        "    def __init__(self, rows, tok, max_len=160):\n",
        "        self.rows = rows; self.tok = tok; self.max_len=max_len\n",
        "    def __len__(self): return len(self.rows)\n",
        "    def __getitem__(self, i):\n",
        "        x = self.rows[i]\n",
        "        enc = self.tok(x[\"text\"], max_length=self.max_len, truncation=True, padding=\"max_length\")\n",
        "        enc[\"labels\"] = label2id[x[\"label\"]]\n",
        "        return {k: torch.tensor(v) for k,v in enc.items()}\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"xlm-roberta-base\",\n",
        "    num_labels=3, id2label=id2label, label2id=label2id\n",
        ").to(device)\n",
        "\n",
        "ds_tr = JsonlDS(mix_train, tok, 160)\n",
        "ds_de = JsonlDS(mix_dev,   tok, 160)\n",
        "ds_te = JsonlDS(mix_test,  tok, 160)\n",
        "\n",
        "dl_tr = DataLoader(ds_tr, batch_size=16, shuffle=True)\n",
        "dl_de = DataLoader(ds_de, batch_size=32, shuffle=False)\n",
        "dl_te = DataLoader(ds_te, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "Co802X6S2G9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "EPOCHS = 3\n",
        "optim = AdamW(model.parameters(), lr=2e-5)\n",
        "best_f1 = -1.0\n",
        "best_state = None\n",
        "\n",
        "def eval_loop(dataloader):\n",
        "    model.eval()\n",
        "    preds=[]; gold=[]\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            labels = batch[\"labels\"].numpy().tolist()\n",
        "            batch = {k:v.to(device) for k,v in batch.items()}\n",
        "            logits = model(**batch).logits.detach().cpu().numpy()\n",
        "            preds.extend(logits.argmax(axis=1).tolist())\n",
        "            gold.extend(labels)\n",
        "    return gold, preds, f1_score(gold, preds, average=\"macro\")\n",
        "\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    for batch in dl_tr:\n",
        "        batch = {k:v.to(device) for k,v in batch.items()}\n",
        "        out = model(**batch)\n",
        "        out.loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optim.step(); optim.zero_grad()\n",
        "\n",
        "    g,p,f1 = eval_loop(dl_de)\n",
        "    print(f\"Epoch {ep} ‚Üí dev macro-F1 = {f1:.4f}\")\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_state = model.state_dict().copy()\n",
        "\n",
        "# load best and save\n",
        "model.load_state_dict(best_state)\n",
        "out_dir = MODELS / \"xlmr_sentiment_eesa_amg_mr\"\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "model.save_pretrained(out_dir.as_posix())\n",
        "tok.save_pretrained(out_dir.as_posix())\n",
        "print(\"‚úÖ Saved best to:\", out_dir)\n"
      ],
      "metadata": {
        "id": "-mAN-3pC2Qhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gold, preds, macro = eval_loop(dl_te)\n",
        "print(\"\\n=== XLM-R (EESA+AMG+MR) on EESA TEST ===\")\n",
        "print(classification_report(gold, preds, target_names=LABELS, digits=4))\n",
        "print(\"Macro-F1:\", macro)\n"
      ],
      "metadata": {
        "id": "imnV3VUf6El3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import sample\n",
        "\n",
        "# choose a small portion (~10%) from each domain dev set\n",
        "amg_sample = sample(amg_de, min(30, len(amg_de)))  # 30 examples or fewer\n",
        "mr_sample = sample(mr_de, min(50, len(mr_de)))     # 50 examples or fewer\n",
        "eesa_sample = sample(eesa_de, min(100, len(eesa_de)))  # small slice for variety\n",
        "\n",
        "mixed_test = amg_sample + mr_sample + eesa_sample\n",
        "show_dist(\"MIXED test (cross-domain)\", mixed_test)\n",
        "\n",
        "OUT_MIX_TEST = DATA / \"mixed_test.jsonl\"\n",
        "with open(OUT_MIX_TEST, \"w\", encoding=\"utf-8\") as f:\n",
        "    for r in mixed_test:\n",
        "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n"
      ],
      "metadata": {
        "id": "iLYfyJhC-KWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_mix = JsonlDS(mixed_test, tok, 160)\n",
        "dl_mix = DataLoader(ds_mix, batch_size=32, shuffle=False)\n",
        "\n",
        "gold, preds, macro = eval_loop(dl_mix)\n",
        "print(\"\\n=== XLM-R (EESA+AMG+MR) on MIXED TEST ===\")\n",
        "print(classification_report(gold, preds, target_names=LABELS, digits=4))\n",
        "print(\"Macro-F1:\", macro)\n"
      ],
      "metadata": {
        "id": "L296G9DH-ag7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import json, torch, pathlib\n",
        "\n",
        "SRC = \"/content/drive/MyDrive/cs-senti/models/xlmr_sentiment_eesa_amg_mr\"\n",
        "DST = \"/content/drive/MyDrive/cs-senti/models/sa_mixed_v3_frozen\"\n",
        "pathlib.Path(DST).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(SRC)\n",
        "mdl = AutoModelForSequenceClassification.from_pretrained(SRC)\n",
        "tok.save_pretrained(DST); mdl.save_pretrained(DST)\n",
        "\n",
        "# persist label map used everywhere in GAN code\n",
        "json.dump({\"labels\":[\"pos\",\"neg\",\"neu\"]}, open(f\"{DST}/label_map.json\",\"w\"))\n"
      ],
      "metadata": {
        "id": "yJDAK-C7Q2dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# /content/drive/MyDrive/cs-senti/utils/sa_reward.py\n",
        "import torch, json\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "class SentimentOracle:\n",
        "    def __init__(self, model_dir):\n",
        "        self.tok  = AutoTokenizer.from_pretrained(model_dir)\n",
        "        self.mdl  = AutoModelForSequenceClassification.from_pretrained(model_dir).eval().cuda()\n",
        "        self.map  = json.load(open(f\"{model_dir}/label_map.json\"))[\"labels\"]\n",
        "        self.idx  = {l:i for i,l in enumerate(self.map)}\n",
        "    @torch.no_grad()\n",
        "    def score(self, texts, targets):\n",
        "        enc = self.tok(texts, truncation=True, padding=True, max_length=128, return_tensors=\"pt\").to(self.mdl.device)\n",
        "        probs = self.mdl(**enc).logits.softmax(-1)\n",
        "        ids   = torch.tensor([self.idx[t] for t in targets], device=probs.device)\n",
        "        return probs[torch.arange(len(texts), device=probs.device), ids].detach().cpu().numpy()\n"
      ],
      "metadata": {
        "id": "ag6u7uKDRLjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LEXICAL REPLACEMENTS"
      ],
      "metadata": {
        "id": "Y5qnQJp7cSVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fallback: downgrade Torch to a version before the new default\n",
        "!pip -q install \"torch==2.5.1\" --index-url https://download.pytorch.org/whl/cpu\n",
        "import os, sys; print(torch.__version__)\n",
        "# Then Runtime -> Restart and run the cell again (no need for the add_safe_globals hack)\n"
      ],
      "metadata": {
        "id": "RGDrw5Z__vfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FIX for PyTorch 2.6 \"weights_only\" UnpicklingError with Stanza ---\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.serialization as ts\n",
        "\n",
        "# Allow-list NumPy globals used by older checkpoints\n",
        "ts.add_safe_globals([\n",
        "    np.core.multiarray._reconstruct,\n",
        "    np.dtype,\n",
        "    np.ufunc,\n",
        "    np.ndarray,\n",
        "])\n",
        "\n",
        "import stanza, spacy\n",
        "\n",
        "# (Re)download Arabic models if needed (safe to call repeatedly)\n",
        "stanza.download('ar')\n",
        "\n",
        "# Build Stanza Arabic pipeline (CPU is fine; set use_gpu=True if CUDA OK)\n",
        "ar_nlp = stanza.Pipeline(\n",
        "    'ar',\n",
        "    processors='tokenize,mwt,pos,lemma,ner',\n",
        "    tokenize_pretokenized=False,\n",
        "    verbose=False,\n",
        "    use_gpu=False  # change to True if your session has CUDA and works\n",
        ")\n",
        "\n",
        "# English spaCy (you already installed en_core_web_sm in Cell 1)\n",
        "nlp_en = spacy.load(\"en_core_web_sm\", disable=[\"ner\",\"parser\",\"lemmatizer\"])\n",
        "\n",
        "print(\"‚úÖ Pipelines ready: Stanza(ar) + spaCy(en)\")\n",
        "\n",
        "# quick sanity check\n",
        "doc = ar_nlp(\"ÿßŸÜÿß ÿ®ÿ≠ÿ® ÿßŸÑŸÇŸáŸàÿ© ŸÖŸÜ Starbucks\")\n",
        "for s in doc.sentences:\n",
        "    print([(w.text, w.upos, w.lemma) for w in s.words])\n"
      ],
      "metadata": {
        "id": "EVD_lsxS96yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell: POS + Morph feature extraction and switch-candidate tagging ===\n",
        "import re, json, math, random\n",
        "from pathlib import Path\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# --- simple language id per token\n",
        "_ar_re = re.compile(r\"[\\u0600-\\u06FF]\")   # Arabic block\n",
        "_en_re = re.compile(r\"[A-Za-z]\")\n",
        "\n",
        "def token_lang(t: str) -> str:\n",
        "    has_ar = bool(_ar_re.search(t))\n",
        "    has_en = bool(_en_re.search(t))\n",
        "    if has_ar and not has_en: return \"ar\"\n",
        "    if has_en and not has_ar: return \"en\"\n",
        "    if has_ar and has_en:     return \"mixed\"\n",
        "    return \"other\"\n",
        "\n",
        "# --- useful sets\n",
        "CONTENT_UPOS = {\"NOUN\",\"VERB\",\"ADJ\",\"PROPN\",\"ADV\"}\n",
        "PUNCT_UPOS   = {\"PUNCT\",\"SYM\"}\n",
        "AR_STOPLIKE  = set([\"Ÿà\",\"ŸÅŸä\",\"ŸÖŸÜ\",\"ÿπŸÑŸâ\",\"ÿπŸÜ\",\"ÿ£ŸÜ\",\"ÿ•ŸÜ\",\"ŸÉÿßŸÜ\",\"ŸÉÿßŸÜÿ™\",\"ŸáŸà\",\"ŸáŸä\",\"ŸáŸÖ\",\"Ÿäÿß\"])  # quick heuristic\n",
        "\n",
        "def stanza_ar_annot(text: str):\n",
        "    \"\"\"\n",
        "    Returns per-token dicts for Arabic using stanza:\n",
        "    [{'text': tok, 'upos': UPOS, 'lemma': lemma, 'feats': 'Feat=Val|...','ner': 'B-PER' or 'O'}...]\n",
        "    If the line contains English/mixed tokens, they will still appear in Stanza tokens (unknown UPOS sometimes).\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    doc = ar_nlp(text)\n",
        "    for s in doc.sentences:\n",
        "        for w in s.words:\n",
        "            out.append({\n",
        "                \"text\": w.text,\n",
        "                \"upos\": w.upos or \"X\",\n",
        "                \"lemma\": w.lemma or w.text,\n",
        "                \"feats\": w.feats or \"\",\n",
        "                \"ner\":  \"O\",  # we‚Äôll fill from token-level NER result below (stanza has sentence.ents for NER spans)\n",
        "            })\n",
        "    # overlay NER spans (token-level tag, rough)\n",
        "    # stanza stores NER on sentence.ents with character offsets; for simplicity, mark tokens that exactly match span text\n",
        "    # (good enough for protecting obvious names like 'Cairo', 'Starbucks')\n",
        "    try:\n",
        "        idx = 0\n",
        "        for s in doc.sentences:\n",
        "            tok_texts = [w.text for w in s.words]\n",
        "            tok_marks = [\"O\"] * len(tok_texts)\n",
        "            for ent in s.ents:\n",
        "                span = ent.text\n",
        "                # naive exact match window search\n",
        "                for i in range(len(tok_texts)):\n",
        "                    if tok_texts[i] == span:\n",
        "                        tok_marks[i] = f\"B-{ent.type}\"\n",
        "            # write back into 'out'\n",
        "            for i in range(len(tok_texts)):\n",
        "                out[idx][\"ner\"] = tok_marks[i]\n",
        "                idx += 1\n",
        "    except Exception:\n",
        "        pass\n",
        "    return out\n",
        "\n",
        "def spacy_en_annot(text: str):\n",
        "    \"\"\"\n",
        "    Returns per-token dicts for English using spaCy:\n",
        "    [{'text': tok, 'pos': coarse, 'lemma': lemma}...]\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    doc = nlp_en(text)\n",
        "    for t in doc:\n",
        "        out.append({\n",
        "            \"text\": t.text,\n",
        "            \"pos\": t.pos_,       # coarse POS\n",
        "            \"lemma\": t.lemma_ if t.lemma_ != \"-PRON-\" else t.text,\n",
        "        })\n",
        "    return out\n",
        "\n",
        "def build_features_line(text: str):\n",
        "    \"\"\"\n",
        "    Combine: language id per token + Arabic POS/morph (if ar) + English POS (if en).\n",
        "    Also compute a 'switch_candidate' boolean: Arabic content word, not NER, not punctuation/stoplike.\n",
        "    \"\"\"\n",
        "    # base tokens by simple whitespace split to align across tools reasonably\n",
        "    raw_tokens = text.split()\n",
        "    # stanza token stream (may differ; we‚Äôll align by greedy scan)\n",
        "    ar_ann = stanza_ar_annot(text)\n",
        "    en_ann = spacy_en_annot(text)\n",
        "\n",
        "    # make fast lookup by exact token for spaCy coarse POS (fallback only)\n",
        "    en_pos_map = defaultdict(list)\n",
        "    for d in en_ann:\n",
        "        en_pos_map[d[\"text\"]].append(d[\"pos\"])\n",
        "\n",
        "    enriched = []\n",
        "    for tok in raw_tokens:\n",
        "        lang = token_lang(tok)\n",
        "        # find closest stanza match (first unused matching token)\n",
        "        match_idx = None\n",
        "        for i, d in enumerate(ar_ann):\n",
        "            if d is None: continue\n",
        "            if d[\"text\"] == tok:\n",
        "                match_idx = i; break\n",
        "        ar_upos = \"X\"; ar_feats = \"\"; ar_lemma = tok; ar_ner = \"O\"\n",
        "        if match_idx is not None:\n",
        "            ar_upos  = ar_ann[match_idx][\"upos\"]\n",
        "            ar_feats = ar_ann[match_idx][\"feats\"]\n",
        "            ar_lemma = ar_ann[match_idx][\"lemma\"]\n",
        "            ar_ner   = ar_ann[match_idx][\"ner\"]\n",
        "            ar_ann[match_idx] = None  # consume\n",
        "\n",
        "        # english POS fallback\n",
        "        en_pos = en_pos_map[tok][0] if en_pos_map.get(tok) else \"\"\n",
        "\n",
        "        # candidate rule (Arabic only): content words, not NER, not punctuation/symbol, not stoplike, reasonable length\n",
        "        is_candidate = False\n",
        "        if lang == \"ar\":\n",
        "            if (ar_upos in CONTENT_UPOS) and (ar_ner == \"O\") and (ar_upos not in PUNCT_UPOS):\n",
        "                if (tok not in AR_STOPLIKE) and (len(tok) >= 2):\n",
        "                    is_candidate = True\n",
        "\n",
        "        enriched.append({\n",
        "            \"tok\": tok,\n",
        "            \"lang\": lang,\n",
        "            \"ar_upos\": ar_upos,\n",
        "            \"ar_feats\": ar_feats,\n",
        "            \"ar_lemma\": ar_lemma,\n",
        "            \"ar_ner\": ar_ner,\n",
        "            \"en_pos\": en_pos,\n",
        "            \"switch_candidate\": bool(is_candidate),\n",
        "        })\n",
        "    return {\n",
        "        \"text\": text,\n",
        "        \"tokens\": enriched,\n",
        "        \"num_candidates\": sum(1 for t in enriched if t[\"switch_candidate\"]),\n",
        "    }\n",
        "\n",
        "def annotate_rows(rows, out_path: Path, limit: int = 500):\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    stats = Counter()\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i, r in enumerate(rows[:limit]):\n",
        "            rec = build_features_line(r[\"text\"])\n",
        "            rec[\"label\"] = r[\"label\"]\n",
        "            json.dump(rec, f, ensure_ascii=False)\n",
        "            f.write(\"\\n\")\n",
        "            stats[\"lines\"] += 1\n",
        "            stats[\"candidates\"] += rec[\"num_candidates\"]\n",
        "    print(f\"‚úÖ wrote {stats['lines']} lines to {out_path} | total candidates={stats['candidates']} | avg per line={stats['candidates']/max(1,stats['lines']):.2f}\")\n",
        "\n",
        "# ---- run on small samples to verify (use your already-loaded eesa_de / amg_de / mr_de) ----\n",
        "LING_DIR = (DATA / \"ling\"); LING_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "annotate_rows(eesa_de, LING_DIR / \"eesa_dev_annot.jsonl\", limit=300)\n",
        "annotate_rows(amg_de,  LING_DIR / \"amg_dev_annot.jsonl\",  limit=200)\n",
        "annotate_rows(mr_de,   LING_DIR / \"mr_dev_annot.jsonl\",   limit=200)\n",
        "\n",
        "# quick peek\n",
        "peek_fp = LING_DIR / \"eesa_dev_annot.jsonl\"\n",
        "print(\"Peek one line:\")\n",
        "with open(peek_fp, encoding=\"utf-8\") as f:\n",
        "    print(f.readline().strip()[:300] + \" ...\")\n"
      ],
      "metadata": {
        "id": "8HVcp50xC5LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/cs-senti\")\n",
        "DATA = BASE / \"data\"\n",
        "LING = DATA / \"ling\"\n",
        "\n",
        "FP_E_ANN = LING / \"eesa_dev_annot.jsonl\"\n",
        "FP_A_ANN = LING / \"amg_dev_annot.jsonl\"\n",
        "FP_M_ANN = LING / \"mr_dev_annot.jsonl\"\n",
        "\n",
        "OUT_E_SK = LING / \"eesa_dev_skeleton.jsonl\"\n",
        "OUT_A_SK = LING / \"amg_dev_skeleton.jsonl\"\n",
        "OUT_M_SK = LING / \"mr_dev_skeleton.jsonl\"\n",
        "OUT_ALL  = LING / \"host_dev_skeleton_all.jsonl\"\n",
        "\n",
        "def read_jsonl(fp: Path):\n",
        "    rows = []\n",
        "    with open(fp, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "def write_jsonl(rows, fp: Path):\n",
        "    with open(fp, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in rows:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "def build_skeleton(annot_rows, domain: str):\n",
        "    \"\"\"\n",
        "    Convert annotated lines into 'skeleton' entries:\n",
        "      - tokens: list of token strings\n",
        "      - langs: language tag per token\n",
        "      - cand_indices: positions where we *may* code-switch\n",
        "      - label: sentiment label\n",
        "      - text: original sentence\n",
        "      - domain: source dataset (eesa / amg / mr)\n",
        "    Skip sentences with 0 candidates.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    total_cands = 0\n",
        "\n",
        "    for obj in annot_rows:\n",
        "        toks_meta = obj.get(\"tokens\", [])\n",
        "        if not toks_meta:\n",
        "            continue\n",
        "\n",
        "        tokens = [t[\"tok\"] for t in toks_meta]\n",
        "        langs  = [t.get(\"lang\", \"other\") for t in toks_meta]\n",
        "\n",
        "        cand_indices = [\n",
        "            i for i, t in enumerate(toks_meta)\n",
        "            if t.get(\"switch_candidate\", False)\n",
        "        ]\n",
        "\n",
        "        if not cand_indices:\n",
        "            # nothing to switch ‚Üí not useful for GAN\n",
        "            continue\n",
        "\n",
        "        total_cands += len(cand_indices)\n",
        "\n",
        "        sk = {\n",
        "            \"text\": obj.get(\"text\", \"\"),\n",
        "            \"label\": obj.get(\"label\", \"neu\"),  # fallback\n",
        "            \"tokens\": tokens,\n",
        "            \"langs\": langs,\n",
        "            \"cand_indices\": cand_indices,\n",
        "            \"domain\": domain\n",
        "        }\n",
        "        out.append(sk)\n",
        "\n",
        "    avg_cands = total_cands / max(len(out), 1)\n",
        "    print(f\"{domain.upper()} skeletons ‚Üí {len(out)} lines | \"\n",
        "          f\"total candidates={total_cands} | avg per line={avg_cands:.2f}\")\n",
        "    return out\n",
        "\n",
        "# 1) Load annotated dev files\n",
        "eesa_ann = read_jsonl(FP_E_ANN)\n",
        "amg_ann  = read_jsonl(FP_A_ANN)\n",
        "mr_ann   = read_jsonl(FP_M_ANN)\n",
        "\n",
        "print(\"Loaded annotated:\")\n",
        "print(\"  EESA:\", len(eesa_ann))\n",
        "print(\"  AMG :\", len(amg_ann))\n",
        "print(\"  MR  :\", len(mr_ann))\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# 2) Build skeletons per domain\n",
        "eesa_sk = build_skeleton(eesa_ann, \"eesa\")\n",
        "amg_sk  = build_skeleton(amg_ann,  \"amg\")\n",
        "mr_sk   = build_skeleton(mr_ann,   \"mr\")\n",
        "\n",
        "# 3) Save per-domain skeletons\n",
        "write_jsonl(eesa_sk, OUT_E_SK)\n",
        "write_jsonl(amg_sk,  OUT_A_SK)\n",
        "write_jsonl(mr_sk,   OUT_M_SK)\n",
        "\n",
        "print(\"‚úÖ Saved:\")\n",
        "print(\" \", OUT_E_SK)\n",
        "print(\" \", OUT_A_SK)\n",
        "print(\" \", OUT_M_SK)\n",
        "\n",
        "# 4) Merge into one host pool for GAN\n",
        "host_all = eesa_sk + amg_sk + mr_sk\n",
        "write_jsonl(host_all, OUT_ALL)\n",
        "\n",
        "# some quick stats\n",
        "lbl_counts = Counter([r[\"label\"] for r in host_all])\n",
        "dom_counts = Counter([r[\"domain\"] for r in host_all])\n",
        "print(\"-\" * 40)\n",
        "print(\"Merged host_dev_skeleton_all:\", len(host_all), \"lines\")\n",
        "print(\"Label dist:\", lbl_counts)\n",
        "print(\"Domain dist:\", dom_counts)\n",
        "print(\"‚úÖ Saved merged host pool to:\", OUT_ALL)\n"
      ],
      "metadata": {
        "id": "M6OLq6GHIQ27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, random\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/cs-senti\")\n",
        "DATA = BASE / \"data\"\n",
        "LING = DATA / \"ling\"\n",
        "\n",
        "FP_HOST_SK = LING / \"host_dev_skeleton_all.jsonl\"\n",
        "OUT_TOY    = LING / \"host_dev_switched_toy.jsonl\"\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "def read_jsonl(fp: Path):\n",
        "    rows = []\n",
        "    with open(fp, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "def write_jsonl(rows, fp: Path):\n",
        "    with open(fp, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in rows:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "# üî§ Very small toy AR‚ÜíEN ‚Äúlexicon‚Äù just to see switching working\n",
        "AR_EN_LEX = {\n",
        "    \"ŸÖÿ≥ŸÑÿ≥ŸÑ\": [\"series\", \"show\"],\n",
        "    \"ŸÅŸäŸÑŸÖ\": [\"movie\", \"film\"],\n",
        "    \"ÿßÿ∫ŸÜŸäÿ©\": [\"song\"],\n",
        "    \"ÿ£ÿ∫ŸÜŸäÿ©\": [\"song\"],\n",
        "    \"ÿ≠ÿßÿ¨Ÿá\": [\"thing\"],\n",
        "    \"ÿ≠ÿßÿ¨ÿ©\": [\"thing\"],\n",
        "    \"ÿ®ÿ±ŸÜÿßŸÖÿ¨\": [\"program\", \"show\"],\n",
        "    \"ŸÖÿ±ÿßÿ¨ÿπÿ©\": [\"review\"],\n",
        "    \"ÿ≠ŸÑŸÇÿ©\": [\"episode\"],\n",
        "    \"ŸÖŸàÿ®ÿßŸäŸÑ\": [\"phone\"],\n",
        "    \"ÿ™ŸÑŸäŸÅŸàŸÜ\": [\"phone\"],\n",
        "    \"ÿßŸÜÿ™ÿ±ŸÅŸäŸà\": [\"interview\"],\n",
        "    \"ÿ¨ÿßŸÖÿπÿ©\": [\"university\"],\n",
        "    \"ŸÖÿßÿ™ÿ¥\": [\"match\", \"game\"],\n",
        "    # fallback if token not in dict\n",
        "}\n",
        "\n",
        "def pick_en_replacement(ar_token: str) -> str:\n",
        "    # basic normalization\n",
        "    base = ar_token.strip()\n",
        "    if base in AR_EN_LEX:\n",
        "        return random.choice(AR_EN_LEX[base])\n",
        "    # fallback: generic English word so we still see a switch\n",
        "    return \"thing\"\n",
        "\n",
        "def build_toy_switched(host_rows):\n",
        "    out_rows = []\n",
        "    for obj in host_rows:\n",
        "        tokens = obj[\"tokens\"]\n",
        "        cand_indices = obj.get(\"cand_indices\", [])\n",
        "        if not cand_indices:\n",
        "            continue\n",
        "\n",
        "        # 1) pick exactly ONE position to switch (toy setup)\n",
        "        switch_pos = random.choice(cand_indices)\n",
        "\n",
        "        new_tokens = tokens.copy()\n",
        "        ar_tok = new_tokens[switch_pos]\n",
        "        en_tok = pick_en_replacement(ar_tok)\n",
        "        new_tokens[switch_pos] = en_tok\n",
        "\n",
        "        switched_text = \" \".join(new_tokens)\n",
        "\n",
        "        out_rows.append({\n",
        "            \"orig_text\": obj[\"text\"],\n",
        "            \"switched_text\": switched_text,\n",
        "            \"label\": obj[\"label\"],\n",
        "            \"domain\": obj[\"domain\"],\n",
        "            \"tokens_orig\": tokens,\n",
        "            \"tokens_switched\": new_tokens,\n",
        "            \"switch_pos\": switch_pos,\n",
        "            \"switched_from\": ar_tok,\n",
        "            \"switched_to\": en_tok\n",
        "        })\n",
        "\n",
        "    return out_rows\n",
        "\n",
        "# 1) Load skeleton pool\n",
        "host_rows = read_jsonl(FP_HOST_SK)\n",
        "print(\"Loaded skeleton rows:\", len(host_rows))\n",
        "\n",
        "# 2) Build toy switched examples\n",
        "toy_rows = build_toy_switched(host_rows)\n",
        "print(\"Toy switched rows:\", len(toy_rows))\n",
        "\n",
        "# 3) Save\n",
        "write_jsonl(toy_rows, OUT_TOY)\n",
        "print(\"‚úÖ Saved toy switched set to:\", OUT_TOY)\n",
        "\n",
        "# 4) Peek a few examples\n",
        "for ex in toy_rows[:5]:\n",
        "    print(\"\\n--- EXAMPLE ---\")\n",
        "    print(\"LABEL  :\", ex[\"label\"], \"| DOMAIN:\", ex[\"domain\"])\n",
        "    print(\"ORIG   :\", ex[\"orig_text\"])\n",
        "    print(\"SWITCH :\", ex[\"switched_text\"])\n",
        "    print(\"switched token:\", ex[\"switched_from\"], \"‚Üí\", ex[\"switched_to\"],\n",
        "          \"@pos\", ex[\"switch_pos\"])\n"
      ],
      "metadata": {
        "id": "nILAKW67KMd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, random\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/cs-senti\")\n",
        "LING = (BASE / \"data\" / \"ling\")\n",
        "FP_HOST_SK = LING / \"host_dev_skeleton_all.jsonl\"\n",
        "OUT_TOY    = LING / \"host_dev_switched_toy_clean.jsonl\"\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "def read_jsonl(fp: Path):\n",
        "    rows = []\n",
        "    with open(fp, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "def write_jsonl(rows, fp: Path):\n",
        "    with open(fp, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in rows:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "# üí° Slightly more reasonable toy lexicon\n",
        "AR_EN_LEX = {\n",
        "    \"ŸÖÿ≥ŸÑÿ≥ŸÑ\": [\"series\", \"show\"],\n",
        "    \"ŸÅŸäŸÑŸÖ\": [\"movie\", \"film\"],\n",
        "    \"ÿ®ÿ±ŸÜÿßŸÖÿ¨\": [\"program\", \"show\"],\n",
        "    \"ŸÖÿ±ÿßÿ¨ÿπÿ©\": [\"review\"],\n",
        "    \"ÿ≠ŸÑŸÇÿ©\": [\"episode\"],\n",
        "    \"ÿ¨ÿßŸÖÿπÿ©\": [\"university\"],\n",
        "    \"ÿßŸÜÿ™ÿ±ŸÅŸäŸà\": [\"interview\"],\n",
        "    \"ŸÖŸàÿ®ÿßŸäŸÑ\": [\"phone\"],\n",
        "    \"ÿ™ŸÑŸäŸÅŸàŸÜ\": [\"phone\"],\n",
        "    \"ÿßÿ∫ŸÜŸäÿ©\": [\"song\"],\n",
        "    \"ÿ£ÿ∫ŸÜŸäÿ©\": [\"song\"],\n",
        "    \"ŸÖÿßÿ™ÿ¥\": [\"match\", \"game\"],\n",
        "}\n",
        "\n",
        "def pick_en_replacement(ar_token: str):\n",
        "    base = ar_token.strip()\n",
        "    if base in AR_EN_LEX:\n",
        "        return random.choice(AR_EN_LEX[base])\n",
        "    # ‚ùå no fallback ‚Üí return None so we can skip this switch\n",
        "    return None\n",
        "\n",
        "def build_toy_switched_clean(host_rows):\n",
        "    out_rows = []\n",
        "    skipped = 0\n",
        "    for obj in host_rows:\n",
        "        tokens = obj[\"tokens\"]\n",
        "        cand_indices = obj.get(\"cand_indices\", [])\n",
        "        if not cand_indices:\n",
        "            continue\n",
        "\n",
        "        # shuffle candidate indices to try a few until we find one with a lexicon match\n",
        "        idxs = cand_indices[:]\n",
        "        random.shuffle(idxs)\n",
        "        switched = False\n",
        "\n",
        "        for switch_pos in idxs:\n",
        "            ar_tok = tokens[switch_pos]\n",
        "            en_tok = pick_en_replacement(ar_tok)\n",
        "            if en_tok is None:\n",
        "                continue  # try another candidate\n",
        "            new_tokens = tokens.copy()\n",
        "            new_tokens[switch_pos] = en_tok\n",
        "            switched_text = \" \".join(new_tokens)\n",
        "            out_rows.append({\n",
        "                \"orig_text\": obj[\"text\"],\n",
        "                \"switched_text\": switched_text,\n",
        "                \"label\": obj[\"label\"],\n",
        "                \"domain\": obj[\"domain\"],\n",
        "                \"tokens_orig\": tokens,\n",
        "                \"tokens_switched\": new_tokens,\n",
        "                \"switch_pos\": switch_pos,\n",
        "                \"switched_from\": ar_tok,\n",
        "                \"switched_to\": en_tok\n",
        "            })\n",
        "            switched = True\n",
        "            break\n",
        "\n",
        "        if not switched:\n",
        "            skipped += 1\n",
        "\n",
        "    print(f\"Total rows in skeleton: {len(host_rows)}\")\n",
        "    print(f\"Kept with at least one lexicon switch: {len(out_rows)}\")\n",
        "    print(f\"Skipped (no lexicon match): {skipped}\")\n",
        "    return out_rows\n",
        "\n",
        "# 1) Load skeleton pool\n",
        "host_rows = read_jsonl(FP_HOST_SK)\n",
        "print(\"Loaded skeleton rows:\", len(host_rows))\n",
        "\n",
        "# 2) Build cleaner toy switched examples\n",
        "toy_rows = build_toy_switched_clean(host_rows)\n",
        "\n",
        "# 3) Save\n",
        "write_jsonl(toy_rows, OUT_TOY)\n",
        "print(\"‚úÖ Saved CLEAN toy switched set to:\", OUT_TOY)\n",
        "\n",
        "# 4) Peek\n",
        "for ex in toy_rows[:5]:\n",
        "    print(\"\\n--- EXAMPLE ---\")\n",
        "    print(\"LABEL  :\", ex[\"label\"], \"| DOMAIN:\", ex[\"domain\"])\n",
        "    print(\"ORIG   :\", ex[\"orig_text\"])\n",
        "    print(\"SWITCH :\", ex[\"switched_text\"])\n",
        "    print(\"switched token:\", ex[\"switched_from\"], \"‚Üí\", ex[\"switched_to\"],\n",
        "          \"@pos\", ex[\"switch_pos\"])\n"
      ],
      "metadata": {
        "id": "H39QKFqqLsJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# ------------------------------\n",
        "# 1) annotate_one()\n",
        "# ------------------------------\n",
        "def annotate_one(text, label, domain, ar_nlp, nlp_en):\n",
        "    \"\"\"Annotate a single text with tokens + POS + lang + switch candidates.\"\"\"\n",
        "    # simple whitespace tokenization\n",
        "    raw_toks = text.strip().split()\n",
        "    if not raw_toks:\n",
        "        return None\n",
        "\n",
        "    # Arabic pass (Stanza)\n",
        "    ar_doc = ar_nlp(text)\n",
        "    ar_tok_map = {}\n",
        "    for s in ar_doc.sentences:\n",
        "        for w in s.words:\n",
        "            ar_tok_map.setdefault(w.text, []).append({\n",
        "                \"lang\": \"ar\",\n",
        "                \"ar_upos\": w.upos,\n",
        "                \"ar_lemma\": w.lemma,\n",
        "                \"ar_ner\": w.ner if hasattr(w, \"ner\") else \"O\",\n",
        "            })\n",
        "\n",
        "    # English POS tagging (spaCy)\n",
        "    en_doc = nlp_en(text)\n",
        "    en_tok_map = {t.text: t.pos_ for t in en_doc}\n",
        "\n",
        "    annotated = []\n",
        "    for tok in raw_toks:\n",
        "        info = {\"tok\": tok}\n",
        "\n",
        "        if tok in ar_tok_map:\n",
        "            # Mark as Arabic\n",
        "            info.update(ar_tok_map[tok][0])\n",
        "            info[\"lang\"] = \"ar\"\n",
        "        else:\n",
        "            # Try English fallback\n",
        "            info[\"lang\"] = \"en\"\n",
        "            info[\"en_pos\"] = en_tok_map.get(tok, \"X\")\n",
        "\n",
        "        # Decide if switchable\n",
        "        info[\"switch_candidate\"] = (\n",
        "            info[\"lang\"] == \"ar\" and\n",
        "            info.get(\"ar_upos\") in [\"NOUN\", \"ADJ\"]  # switch nouns + adjectives\n",
        "        )\n",
        "        annotated.append(info)\n",
        "\n",
        "    return {\n",
        "        \"text\": text,\n",
        "        \"label\": label,\n",
        "        \"domain\": domain,\n",
        "        \"tokens\": annotated,\n",
        "    }\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# 2) process_split()\n",
        "# ------------------------------\n",
        "def process_split(rows, domain, out_path, split_name=\"\", max_lines=None):\n",
        "    \"\"\"Annotate many rows and write jsonl file.\"\"\"\n",
        "    count = 0\n",
        "    total_candidates = 0\n",
        "\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i, r in enumerate(rows):\n",
        "            if max_lines and count >= max_lines:\n",
        "                break\n",
        "\n",
        "            out = annotate_one(\n",
        "                text=r[\"text\"],\n",
        "                label=r[\"label\"],\n",
        "                domain=domain,\n",
        "                ar_nlp=ar_nlp,\n",
        "                nlp_en=nlp_en\n",
        "            )\n",
        "            if not out:\n",
        "                continue\n",
        "\n",
        "            cand = sum(1 for t in out[\"tokens\"] if t.get(\"switch_candidate\"))\n",
        "            if cand == 0:\n",
        "                continue\n",
        "\n",
        "            total_candidates += cand\n",
        "            f.write(json.dumps(out, ensure_ascii=False) + \"\\n\")\n",
        "            count += 1\n",
        "\n",
        "            if i % 200 == 0:\n",
        "                print(f\"[{split_name}] processed {i} rows...\")\n",
        "\n",
        "    print(f\"‚úÖ wrote {count} lines to {out_path}\")\n",
        "    print(f\"   total candidates={total_candidates} | avg per line={total_candidates/count if count else 0:.2f}\")\n"
      ],
      "metadata": {
        "id": "MW5_R2jVRQSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "# --- paths ---\n",
        "LING = DATA / \"ling\"\n",
        "LING.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "FP_EESA_TR_ANNOT = LING / \"eesa_train_annot.jsonl\"\n",
        "FP_AMG_TR_ANNOT  = LING / \"amg_train_annot.jsonl\"\n",
        "FP_MR_TR_ANNOT   = LING / \"mr_train_annot.jsonl\"\n",
        "\n",
        "# ---------- 1) annotate TRAIN splits (same as you did for DEV) ----------\n",
        "\n",
        "# process_split(rows, domain, out_path, max_lines=None)\n",
        "# we set split_name just for logging inside the function if it uses it\n",
        "\n",
        "print(\"üîÅ Annotating TRAIN splits...\")\n",
        "\n",
        "process_split(\n",
        "    rows=eesa_tr,\n",
        "    domain=\"eesa\",\n",
        "    out_path=FP_EESA_TR_ANNOT,\n",
        "    split_name=\"eesa_train\",\n",
        "    max_lines=None,      # use full train\n",
        ")\n",
        "\n",
        "process_split(\n",
        "    rows=amg_tr,\n",
        "    domain=\"amg\",\n",
        "    out_path=FP_AMG_TR_ANNOT,\n",
        "    split_name=\"amg_train\",\n",
        "    max_lines=None,\n",
        ")\n",
        "\n",
        "process_split(\n",
        "    rows=mr_tr,\n",
        "    domain=\"mr\",\n",
        "    out_path=FP_MR_TR_ANNOT,\n",
        "    split_name=\"mr_train\",\n",
        "    max_lines=None,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Finished annotating TRAIN splits.\")\n",
        "print(\"Files:\")\n",
        "print(\" -\", FP_EESA_TR_ANNOT)\n",
        "print(\" -\", FP_AMG_TR_ANNOT)\n",
        "print(\" -\", FP_MR_TR_ANNOT)\n",
        "\n",
        "# ---------- 2) Build host skeleton from *_train_annot ----------\n",
        "\n",
        "def load_annot(fp: Path, domain_hint=None):\n",
        "    \"\"\"Load annotated jsonl and (optionally) enforce domain field.\"\"\"\n",
        "    rows = []\n",
        "    with open(fp, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            obj = json.loads(line)\n",
        "            # make sure these fields exist\n",
        "            if \"domain\" not in obj and domain_hint is not None:\n",
        "                obj[\"domain\"] = domain_hint\n",
        "            rows.append(obj)\n",
        "    return rows\n",
        "\n",
        "# load all three\n",
        "eesa_ann = load_annot(FP_EESA_TR_ANNOT, domain_hint=\"eesa\")\n",
        "amg_ann  = load_annot(FP_AMG_TR_ANNOT,  domain_hint=\"amg\")\n",
        "mr_ann   = load_annot(FP_MR_TR_ANNOT,   domain_hint=\"mr\")\n",
        "\n",
        "print(\"Loaded annotated TRAIN:\")\n",
        "print(\"  EESA:\", len(eesa_ann))\n",
        "print(\"  AMG :\", len(amg_ann))\n",
        "print(\"  MR  :\", len(mr_ann))\n",
        "\n",
        "host_skeleton = []\n",
        "for src_rows in (eesa_ann, amg_ann, mr_ann):\n",
        "    for r in src_rows:\n",
        "        text   = r.get(\"text\", \"\").strip()\n",
        "        label  = r.get(\"label\", \"neu\")\n",
        "        domain = r.get(\"domain\", \"unk\")\n",
        "        toks   = r.get(\"tokens\", [])\n",
        "\n",
        "        if not text or not toks:\n",
        "            continue\n",
        "\n",
        "        # indices of candidate tokens\n",
        "        cand_idx = [\n",
        "            i for i, t in enumerate(toks)\n",
        "            if t.get(\"switch_candidate\", False) is True\n",
        "        ]\n",
        "        if not cand_idx:\n",
        "            continue  # nothing interesting to switch here\n",
        "\n",
        "        # optional filter: require at least 1 Arabic token\n",
        "        has_ar = any(t.get(\"lang\") == \"ar\" for t in toks)\n",
        "        if not has_ar:\n",
        "            continue\n",
        "\n",
        "        host_skeleton.append({\n",
        "            \"text\": text,\n",
        "            \"label\": label,\n",
        "            \"domain\": domain,\n",
        "            \"tokens\": [t[\"tok\"] for t in toks],\n",
        "            \"cand_indices\": cand_idx,\n",
        "        })\n",
        "\n",
        "print(\"Total host skeleton rows (TRAIN):\", len(host_skeleton))\n",
        "print(\"Label distribution:\", Counter([r[\"label\"] for r in host_skeleton]))\n",
        "print(\"Domain distribution:\", Counter([r[\"domain\"] for r in host_skeleton]))\n",
        "\n",
        "FP_HOST_TRAIN = LING / \"host_train_skeleton_all.jsonl\"\n",
        "with open(FP_HOST_TRAIN, \"w\", encoding=\"utf-8\") as f:\n",
        "    for r in host_skeleton:\n",
        "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"‚úÖ Saved HOST TRAIN skeleton to:\", FP_HOST_TRAIN)\n",
        "\n",
        "# quick peek\n",
        "if host_skeleton:\n",
        "    ex = host_skeleton[0]\n",
        "    print(\"\\n--- EXAMPLE HOST TRAIN ---\")\n",
        "    print(\"LABEL :\", ex[\"label\"], \"| DOMAIN:\", ex[\"domain\"])\n",
        "    print(\"TEXT  :\", ex[\"text\"])\n",
        "    print(\"TOKENS:\", ex[\"tokens\"])\n",
        "    print(\"CAND  :\", ex[\"cand_indices\"])\n"
      ],
      "metadata": {
        "id": "Hv97RFoyPTxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json, random, math\n",
        "from collections import Counter\n",
        "\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "# ---------------- paths ----------------\n",
        "LING = DATA / \"ling\"\n",
        "FP_HOST_TRAIN = LING / \"host_train_skeleton_all.jsonl\"\n",
        "FP_LEXICON    = LING / \"ar_en_lex_mt_from_host.json\"\n",
        "\n",
        "# ---------------- load host skeleton ----------------\n",
        "host_skeleton = []\n",
        "with open(FP_HOST_TRAIN, encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        host_skeleton.append(json.loads(line))\n",
        "\n",
        "print(\"Loaded host skeleton rows:\", len(host_skeleton))\n",
        "\n",
        "# ---------------- collect unique candidate forms ----------------\n",
        "cand_forms = set()\n",
        "for r in host_skeleton:\n",
        "    toks = r[\"tokens\"]\n",
        "    for idx in r[\"cand_indices\"]:\n",
        "        if 0 <= idx < len(toks):\n",
        "            tok = toks[idx].strip()\n",
        "            if tok and any(\"\\u0600\" <= ch <= \"\\u06FF\" for ch in tok):  # has Arabic\n",
        "                cand_forms.add(tok)\n",
        "\n",
        "cand_forms = sorted(cand_forms)\n",
        "print(\"Unique Arabic candidate forms:\", len(cand_forms))\n",
        "print(\"Sample:\", cand_forms[:20])\n",
        "\n",
        "# ---------------- load AR‚ÜíEN MT model ----------------\n",
        "mt_name = \"Helsinki-NLP/opus-mt-ar-en\"\n",
        "mt_tok   = MarianTokenizer.from_pretrained(mt_name)\n",
        "mt_model = MarianMTModel.from_pretrained(mt_name).to(device)\n",
        "mt_model.eval()\n",
        "\n",
        "def translate_batch(words):\n",
        "    \"\"\"Translate a batch of Arabic tokens to short English words.\"\"\"\n",
        "    if not words:\n",
        "        return []\n",
        "    batch = [w if w.strip() else \"UNK\" for w in words]\n",
        "    enc = mt_tok(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        gen = mt_model.generate(**enc, max_length=6, num_beams=5)\n",
        "    outs = mt_tok.batch_decode(gen, skip_special_tokens=True)\n",
        "\n",
        "    cleaned = []\n",
        "    for out in outs:\n",
        "        txt = out.strip()\n",
        "        # Simple cleanup\n",
        "        txt = txt.replace(\" .\", \"\").replace(\".\", \"\").strip()\n",
        "        # Take first content token (avoid long phrases)\n",
        "        parts = txt.split()\n",
        "        if not parts:\n",
        "            cleaned.append(\"thing\")   # fallback\n",
        "        else:\n",
        "            # drop articles like \"the\", \"a\", \"an\" if there is a second word\n",
        "            if parts[0].lower() in {\"the\", \"a\", \"an\"} and len(parts) > 1:\n",
        "                cleaned.append(parts[1].lower())\n",
        "            else:\n",
        "                cleaned.append(parts[0].lower())\n",
        "    return cleaned\n",
        "\n",
        "# ---------------- build lexicon in batches ----------------\n",
        "BATCH = 32\n",
        "lex = {}\n",
        "for i in range(0, len(cand_forms), BATCH):\n",
        "    chunk = cand_forms[i : i + BATCH]\n",
        "    trans = translate_batch(chunk)\n",
        "    for src, tgt in zip(chunk, trans):\n",
        "        lex[src] = tgt\n",
        "    print(f\"Translated {i+len(chunk)}/{len(cand_forms)} candidate forms...\", end=\"\\r\")\n",
        "\n",
        "print(\"\\nDone translating candidates.\")\n",
        "print(\"Sample lexicon entries:\")\n",
        "for k in list(lex.keys())[:15]:\n",
        "    print(f\"  {k}  ‚Üí  {lex[k]}\")\n",
        "\n",
        "# optionally fix some bad ones manually if you spot them\n",
        "manual_fixes = {\n",
        "    # \"ÿπÿ≥ŸÑ\": \"sweetheart\",\n",
        "    # \"ÿ¥ÿÆÿµŸäÿ©\": \"character\",\n",
        "}\n",
        "lex.update(manual_fixes)\n",
        "\n",
        "# ---------------- save lexicon ----------------\n",
        "with open(FP_LEXICON, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(lex, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"‚úÖ Saved lexicon to:\", FP_LEXICON, \"| size:\", len(lex))\n"
      ],
      "metadata": {
        "id": "W7778Klca89z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, random\n",
        "from pathlib import Path\n",
        "\n",
        "FP_LEXICON = DATA / \"ling\" / \"ar_en_lex_mt_from_host.json\"\n",
        "\n",
        "with open(FP_LEXICON, encoding=\"utf-8\") as f:\n",
        "    lex = json.load(f)\n",
        "\n",
        "print(\"Lexicon size:\", len(lex))\n",
        "\n",
        "# show 30 random entries\n",
        "for src in random.sample(list(lex.keys()), 30):\n",
        "    print(f\"{src:15s} ‚Üí {lex[src]}\")\n"
      ],
      "metadata": {
        "id": "yLL6MZ-EeRxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "FP_HOST_TRAIN = DATA / \"ling\" / \"host_train_skeleton_all.jsonl\"\n",
        "FP_LEXICON    = DATA / \"ling\" / \"ar_en_lex_mt_from_host.json\"\n",
        "\n",
        "# load lexicon\n",
        "with open(FP_LEXICON, encoding=\"utf-8\") as f:\n",
        "    lex = json.load(f)\n",
        "\n",
        "# load host skeleton\n",
        "host_skeleton = []\n",
        "with open(FP_HOST_TRAIN, encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        host_skeleton.append(json.loads(line))\n",
        "\n",
        "cand_freq = Counter()\n",
        "for r in host_skeleton:\n",
        "    toks = r[\"tokens\"]\n",
        "    for idx in r[\"cand_indices\"]:\n",
        "        if 0 <= idx < len(toks):\n",
        "            tok = toks[idx]\n",
        "            if tok in lex:\n",
        "                cand_freq[tok] += 1\n",
        "\n",
        "print(\"Unique candidate tokens in lexicon:\", len(cand_freq))\n",
        "print(\"\\nTop 40 by frequency:\")\n",
        "for src, cnt in cand_freq.most_common(40):\n",
        "    print(f\"{cnt:4d}x  {src:15s} ‚Üí {lex[src]}\")\n"
      ],
      "metadata": {
        "id": "nBoh9Z8jeZhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "LEX_PATH = Path(\"/content/drive/MyDrive/cs-senti/data/ling/ar_en_lex_mt_from_host.json\")\n",
        "\n",
        "with open(LEX_PATH, encoding=\"utf-8\") as f:\n",
        "    raw = json.load(f)\n",
        "\n",
        "print(\"Type of top-level object:\", type(raw))\n",
        "\n",
        "# Normalize into list of dicts: {ar, mt_en}\n",
        "norm = []\n",
        "\n",
        "if isinstance(raw, dict):\n",
        "    # assume: { \"ÿ¢ÿÆÿØ\": \"take\", ... }\n",
        "    for ar, mt_en in raw.items():\n",
        "        norm.append({\"ar\": ar, \"mt_en\": mt_en})\n",
        "elif isinstance(raw, list):\n",
        "    # assume: [ {\"ar\": \"...\", \"en\": \"...\"}, ... ]\n",
        "    for obj in raw:\n",
        "        ar = obj.get(\"ar\")\n",
        "        mt_en = obj.get(\"en\") or obj.get(\"mt_en\") or obj.get(\"mt\") or \"\"\n",
        "        if ar is None:\n",
        "            continue\n",
        "        norm.append({\"ar\": ar, \"mt_en\": mt_en})\n",
        "else:\n",
        "    raise ValueError(\"Unexpected lexicon format\")\n",
        "\n",
        "print(f\"Loaded {len(norm)} entries.\")\n",
        "print(\"Sample:\", norm[:10])\n"
      ],
      "metadata": {
        "id": "Xs-YjH8cm-21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import textwrap\n",
        "\n",
        "BATCH_SIZE = 150   # adjust if you want smaller/larger chunks\n",
        "\n",
        "def make_batches(items, batch_size):\n",
        "    for i in range(0, len(items), batch_size):\n",
        "        yield i // batch_size, items[i:i+batch_size]\n",
        "\n",
        "def build_prompt(batch_idx, total_batches, batch_items):\n",
        "    \"\"\"\n",
        "    Build a clear instruction prompt for the LLM.\n",
        "    \"\"\"\n",
        "    header = f\"\"\"You are helping me clean a bilingual lexicon for code-switched Egyptian Arabic ‚Üí English.\n",
        "\n",
        "TASK:\n",
        "- I will give you a JSON array of objects. Each object has:\n",
        "  - \"ar\": an Egyptian Arabic word or short phrase (informal, social media style).\n",
        "  - \"mt_en\": an approximate or incorrect machine translation.\n",
        "- Your job is to produce a new JSON array where:\n",
        "  - You KEEP the \"ar\" field exactly as it is.\n",
        "  - You REPLACE \"mt_en\" with a better English translation in a new field \"en\".\n",
        "  - The translation should be:\n",
        "    - Short (1‚Äì3 English words).\n",
        "    - The most common / natural meaning in everyday speech and social media.\n",
        "    - Neutral, not overly formal.\n",
        "    - If the Arabic is a named entity or proper noun, translate to its common English form (or keep as is if there's no translation).\n",
        "    - Do NOT include explanations, transliterations, or Arabic text in \"en\".\n",
        "\n",
        "FORMAT:\n",
        "- Input:  JSON array with fields: \"ar\", \"mt_en\"\n",
        "- Output: JSON array with fields: \"ar\", \"en\"\n",
        "- IMPORTANT:\n",
        "  - Same number of items in the same order.\n",
        "  - Only change the translation.\n",
        "  - Do NOT add comments or any text around the JSON.\n",
        "\n",
        "Now here is batch {batch_idx + 1} of {total_batches} as JSON:\n",
        "\"\"\"\n",
        "\n",
        "    json_block = json.dumps(batch_items, ensure_ascii=False, indent=2)\n",
        "    return header + \"\\n\" + json_block\n",
        "\n",
        "\n",
        "batches = list(make_batches(norm, BATCH_SIZE))\n",
        "print(f\"Total batches: {len(batches)}\")\n",
        "\n",
        "# Show the first prompt as example:\n",
        "idx0, batch0 = batches[0]\n",
        "prompt0 = build_prompt(idx0, len(batches), batch0)\n",
        "print(prompt0[:2000])  # preview first 2000 chars\n"
      ],
      "metadata": {
        "id": "o65bvso5nHpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, batch_items in batches:\n",
        "    prompt = build_prompt(idx, len(batches), batch_items)\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"=== BATCH {idx+1} / {len(batches)} ===\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "    print(prompt)\n",
        "    # You manually copy this, paste into ChatGPT, and save the returned JSON somewhere.\n",
        "    # Then go to the next batch.\n"
      ],
      "metadata": {
        "id": "px6iLrq5n_PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "fp = Path(\"/content/drive/MyDrive/cs-senti/data/ling/ar_en_lex_llm_fixed.jsonl\")\n",
        "\n",
        "with fp.open(encoding=\"utf-8\") as f:\n",
        "    rows = [json.loads(line) for line in f if line.strip()]\n",
        "\n",
        "print(\"Total entries:\", len(rows))\n",
        "print(\"Sample:\", rows[:10])\n"
      ],
      "metadata": {
        "id": "4Cn3cP9z4HkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/cs-senti\")\n",
        "DATA = BASE / \"data\"\n",
        "LING = DATA / \"ling\"\n",
        "LING.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"BASE:\", BASE)\n",
        "print(\"DATA:\", DATA)\n",
        "print(\"LING:\", LING)\n"
      ],
      "metadata": {
        "id": "V1K9btfb7Ew7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, random\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "LING = DATA / \"ling\"\n",
        "\n",
        "FP_HOST_TRAIN = LING / \"host_train_skeleton_all.jsonl\"\n",
        "FP_LEX_LLM    = LING / \"ar_en_lex_llm_fixed.jsonl\"   # <- your cleaned file\n",
        "FP_TRAIN_SW   = LING / \"host_train_switched_lex_llm.jsonl\"\n",
        "\n",
        "# 1) load host skeleton\n",
        "host_rows = []\n",
        "with open(FP_HOST_TRAIN, encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        host_rows.append(json.loads(line))\n",
        "\n",
        "print(\"Loaded host skeleton rows:\", len(host_rows))\n",
        "\n",
        "# 2) load LLM lexicon into dict: ar_token -> en_phrase\n",
        "lex_llm = {}\n",
        "with open(FP_LEX_LLM, encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        obj = json.loads(line)\n",
        "        ar = str(obj.get(\"ar\", \"\")).strip()\n",
        "        en = str(obj.get(\"en\", \"\")).strip()\n",
        "        if not ar or not en:\n",
        "            continue\n",
        "        # simple filter: must contain at least one Latin letter\n",
        "        if not any(c.isalpha() and c.lower() in \"abcdefghijklmnopqrstuvwxyz\" for c in en):\n",
        "            continue\n",
        "        # strip quotes\n",
        "        en = en.replace('\"', '').replace(\"'\", \"\").strip()\n",
        "        lex_llm[ar] = en\n",
        "\n",
        "print(\"Lexicon entries:\", len(lex_llm))\n",
        "sample_items = list(lex_llm.items())[:10]\n",
        "print(\"Sample lexicon items:\")\n",
        "for ar, en in sample_items:\n",
        "    print(\" \", ar, \"‚Üí\", en)\n",
        "\n",
        "# 3) apply switching\n",
        "switched_rows = []\n",
        "\n",
        "for r in host_rows:\n",
        "    toks = r[\"tokens\"]\n",
        "    label = r[\"label\"]\n",
        "    domain = r[\"domain\"]\n",
        "    cand_idx = r[\"cand_indices\"]\n",
        "\n",
        "    # keep only candidates that exist in lexicon\n",
        "    valid_idx = [i for i in cand_idx if toks[i] in lex_llm]\n",
        "    if not valid_idx:\n",
        "        continue\n",
        "\n",
        "    # choose 1‚Äì2 random positions to switch (avoid over-switching)\n",
        "    k = random.randint(1, min(2, len(valid_idx)))\n",
        "    chosen = random.sample(valid_idx, k)\n",
        "\n",
        "    new_toks = toks[:]\n",
        "    switches = []\n",
        "    for idx in chosen:\n",
        "        src = toks[idx]\n",
        "        tgt = lex_llm[src]\n",
        "        new_toks[idx] = tgt\n",
        "        switches.append({\"idx\": idx, \"src\": src, \"tgt\": tgt})\n",
        "\n",
        "    switched_text = \" \".join(new_toks)\n",
        "\n",
        "    switched_rows.append({\n",
        "        \"orig_text\": r[\"text\"],\n",
        "        \"switched_text\": switched_text,\n",
        "        \"label\": label,\n",
        "        \"domain\": domain,\n",
        "        \"switches\": switches\n",
        "    })\n",
        "\n",
        "print(\"Total switched rows:\", len(switched_rows))\n",
        "print(\"Label dist:\", Counter([r[\"label\"] for r in switched_rows]))\n",
        "print(\"Domain dist:\", Counter([r[\"domain\"] for r in switched_rows]))\n",
        "\n",
        "# 4) save\n",
        "with open(FP_TRAIN_SW, \"w\", encoding=\"utf-8\") as f:\n",
        "    for r in switched_rows:\n",
        "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"‚úÖ Saved switched TRAIN to:\", FP_TRAIN_SW)\n",
        "\n",
        "# 5) peek a few examples\n",
        "for ex in switched_rows[:5]:\n",
        "    print(\"\\nLABEL :\", ex[\"label\"], \"| DOMAIN:\", ex[\"domain\"])\n",
        "    print(\"ORIG  :\", ex[\"orig_text\"])\n",
        "    print(\"SW    :\", ex[\"switched_text\"])\n",
        "    print(\"SWAPS :\", ex[\"switches\"])\n"
      ],
      "metadata": {
        "id": "pixNiFKT64mH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, random\n",
        "from pathlib import Path\n",
        "\n",
        "FP_SWITCHED_TRAIN = Path(\"/content/drive/MyDrive/cs-senti/data/ling/host_train_switched_lex_llm.jsonl\")\n",
        "\n",
        "# load all switched rows\n",
        "switched = []\n",
        "with open(FP_SWITCHED_TRAIN, encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        if line.strip():\n",
        "            switched.append(json.loads(line))\n",
        "\n",
        "print(\"Total switched rows:\", len(switched))\n",
        "\n",
        "def show_examples(domain, k=5):\n",
        "    print(f\"\\n===== DOMAIN: {domain} =====\")\n",
        "    rows = [r for r in switched if r.get(\"domain\") == domain]\n",
        "    print(f\"Found {len(rows)} rows.\")\n",
        "    for ex in random.sample(rows, min(k, len(rows))):\n",
        "        print(\"\\nLABEL :\", ex.get(\"label\"))\n",
        "        print(\"ORIG  :\", ex.get(\"orig_text\", ex.get(\"text\", \"\")))\n",
        "        print(\"SW    :\", ex.get(\"switched_text\", ex.get(\"sw\", \"\")))\n",
        "        print(\"SWAPS :\", ex.get(\"swaps\", []))\n",
        "\n",
        "# show some from each domain\n",
        "show_examples(\"eesa\", k=5)\n",
        "show_examples(\"amg\",  k=5)\n",
        "show_examples(\"mr\",   k=5)\n"
      ],
      "metadata": {
        "id": "mNASfp3d8V5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to train a new XLM-R on:\n",
        "\n",
        "Train (AUG): mixed_train (real) + host_train_switched_lex_llm (synthetic)\n",
        "\n",
        "Dev: same mixed_dev (real only, no synthetic)\n",
        "\n",
        "Test: same eesa_test and mixed_test (unchanged!)\n",
        "\n",
        "Then compare:\n",
        "\n",
        "Baseline (Real only) vs Augmented (Real + Lex-switched)\n",
        "‚Üí on the same test sets.\n",
        "\n",
        "Your sa_mixed_v3_frozen copy stays as the frozen oracle for GAN rewards. We‚Äôre training a new classifier for the augmentation experiment."
      ],
      "metadata": {
        "id": "u7qI5hA0AiXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, random\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/cs-senti\")\n",
        "DATA = BASE / \"data\"\n",
        "LING = DATA / \"ling\"\n",
        "MODELS = BASE / \"models\"\n",
        "\n",
        "def read_jsonl(fp: Path):\n",
        "    rows = []\n",
        "    with open(fp, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "# 1) Real mixed train/dev\n",
        "FP_MIX_TR = DATA / \"mixed_train.jsonl\"\n",
        "FP_MIX_DE = DATA / \"mixed_dev.jsonl\"\n",
        "\n",
        "mix_train = read_jsonl(FP_MIX_TR)\n",
        "mix_dev   = read_jsonl(FP_MIX_DE)\n",
        "\n",
        "print(\"Real mixed train:\", len(mix_train))\n",
        "print(\"Real mixed dev  :\", len(mix_dev))\n",
        "\n",
        "# 2) Lex-switched train (EESA+AMG+MR)\n",
        "FP_SWITCHED_TRAIN = LING / \"host_train_switched_lex_llm.jsonl\"\n",
        "switched_raw = read_jsonl(FP_SWITCHED_TRAIN)\n",
        "print(\"Switched train raw:\", len(switched_raw))\n",
        "\n",
        "# normalise the structure -> {text, label}\n",
        "def norm_switched(r):\n",
        "    # we stored both original text and switched text; prefer switched\n",
        "    txt = r.get(\"switched_text\") or r.get(\"sw\", r.get(\"text\", \"\"))\n",
        "    return {\n",
        "        \"text\": txt,\n",
        "        \"label\": r[\"label\"]\n",
        "    }\n",
        "\n",
        "switched = [norm_switched(r) for r in switched_raw if r.get(\"label\")]\n",
        "\n",
        "print(\"Switched usable:\", len(switched))\n",
        "print(\"Label dist (switched):\", Counter([r[\"label\"] for r in switched]))\n",
        "\n",
        "# 3) Build augmented training set\n",
        "aug_train = mix_train + switched\n",
        "print(\"\\nAugmented TRAIN size:\", len(aug_train))\n",
        "print(\"Label dist (real train):\", Counter([r[\"label\"] for r in mix_train]))\n",
        "print(\"Label dist (aug train) :\", Counter([r[\"label\"] for r in aug_train]))\n"
      ],
      "metadata": {
        "id": "Mpj8BXV_Aefh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "LABELS = [\"pos\", \"neu\", \"neg\"]  # same order you used earlier\n",
        "label2id = {l: i for i, l in enumerate(LABELS)}\n",
        "id2label = {i: l for l, i in label2id.items()}\n",
        "\n",
        "class JsonlDS(Dataset):\n",
        "    def __init__(self, rows, tok, max_len=160):\n",
        "        self.rows = rows\n",
        "        self.tok = tok\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        x = self.rows[i]\n",
        "        enc = self.tok(\n",
        "            x[\"text\"],\n",
        "            max_length=self.max_len,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "        enc[\"labels\"] = label2id[x[\"label\"]]\n",
        "        return {k: torch.tensor(v) for k, v in enc.items()}\n",
        "\n",
        "def eval_loop(model, dataloader):\n",
        "    model.eval()\n",
        "    preds, gold = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            labels = batch[\"labels\"].numpy().tolist()\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            logits = model(**batch).logits.detach().cpu().numpy()\n",
        "            preds.extend(logits.argmax(axis=1).tolist())\n",
        "            gold.extend(labels)\n",
        "    macro = f1_score(gold, preds, average=\"macro\")\n",
        "    return gold, preds, macro\n"
      ],
      "metadata": {
        "id": "AgdAHQgqAtJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer + model from base XLM-R (same as baseline setup)\n",
        "tok_aug = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "model_aug = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"xlm-roberta-base\",\n",
        "    num_labels=3,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ").to(device)\n",
        "\n",
        "# datasets / loaders\n",
        "ds_tr_aug = JsonlDS(aug_train, tok_aug, 160)\n",
        "ds_de     = JsonlDS(mix_dev,   tok_aug, 160)\n",
        "\n",
        "dl_tr_aug = DataLoader(ds_tr_aug, batch_size=16, shuffle=True)\n",
        "dl_de     = DataLoader(ds_de,     batch_size=32, shuffle=False)\n",
        "\n",
        "EPOCHS = 3\n",
        "optim = AdamW(model_aug.parameters(), lr=2e-5)\n",
        "best_f1 = -1.0\n",
        "best_state = None\n",
        "\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    model_aug.train()\n",
        "    for batch in dl_tr_aug:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        out = model_aug(**batch)\n",
        "        out.loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model_aug.parameters(), 1.0)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "\n",
        "    _, _, f1_dev = eval_loop(model_aug, dl_de)\n",
        "    print(f\"Epoch {ep} ‚Üí dev macro-F1 (augmented) = {f1_dev:.4f}\")\n",
        "    if f1_dev > best_f1:\n",
        "        best_f1 = f1_dev\n",
        "        best_state = model_aug.state_dict().copy()\n",
        "\n",
        "# restore best dev checkpoint and save it\n",
        "model_aug.load_state_dict(best_state)\n",
        "out_dir_aug = MODELS / \"xlmr_sentiment_eesa_amg_mr_lexaug\"\n",
        "out_dir_aug.mkdir(parents=True, exist_ok=True)\n",
        "model_aug.save_pretrained(out_dir_aug.as_posix())\n",
        "tok_aug.save_pretrained(out_dir_aug.as_posix())\n",
        "print(\"‚úÖ Saved AUGMENTED model to:\", out_dir_aug)\n",
        "print(\"Best dev Macro-F1 (aug):\", best_f1)\n"
      ],
      "metadata": {
        "id": "vQxRxzOaBQf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1) Reload test sets ---\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/cs-senti\")\n",
        "DATA = BASE / \"data\"\n",
        "\n",
        "def read_jsonl(fp: Path):\n",
        "    rows = []\n",
        "    with open(fp, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "# EESA held-out test\n",
        "FP_EESA_TE = DATA / \"eesa_test.jsonl\"\n",
        "eesa_te = read_jsonl(FP_EESA_TE)\n",
        "\n",
        "# Mixed cross-domain test (you saved it earlier)\n",
        "FP_MIX_TEST = DATA / \"mixed_test.jsonl\"\n",
        "mixed_test = read_jsonl(FP_MIX_TEST)\n",
        "\n",
        "print(\"EESA test:\", len(eesa_te))\n",
        "print(\"Mixed test:\", len(mixed_test))\n"
      ],
      "metadata": {
        "id": "4p0KvtfN8dXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "LABELS = [\"pos\",\"neu\",\"neg\"]\n",
        "label2id = {l:i for i,l in enumerate(LABELS)}\n",
        "id2label = {i:l for l,i in label2id.items()}\n",
        "\n",
        "class JsonlDS(Dataset):\n",
        "    def __init__(self, rows, tok, max_len=160):\n",
        "        self.rows = rows\n",
        "        self.tok = tok\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        x = self.rows[i]\n",
        "        enc = self.tok(\n",
        "            x[\"text\"],\n",
        "            max_length=self.max_len,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "        enc[\"labels\"] = label2id[x[\"label\"]]\n",
        "        return {k: torch.tensor(v) for k,v in enc.items()}\n"
      ],
      "metadata": {
        "id": "P_By8yF78g1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "AUG_DIR = BASE / \"models\" / \"xlmr_sentiment_eesa_amg_mr_lexaug\"\n",
        "\n",
        "tok_aug = AutoTokenizer.from_pretrained(AUG_DIR.as_posix())\n",
        "model_aug = AutoModelForSequenceClassification.from_pretrained(\n",
        "    AUG_DIR.as_posix()\n",
        ").to(device)\n",
        "\n",
        "# reuse eval_loop from your cell\n",
        "def eval_loop(model, dataloader):\n",
        "    model.eval()\n",
        "    preds, gold = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            labels = batch[\"labels\"].numpy().tolist()\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            logits = model(**batch).logits.detach().cpu().numpy()\n",
        "            preds.extend(logits.argmax(axis=1).tolist())\n",
        "            gold.extend(labels)\n",
        "    macro = f1_score(gold, preds, average=\"macro\")\n",
        "    return gold, preds, macro\n",
        "\n",
        "# build loaders\n",
        "ds_eesa_te = JsonlDS(eesa_te, tok_aug, 160)\n",
        "ds_mix_te  = JsonlDS(mixed_test, tok_aug, 160)\n",
        "\n",
        "dl_eesa_te = DataLoader(ds_eesa_te, batch_size=32, shuffle=False)\n",
        "dl_mix_te  = DataLoader(ds_mix_te,  batch_size=32, shuffle=False)\n",
        "\n",
        "# --- Evaluate on EESA-test ---\n",
        "gold_e, preds_e, macro_e = eval_loop(model_aug, dl_eesa_te)\n",
        "print(\"\\n=== XLM-R + LexAug on EESA TEST ===\")\n",
        "print(classification_report(gold_e, preds_e, target_names=LABELS, digits=4))\n",
        "print(\"Macro-F1 (EESA):\", macro_e)\n",
        "\n",
        "# --- Evaluate on Mixed-test ---\n",
        "gold_m, preds_m, macro_m = eval_loop(model_aug, dl_mix_te)\n",
        "print(\"\\n=== XLM-R + LexAug on MIXED TEST ===\")\n",
        "print(classification_report(gold_m, preds_m, target_names=LABELS, digits=4))\n",
        "print(\"Macro-F1 (MIXED):\", macro_m)\n"
      ],
      "metadata": {
        "id": "nOIUgu258l6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß™ Step 1 ‚Äì Build the supervised ‚Äúswitch decision‚Äù dataset"
      ],
      "metadata": {
        "id": "Uh7qB3mfTEZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/cs-senti\")\n",
        "DATA = BASE / \"data\"\n",
        "LING = DATA / \"ling\"\n",
        "\n",
        "FP_SWITCHED_TRAIN = LING / \"host_train_switched_lex_llm.jsonl\"\n",
        "\n",
        "# use the actual file name you see in Drive\n",
        "FP_LEXICON = LING / \"ar_en_lex_llm_fixed.jsonl\"   # or .json if that's the extension\n",
        "\n",
        "def read_jsonl(fp: Path):\n",
        "    rows = []\n",
        "    with open(fp, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "def load_lexicon_flexible(fp: Path):\n",
        "    \"\"\"Handle both JSON array and JSONL formats.\"\"\"\n",
        "    with open(fp, encoding=\"utf-8\") as f:\n",
        "        txt = f.read().strip()\n",
        "    # Case 1: starts with '[' ‚Üí JSON array\n",
        "    if txt.startswith(\"[\"):\n",
        "        return json.loads(txt)\n",
        "    # Case 2: JSONL ‚Üí one JSON obj per line\n",
        "    lex = []\n",
        "    for line in txt.splitlines():\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        lex.append(json.loads(line))\n",
        "    return lex\n",
        "\n",
        "# 1) load switched rows\n",
        "switched_rows = read_jsonl(FP_SWITCHED_TRAIN)\n",
        "print(\"Loaded switched rows:\", len(switched_rows))\n",
        "\n",
        "# 2) load lexicon (LLM-corrected, any format)\n",
        "lex_list = load_lexicon_flexible(FP_LEXICON)\n",
        "lexicon = {e[\"ar\"]: e[\"en\"] for e in lex_list}\n",
        "print(\"Lexicon size:\", len(lexicon))\n"
      ],
      "metadata": {
        "id": "iaFarPHKTA7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "switch_examples = []\n",
        "\n",
        "for r in switched_rows:\n",
        "    text   = r.get(\"text\", \"\").strip()\n",
        "    label  = r.get(\"label\", \"neu\")\n",
        "    domain = r.get(\"domain\", \"unk\")\n",
        "    toks   = r.get(\"tokens\", [])\n",
        "    swaps  = r.get(\"swaps\", [])\n",
        "\n",
        "    if not text or not toks:\n",
        "        continue\n",
        "\n",
        "    # indices that were actually switched in your lex-aug data\n",
        "    switched_idx = {s[\"idx\"] for s in swaps} if swaps else set()\n",
        "\n",
        "    # candidate indices = tokens that exist in lexicon\n",
        "    cand_idx = [i for i, tok in enumerate(toks) if tok in lexicon]\n",
        "\n",
        "    if not cand_idx:\n",
        "        continue\n",
        "\n",
        "    for idx in cand_idx:\n",
        "        if idx < 0 or idx >= len(toks):\n",
        "            continue\n",
        "        switch_examples.append({\n",
        "            \"text\": text,\n",
        "            \"label\": label,\n",
        "            \"domain\": domain,\n",
        "            \"token\": toks[idx],\n",
        "            \"token_idx\": idx,\n",
        "            \"switch\": 1 if idx in switched_idx else 0\n",
        "        })\n",
        "\n",
        "print(\"Total switch examples:\", len(switch_examples))\n",
        "print(\"Switch label dist:\", Counter([e[\"switch\"] for e in switch_examples]))\n",
        "print(\"Domain dist:\", Counter([e[\"domain\"] for e in switch_examples]))\n",
        "\n",
        "for ex in switch_examples[:5]:\n",
        "    print(\"\\nTEXT :\", ex[\"text\"])\n",
        "    print(\"CAND  :\", ex[\"token\"], \"@\", ex[\"token_idx\"])\n",
        "    print(\"SWITCH:\", ex[\"switch\"])\n",
        "    print(\"LBL   :\", ex[\"label\"], \"| DOMAIN:\", ex[\"domain\"])\n"
      ],
      "metadata": {
        "id": "q2E5IOfKUA0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import islice\n",
        "for r in islice(switched_rows, 3):\n",
        "    print(r.keys())\n",
        "    print(r)\n",
        "    print(\"-\"*50)\n"
      ],
      "metadata": {
        "id": "G6klWfdMXi_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, random\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/cs-senti\")\n",
        "DATA = BASE / \"data\"\n",
        "LING = DATA / \"ling\"\n",
        "\n",
        "FP_HOST_TRAIN = LING / \"host_train_skeleton_all.jsonl\"\n",
        "FP_LEXICON    = LING / \"ar_en_lex_llm_fixed.jsonl\"\n",
        "\n",
        "# ---------- helpers ----------\n",
        "\n",
        "def read_jsonl(fp: Path):\n",
        "    rows = []\n",
        "    with open(fp, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "def load_lexicon_flexible(fp: Path):\n",
        "    \"\"\"Handle both JSON array and JSONL lexicon formats.\"\"\"\n",
        "    with open(fp, encoding=\"utf-8\") as f:\n",
        "        txt = f.read().strip()\n",
        "    if txt.startswith(\"[\"):\n",
        "        return json.loads(txt)\n",
        "    lex = []\n",
        "    for line in txt.splitlines():\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        lex.append(json.loads(line))\n",
        "    return lex\n",
        "\n",
        "# ---------- 1) load host skeleton + lexicon ----------\n",
        "\n",
        "host_rows = read_jsonl(FP_HOST_TRAIN)\n",
        "print(\"Loaded host skeleton rows:\", len(host_rows))\n",
        "\n",
        "lex_list = load_lexicon_flexible(FP_LEXICON)\n",
        "lexicon  = {e[\"ar\"]: e[\"en\"] for e in lex_list}\n",
        "print(\"Lexicon entries:\", len(lexicon))\n",
        "\n",
        "# ---------- 2) build switched sentences + token-level labels ----------\n",
        "\n",
        "switched_rows   = []\n",
        "switch_examples = []\n",
        "\n",
        "for r in host_rows:\n",
        "    text   = r.get(\"text\", \"\").strip()\n",
        "    label  = r.get(\"label\", \"neu\")\n",
        "    domain = r.get(\"domain\", \"unk\")\n",
        "    toks   = r.get(\"tokens\", [])\n",
        "    cand   = r.get(\"cand_indices\", [])\n",
        "\n",
        "    if not text or not toks or not cand:\n",
        "        continue\n",
        "\n",
        "    # keep only candidates that actually exist in lexicon\n",
        "    cand_lex = [i for i in cand if 0 <= i < len(toks) and toks[i] in lexicon]\n",
        "    if not cand_lex:\n",
        "        continue\n",
        "\n",
        "    # decide how many to switch (1‚Äì3 or all if fewer)\n",
        "    max_sw = min(3, len(cand_lex))\n",
        "    n_sw   = random.randint(1, max_sw)\n",
        "    sw_idx = set(random.sample(cand_lex, n_sw))\n",
        "\n",
        "    new_toks = toks[:]  # shallow copy\n",
        "    swaps = []\n",
        "\n",
        "    for i in cand_lex:\n",
        "        src_tok = toks[i]\n",
        "        if src_tok not in lexicon:\n",
        "            continue\n",
        "        if i in sw_idx:\n",
        "            # actually switch\n",
        "            tgt_tok = lexicon[src_tok]\n",
        "            new_toks[i] = tgt_tok\n",
        "            swaps.append({\"idx\": i, \"src\": src_tok, \"tgt\": tgt_tok})\n",
        "            sw_label = 1\n",
        "        else:\n",
        "            # candidate but we keep it Arabic\n",
        "            tgt_tok = None\n",
        "            sw_label = 0\n",
        "\n",
        "        # record token-level supervision\n",
        "        switch_examples.append({\n",
        "            \"text\": text,\n",
        "            \"label\": label,\n",
        "            \"domain\": domain,\n",
        "            \"token\": src_tok,\n",
        "            \"token_idx\": i,\n",
        "            \"switch\": sw_label\n",
        "        })\n",
        "\n",
        "    # build final switched sentence row\n",
        "    switched_rows.append({\n",
        "        \"orig_text\": text,\n",
        "        \"switched_text\": \" \".join(new_toks),\n",
        "        \"label\": label,\n",
        "        \"domain\": domain,\n",
        "        \"tokens\": toks,\n",
        "        \"cand_indices\": cand_lex,\n",
        "        \"switched_indices\": sorted(list(sw_idx)),\n",
        "        \"swaps\": swaps\n",
        "    })\n",
        "\n",
        "print(\"\\nSwitched sentence rows:\", len(switched_rows))\n",
        "print(\"Token-level examples:\", len(switch_examples))\n",
        "print(\"Switch label dist:\", Counter([e[\"switch\"] for e in switch_examples]))\n",
        "print(\"Domain dist:\", Counter([e[\"domain\"] for e in switch_examples]))\n",
        "\n",
        "# ---------- 3) save to disk ----------\n",
        "\n",
        "FP_SWITCHED_NEW  = LING / \"host_train_switched_new.jsonl\"\n",
        "FP_SWITCH_EXAMP  = LING / \"switch_examples.jsonl\"\n",
        "\n",
        "with open(FP_SWITCHED_NEW, \"w\", encoding=\"utf-8\") as f:\n",
        "    for r in switched_rows:\n",
        "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "with open(FP_SWITCH_EXAMP, \"w\", encoding=\"utf-8\") as f:\n",
        "    for e in switch_examples:\n",
        "        f.write(json.dumps(e, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"\\n‚úÖ Saved switched TRAIN to:\", FP_SWITCHED_NEW)\n",
        "print(\"‚úÖ Saved token-level switch examples to:\", FP_SWITCH_EXAMP)\n",
        "\n",
        "# quick peek at a couple of rows\n",
        "for r in switched_rows[:3]:\n",
        "    print(\"\\nLABEL :\", r[\"label\"], \"| DOMAIN:\", r[\"domain\"])\n",
        "    print(\"ORIG  :\", r[\"orig_text\"])\n",
        "    print(\"SW    :\", r[\"switched_text\"])\n",
        "    print(\"SWAPS :\", r[\"swaps\"])\n"
      ],
      "metadata": {
        "id": "dMWm7oiKXzBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/cs-senti\")\n",
        "DATA = BASE / \"data\"\n",
        "LING = DATA / \"ling\"\n",
        "\n",
        "FP_SWITCH_EX = LING / \"switch_examples.jsonl\"\n",
        "\n",
        "def read_jsonl(fp: Path):\n",
        "    rows = []\n",
        "    with open(fp, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "switch_rows = read_jsonl(FP_SWITCH_EX)\n",
        "print(\"Total token-level examples:\", len(switch_rows))\n",
        "\n",
        "print(\"Switch label dist:\", Counter([r[\"switch\"] for r in switch_rows]))\n",
        "print(\"Domain dist:\", Counter([r.get(\"domain\",\"unk\") for r in switch_rows]))\n",
        "\n",
        "# quick peek\n",
        "for ex in switch_rows[:5]:\n",
        "    print(\"\\nTEXT :\", ex[\"text\"])\n",
        "    print(\"TOKEN:\", ex[\"token\"], \"@\", ex[\"token_idx\"])\n",
        "    print(\"SWITCH:\", ex[\"switch\"], \"| LABEL:\", ex[\"label\"], \"| DOMAIN:\", ex[\"domain\"])\n"
      ],
      "metadata": {
        "id": "oZ71hhhm4XQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we will use all domains together (eesa+amg+mr)\n",
        "# you can filter by domain later if you want\n",
        "\n",
        "train_rows, dev_rows = train_test_split(\n",
        "    switch_rows,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=[r[\"switch\"] for r in switch_rows]  # keep 0/1 balance\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(train_rows))\n",
        "print(\"Dev size  :\", len(dev_rows))\n",
        "\n",
        "print(\"Train switch dist:\", Counter([r[\"switch\"] for r in train_rows]))\n",
        "print(\"Dev switch dist  :\", Counter([r[\"switch\"] for r in dev_rows]))\n"
      ],
      "metadata": {
        "id": "LeCUjPTm4e00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "class SwitchDecisionDS(Dataset):\n",
        "    def __init__(self, rows, tok, max_len=160):\n",
        "        self.rows = rows\n",
        "        self.tok = tok\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rows)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        r = self.rows[i]\n",
        "        text = r[\"text\"]\n",
        "        tok_str = r[\"token\"]\n",
        "        idx = r[\"token_idx\"]\n",
        "\n",
        "        # very simple: re-tokenize text by space to put markers\n",
        "        # (we rely on the \"tokens\" that produced token_idx, but here we only have text + token_idx)\n",
        "        # we approximate by splitting on spaces:\n",
        "        words = text.split()\n",
        "        if 0 <= idx < len(words):\n",
        "            words_marked = (\n",
        "                words[:idx] +\n",
        "                [\"<SW>\", words[idx], \"</SW>\"] +\n",
        "                words[idx+1:]\n",
        "            )\n",
        "            marked_text = \" \".join(words_marked)\n",
        "        else:\n",
        "            # fallback: no marking if idx weird\n",
        "            marked_text = text\n",
        "\n",
        "        label = int(r[\"switch\"])  # 0 or 1\n",
        "\n",
        "        enc = self.tok(\n",
        "            marked_text,\n",
        "            max_length=self.max_len,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )\n",
        "        enc[\"labels\"] = label\n",
        "        return {k: torch.tensor(v) for k, v in enc.items()}\n",
        "\n",
        "# binary labels\n",
        "id2label = {0: \"no_switch\", 1: \"switch\"}\n",
        "label2id = {\"no_switch\": 0, \"switch\": 1}\n",
        "\n",
        "tok_sw = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "model_sw = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"xlm-roberta-base\",\n",
        "    num_labels=2,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ").to(device)\n",
        "\n",
        "ds_sw_tr = SwitchDecisionDS(train_rows, tok_sw, max_len=160)\n",
        "ds_sw_de = SwitchDecisionDS(dev_rows,   tok_sw, max_len=160)\n",
        "\n",
        "dl_sw_tr = DataLoader(ds_sw_tr, batch_size=16, shuffle=True)\n",
        "dl_sw_de = DataLoader(ds_sw_de, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "4kEf-TqX4kG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "def eval_switch(model, dataloader):\n",
        "    model.eval()\n",
        "    preds, gold = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            labels = batch[\"labels\"].numpy().tolist()\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            logits = model(**batch).logits.detach().cpu().numpy()\n",
        "            preds.extend(logits.argmax(axis=1).tolist())\n",
        "            gold.extend(labels)\n",
        "    macro = f1_score(gold, preds, average=\"macro\")\n",
        "    return gold, preds, macro\n",
        "\n",
        "EPOCHS = 3\n",
        "optim = AdamW(model_sw.parameters(), lr=2e-5)\n",
        "best_f1 = -1.0\n",
        "best_state = None\n",
        "\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    model_sw.train()\n",
        "    for batch in dl_sw_tr:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        out = model_sw(**batch)\n",
        "        out.loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model_sw.parameters(), 1.0)\n",
        "        optim.step()\n",
        "        optim.zero_grad()\n",
        "\n",
        "    _, _, f1_dev = eval_switch(model_sw, dl_sw_de)\n",
        "    print(f\"Epoch {ep} ‚Üí dev macro-F1 (switch decision) = {f1_dev:.4f}\")\n",
        "    if f1_dev > best_f1:\n",
        "        best_f1 = f1_dev\n",
        "        best_state = model_sw.state_dict().copy()\n",
        "\n",
        "# restore best and save\n",
        "model_sw.load_state_dict(best_state)\n",
        "out_dir_sw = MODELS / \"xlmr_switch_decider_lexsupervised\"\n",
        "out_dir_sw.mkdir(parents=True, exist_ok=True)\n",
        "model_sw.save_pretrained(out_dir_sw.as_posix())\n",
        "tok_sw.save_pretrained(out_dir_sw.as_posix())\n",
        "print(\"‚úÖ Saved switch-decider model to:\", out_dir_sw)\n",
        "print(\"Best dev Macro-F1:\", best_f1)\n"
      ],
      "metadata": {
        "id": "9BHJz-Du4nSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "gold, preds, macro = eval_switch(model_sw, dl_sw_de)\n",
        "print(\"\\n=== Switch decision dev report ===\")\n",
        "print(classification_report(gold, preds, target_names=[\"no_switch\",\"switch\"], digits=4))\n",
        "print(\"Macro-F1:\", macro)\n"
      ],
      "metadata": {
        "id": "cOZBLvFwl9sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 1: imports & paths\n",
        "from pathlib import Path\n",
        "import json, random\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "# fix seed\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "BASE   = Path(\"/content/drive/MyDrive/cs-senti\")\n",
        "DATA   = BASE / \"data\"\n",
        "LING   = DATA / \"ling\"\n",
        "MODELS = BASE / \"models\"\n",
        "\n",
        "FP_SWITCHED_TRAIN = LING / \"host_train_switched_new.jsonl\"\n",
        "\n",
        "def read_jsonl(fp: Path):\n",
        "    rows = []\n",
        "    with open(fp, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "    return rows\n"
      ],
      "metadata": {
        "id": "suAaxGplEHIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/cs-senti\")\n",
        "DATA = BASE / \"data\"\n",
        "LING = DATA / \"ling\"\n",
        "\n",
        "FP_HOST_SKELETON = LING / \"host_train_switched_new.jsonl\"\n",
        "FP_LEXICON       = LING / \"ar_en_lex_llm_fixed_clean.json\"\n",
        "FP_SW_TRAIN      = LING / \"host_train_switched_supervised.jsonl\"\n",
        "FP_TOK_EXAMPLES  = LING / \"switch_examples_supervised.jsonl\"\n",
        "\n",
        "def read_jsonl(fp: Path):\n",
        "    rows = []\n",
        "    with open(fp, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "print(\"Loading host skeleton...\")\n",
        "host_rows = read_jsonl(FP_HOST_SKELETON)\n",
        "print(\"Loaded host skeleton rows:\", len(host_rows))\n",
        "\n",
        "print(\"Loading lexicon...\")\n",
        "lex_list = json.load(open(FP_LEXICON, encoding=\"utf-8\"))\n",
        "lexicon = {e[\"ar\"]: e[\"en\"] for e in lex_list}\n",
        "print(\"Lexicon entries:\", len(lexicon))\n",
        "\n",
        "switched_rows = []\n",
        "token_level_examples = []\n",
        "\n",
        "for r in host_rows:\n",
        "    text   = r.get(\"text\", \"\").strip()\n",
        "    label  = r.get(\"label\", \"neu\")\n",
        "    domain = r.get(\"domain\", \"unk\")\n",
        "    toks   = r.get(\"tokens\", [])\n",
        "\n",
        "    if not text or not toks:\n",
        "        continue\n",
        "\n",
        "    sw_tokens = []\n",
        "    swaps     = []\n",
        "\n",
        "    for i, tok in enumerate(toks):\n",
        "        if tok in lexicon:\n",
        "            en = lexicon[tok]\n",
        "            sw_tokens.append(en)\n",
        "            swaps.append({\"idx\": i, \"src\": tok, \"tgt\": en})\n",
        "        else:\n",
        "            sw_tokens.append(tok)\n",
        "\n",
        "    # require at least one actual swap\n",
        "    if not swaps:\n",
        "        continue\n",
        "\n",
        "    switched_text = \" \".join(sw_tokens)\n",
        "\n",
        "    switched_rows.append({\n",
        "        \"text\": text,\n",
        "        \"tokens\": toks,\n",
        "        \"switched_text\": switched_text,\n",
        "        \"sw_tokens\": sw_tokens,\n",
        "        \"swaps\": swaps,\n",
        "        \"label\": label,\n",
        "        \"domain\": domain\n",
        "    })\n",
        "\n",
        "    # also build token-level examples for possible later use\n",
        "    for i, tok in enumerate(toks):\n",
        "        token_level_examples.append({\n",
        "            \"text\": text,\n",
        "            \"label\": label,\n",
        "            \"domain\": domain,\n",
        "            \"token\": tok,\n",
        "            \"token_idx\": i,\n",
        "            \"switch\": 1 if any(s[\"idx\"] == i for s in swaps) else 0\n",
        "        })\n",
        "\n",
        "print(\"\\nSwitched sentence rows:\", len(switched_rows))\n",
        "print(\"Token-level examples:\", len(token_level_examples))\n",
        "print(\"Switch label dist:\", Counter([e[\"switch\"] for e in token_level_examples]))\n",
        "print(\"Domain dist:\", Counter([e[\"domain\"] for e in token_level_examples]))\n",
        "\n",
        "# save sentence-level supervised pairs\n",
        "with open(FP_SW_TRAIN, \"w\", encoding=\"utf-8\") as f:\n",
        "    for r in switched_rows:\n",
        "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "# save token-level examples\n",
        "with open(FP_TOK_EXAMPLES, \"w\", encoding=\"utf-8\") as f:\n",
        "    for e in token_level_examples:\n",
        "        f.write(json.dumps(e, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"\\n‚úÖ Saved switched TRAIN to:\", FP_SW_TRAIN)\n",
        "print(\"‚úÖ Saved token-level switch examples to:\", FP_TOK_EXAMPLES)\n",
        "\n",
        "# quick peek\n",
        "for ex in switched_rows[:3]:\n",
        "    print(\"\\nLABEL :\", ex[\"label\"], \"| DOMAIN:\", ex[\"domain\"])\n",
        "    print(\"ORIG  :\", ex[\"text\"])\n",
        "    print(\"SW    :\", ex[\"switched_text\"])\n",
        "    print(\"SWAPS :\", ex[\"swaps\"])\n",
        "    break\n"
      ],
      "metadata": {
        "id": "CHPP1wLOCWH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json, re\n",
        "\n",
        "FP = Path(\"/content/drive/MyDrive/cs-senti/data/ling/ar_en_lex_llm_fixed.jsonl\")\n",
        "\n",
        "print(\"Loading raw text...\")\n",
        "raw = open(FP, encoding=\"utf-8\").read()\n",
        "\n",
        "# --- AUTO-REPAIR LOGIC ---\n",
        "# 1) remove BOM\n",
        "raw = raw.lstrip(\"\\ufeff\")\n",
        "\n",
        "# 2) extract ALL JSON objects inside the file\n",
        "objects = re.findall(r\"\\{[^}]+\\}\", raw)\n",
        "\n",
        "print(f\"Found {len(objects)} entries.\")\n",
        "\n",
        "# 3) rebuild as proper JSON array\n",
        "fixed = [json.loads(obj) for obj in objects]\n",
        "\n",
        "# 4) save as valid JSON\n",
        "FP_FIXED = FP.parent / \"ar_en_lex_llm_fixed_clean.json\"\n",
        "json.dump(fixed, open(FP_FIXED, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"\\n‚úÖ Saved cleaned lexicon to:\", FP_FIXED)\n"
      ],
      "metadata": {
        "id": "JgOppuXTDAKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "lex = json.load(open(\"/content/drive/MyDrive/cs-senti/data/ling/ar_en_lex_llm_fixed_clean.json\"))\n",
        "print(\"Entries:\", len(lex))\n",
        "print(lex[:5])\n"
      ],
      "metadata": {
        "id": "7H1c9T9wDDAd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2: build supervised pairs from lexical-switched file\n",
        "\n",
        "rows = read_jsonl(FP_SWITCHED_TRAIN)\n",
        "print(\"Total rows in host_train_switched_new:\", len(rows))\n",
        "\n",
        "pairs = []\n",
        "for r in rows:\n",
        "    src = r.get(\"text\", \"\").strip()\n",
        "    tgt = r.get(\"switched_text\") or r.get(\"sw\", \"\").strip()\n",
        "    lab = r.get(\"label\", \"neu\").strip()\n",
        "\n",
        "    if not src or not tgt:\n",
        "        continue\n",
        "\n",
        "    # optional: filter out extremely short outputs\n",
        "    if len(tgt.split()) < 2:\n",
        "        continue\n",
        "\n",
        "    pairs.append({\n",
        "        \"src\": src,\n",
        "        \"tgt\": tgt,\n",
        "        \"label\": lab,\n",
        "        \"domain\": r.get(\"domain\", \"unk\")\n",
        "    })\n",
        "\n",
        "print(\"Usable pairs:\", len(pairs))\n",
        "print(\"Label dist:\", Counter([p[\"label\"] for p in pairs]))\n",
        "print(\"Domain dist:\", Counter([p[\"domain\"] for p in pairs]) )\n"
      ],
      "metadata": {
        "id": "5dv494k-_8-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/cs-senti\")\n",
        "DATA = BASE / \"data\"\n",
        "LING = DATA / \"ling\"\n",
        "\n",
        "FP_SWITCHED_NEW = LING / \"host_train_switched_new.jsonl\"\n",
        "\n",
        "def read_jsonl(fp: Path):\n",
        "    rows = []\n",
        "    with open(fp, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "switched_rows = read_jsonl(FP_SWITCHED_NEW)\n",
        "print(\"Total rows in host_train_switched_new:\", len(switched_rows))\n",
        "\n",
        "# peek structure of first few rows\n",
        "for i, r in enumerate(switched_rows[:3]):\n",
        "    print(f\"\\n--- ROW {i} ---\")\n",
        "    print(\"keys:\", list(r.keys()))\n",
        "    print(\"label :\", r.get(\"label\"))\n",
        "    print(\"domain:\", r.get(\"domain\"))\n",
        "    print(\"text  :\", r.get(\"text\"))\n",
        "    print(\"switched_text:\", r.get(\"switched_text\"))\n",
        "    print(\"tokens:\", r.get(\"tokens\")[:10] if isinstance(r.get(\"tokens\"), list) else None)\n",
        "    print(\"swaps :\", r.get(\"swaps\"))\n"
      ],
      "metadata": {
        "id": "BmQF8LE0Fdiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/cs-senti\")\n",
        "DATA = BASE / \"data\"\n",
        "LING = DATA / \"ling\"\n",
        "\n",
        "FP_SWITCHED_NEW = LING / \"host_train_switched_new.jsonl\"\n",
        "FP_GAN_PAIRS    = LING / \"gan_supervised_pairs.jsonl\"  # output file\n",
        "\n",
        "def read_jsonl(fp: Path):\n",
        "    rows = []\n",
        "    with open(fp, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "switched_rows = read_jsonl(FP_SWITCHED_NEW)\n",
        "print(\"Total rows in host_train_switched_new:\", len(switched_rows))\n",
        "\n",
        "pairs = []\n",
        "\n",
        "for r in switched_rows:\n",
        "    # FIX: use orig_text instead of text\n",
        "    src = (r.get(\"orig_text\") or \"\").strip()\n",
        "    tgt = (r.get(\"switched_text\") or \"\").strip()\n",
        "\n",
        "    label  = r.get(\"label\", \"neu\")\n",
        "    domain = r.get(\"domain\", \"unk\")\n",
        "\n",
        "    # sanity checks\n",
        "    if not src or not tgt:\n",
        "        continue\n",
        "    if src == tgt:\n",
        "        continue\n",
        "\n",
        "    # build a proper supervised pair\n",
        "    pairs.append({\n",
        "        \"host\": src,    # original monolingual-ish Arabic\n",
        "        \"cs\": tgt,      # lexical-switched target\n",
        "        \"label\": label,\n",
        "        \"domain\": domain\n",
        "    })\n",
        "\n",
        "print(\"\\nUsable pairs:\", len(pairs))\n",
        "print(\"Label dist:\", Counter(p[\"label\"] for p in pairs))\n",
        "print(\"Domain dist:\", Counter(p[\"domain\"] for p in pairs))\n",
        "\n",
        "for ex in pairs[:5]:\n",
        "    print(\"\\nLABEL :\", ex[\"label\"], \"| DOMAIN:\", ex[\"domain\"])\n",
        "    print(\"HOST  :\", ex[\"host\"])\n",
        "    print(\"CS    :\", ex[\"cs\"])\n",
        "\n",
        "# save supervised GAN pairs\n",
        "with open(FP_GAN_PAIRS, \"w\", encoding=\"utf-8\") as f:\n",
        "    for p in pairs:\n",
        "        f.write(json.dumps(p, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"\\n‚úÖ Saved GAN supervised pairs to:\", FP_GAN_PAIRS)\n"
      ],
      "metadata": {
        "id": "Jo6Y4iFHGFQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 1 ‚Äì Load GAN pairs & create train/dev splits\n",
        "\n",
        "This uses the file we just created:\n",
        "/content/drive/MyDrive/cs-senti/data/ling/gan_supervised_pairs.jsonl"
      ],
      "metadata": {
        "id": "cA8mavr7JFfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 1: load GAN supervised pairs + train/dev split ===\n",
        "import json, random\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/cs-senti\")\n",
        "DATA = BASE / \"data\"\n",
        "LING = DATA / \"ling\"\n",
        "\n",
        "FP_GAN_PAIRS = LING / \"gan_supervised_pairs.jsonl\"\n",
        "\n",
        "def read_jsonl(fp: Path):\n",
        "    rows = []\n",
        "    with open(fp, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "pairs = read_jsonl(FP_GAN_PAIRS)\n",
        "print(\"Total supervised CS pairs:\", len(pairs))\n",
        "print(\"Label dist:\", Counter(p[\"label\"] for p in pairs))\n",
        "print(\"Domain dist:\", Counter(p[\"domain\"] for p in pairs))\n",
        "\n",
        "# make sure each example has host + cs\n",
        "pairs = [p for p in pairs if p.get(\"host\") and p.get(\"cs\")]\n",
        "print(\"Usable pairs after sanity check:\", len(pairs))\n",
        "\n",
        "# train/dev split (e.g. 90/10, stratified by label)\n",
        "labels = [p[\"label\"] for p in pairs]\n",
        "train_pairs, dev_pairs = train_test_split(\n",
        "    pairs,\n",
        "    test_size=0.1,\n",
        "    random_state=42,\n",
        "    stratify=labels\n",
        ")\n",
        "\n",
        "print(\"\\nTrain size:\", len(train_pairs))\n",
        "print(\"Dev size  :\", len(dev_pairs))\n",
        "\n",
        "print(\"Train label dist:\", Counter(p[\"label\"] for p in train_pairs))\n",
        "print(\"Dev label dist  :\", Counter(p[\"label\"] for p in dev_pairs))\n",
        "\n",
        "# optionally save them (useful for reproducibility)\n",
        "FP_GAN_TR = LING / \"gan_pairs_train.jsonl\"\n",
        "FP_GAN_DE = LING / \"gan_pairs_dev.jsonl\"\n",
        "\n",
        "with open(FP_GAN_TR, \"w\", encoding=\"utf-8\") as f:\n",
        "    for r in train_pairs:\n",
        "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "with open(FP_GAN_DE, \"w\", encoding=\"utf-8\") as f:\n",
        "    for r in dev_pairs:\n",
        "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"\\n‚úÖ Saved:\")\n",
        "print(\" -\", FP_GAN_TR)\n",
        "print(\" -\", FP_GAN_DE)\n"
      ],
      "metadata": {
        "id": "YCpoSP0oH0aA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 2 ‚Äì Build a seq2seq dataset (mT5) for HOST ‚Üí CS\n",
        "\n",
        "Here we prepare data for a generator model.\n",
        "I‚Äôll use google/mt5-small (multilingual, lightweight enough for Colab)."
      ],
      "metadata": {
        "id": "tkoP0mecJBlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 2: build HF Datasets + tokenizer for mT5 ===\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_NAME = \"google/mt5-small\"\n",
        "\n",
        "tok_g = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# reload from disk to be safe\n",
        "train_pairs = read_jsonl(FP_GAN_TR)\n",
        "dev_pairs   = read_jsonl(FP_GAN_DE)\n",
        "\n",
        "print(\"Train pairs:\", len(train_pairs))\n",
        "print(\"Dev pairs  :\", len(dev_pairs))\n",
        "\n",
        "# Build HF datasets from Python lists\n",
        "ds_train = Dataset.from_list(train_pairs)\n",
        "ds_dev   = Dataset.from_list(dev_pairs)\n",
        "\n",
        "MAX_SRC_LEN = 64   # host sentence length\n",
        "MAX_TGT_LEN = 64   # cs sentence length\n",
        "\n",
        "def preprocess_fn(batch):\n",
        "    # inputs: host (Arabic-ish)\n",
        "    model_inputs = tok_g(\n",
        "        batch[\"host\"],\n",
        "        max_length=MAX_SRC_LEN,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "    # targets: cs (code-switched sentence)\n",
        "    with tok_g.as_target_tokenizer():\n",
        "        labels = tok_g(\n",
        "            batch[\"cs\"],\n",
        "            max_length=MAX_TGT_LEN,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\"\n",
        "        )[\"input_ids\"]\n",
        "\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs\n",
        "\n",
        "ds_train_tok = ds_train.map(preprocess_fn, batched=True, remove_columns=ds_train.column_names)\n",
        "ds_dev_tok   = ds_dev.map(preprocess_fn,   batched=True, remove_columns=ds_dev.column_names)\n",
        "\n",
        "print(ds_train_tok)\n",
        "print(ds_dev_tok)\n"
      ],
      "metadata": {
        "id": "6B994KO1H9Nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cell 3 ‚Äì Define the generator model + Trainer\n",
        "\n",
        "Now we create an mT5 generator and a Seq2SeqTrainer to learn HOST ‚Üí CS mapping."
      ],
      "metadata": {
        "id": "UUrM8TN9I81A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 3: define training arguments + trainer (SAFE VERSION) ===\n",
        "from transformers import (\n",
        "    MT5ForConditionalGeneration,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "\n",
        "# Load mT5-small model\n",
        "model_g = MT5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "\n",
        "OUT_DIR_G = MODELS / \"gan_stage1_generator_supervised\"\n",
        "OUT_DIR_G.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Collator ensures correct padding for seq2seq\n",
        "data_collator = DataCollatorForSeq2Seq(tok_g, model=model_g)\n",
        "\n",
        "# IMPORTANT: ONLY use arguments supported by your transformers version\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    output_dir=str(OUT_DIR_G),\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    warmup_steps=50,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        "    save_total_limit=1,\n",
        "    predict_with_generate=False,  # avoids errors in older versions\n",
        ")\n",
        "\n",
        "trainer_g = Seq2SeqTrainer(\n",
        "    model=model_g,\n",
        "    args=args,\n",
        "    train_dataset=ds_train_tok,\n",
        "    eval_dataset=ds_dev_tok,       # if you want no eval, set to None\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tok_g,\n",
        ")\n",
        "\n",
        "print(\"Trainer created successfully!\")\n"
      ],
      "metadata": {
        "id": "Q5nFHzl1I2LF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 4: train generator (supervised) ===\n",
        "\n",
        "# Disable W&B completely to avoid crashes\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"offline\"\n",
        "os.environ[\"WANDB_SILENT\"] = \"true\"\n",
        "\n",
        "print(\"üöÄ Training supervised generator...\")\n",
        "trainer_g.train()\n",
        "\n",
        "# Save final model\n",
        "trainer_g.save_model()\n",
        "tok_g.save_pretrained(OUT_DIR_G)\n",
        "\n",
        "print(\"‚úÖ Supervised generator saved to:\", OUT_DIR_G)\n"
      ],
      "metadata": {
        "id": "uR7IRiovMVBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quick sanity check: generate from the supervised generator\n",
        "\n",
        "Before going to GAN/RL stuff, we should see what it‚Äôs producing."
      ],
      "metadata": {
        "id": "e3z5_RphRUEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, MT5ForConditionalGeneration\n",
        "from pathlib import Path\n",
        "import json, random\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/cs-senti\")\n",
        "MODELS = BASE / \"models\"\n",
        "DATA = BASE / \"data\"\n",
        "LING = DATA / \"ling\"\n",
        "\n",
        "GEN_DIR = MODELS / \"gan_stage1_generator_supervised\"\n",
        "tok_g = AutoTokenizer.from_pretrained(GEN_DIR)\n",
        "gen_mdl = MT5ForConditionalGeneration.from_pretrained(GEN_DIR).to(\"cuda\")\n",
        "\n",
        "# load a few host sentences to test (the same host we used to build pairs)\n",
        "FP_GAN_TR = LING / \"gan_pairs_train.jsonl\"   # from earlier\n",
        "def read_jsonl(fp):\n",
        "    rows=[]\n",
        "    with open(fp, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line=line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "pairs = read_jsonl(FP_GAN_TR)\n",
        "print(\"Total train pairs:\", len(pairs))\n",
        "\n",
        "# sample a few\n",
        "for ex in random.sample(pairs, 5):\n",
        "    host = ex[\"host\"]\n",
        "    gold_cs = ex[\"cs\"]\n",
        "    inputs = tok_g(\n",
        "        host,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=64,\n",
        "        truncation=True\n",
        "    ).to(gen_mdl.device)\n",
        "\n",
        "    out_ids = gen_mdl.generate(\n",
        "        **inputs,\n",
        "        max_length=64,\n",
        "        num_beams=4,\n",
        "        do_sample=False\n",
        "    )\n",
        "    pred_cs = tok_g.decode(out_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    print(\"\\nLABEL:\", ex[\"label\"], \"| DOMAIN:\", ex[\"domain\"])\n",
        "    print(\"HOST :\", host)\n",
        "    print(\"GOLD :\", gold_cs)\n",
        "    print(\"PRED :\", pred_cs)\n"
      ],
      "metadata": {
        "id": "qCHrjCz5RReV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "for row in random.sample(train_pairs, 5):\n",
        "    print(\"\\nHOST:\", row['host'])\n",
        "    print(\"CS  :\", row['cs'])\n"
      ],
      "metadata": {
        "id": "3gLFilWbT_vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 ‚Äì Build RL dataset (prompts + labels)\n",
        "\n",
        "We‚Äôll use the host side of your supervised pairs + their labels:"
      ],
      "metadata": {
        "id": "ZPeiEhBcYrm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import json, random\n",
        "from collections import Counter\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/cs-senti\")\n",
        "DATA = BASE / \"data\"\n",
        "LING = DATA / \"ling\"\n",
        "\n",
        "FP_GAN_TR = LING / \"gan_pairs_train.jsonl\"\n",
        "\n",
        "def read_jsonl(fp):\n",
        "    rows = []\n",
        "    with open(fp, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "pairs = read_jsonl(FP_GAN_TR)\n",
        "print(\"Total supervised pairs:\", len(pairs))\n",
        "\n",
        "# RL dataset: (host_prompt, target_label)\n",
        "rl_data = [\n",
        "    {\n",
        "        \"host\": p[\"host\"],\n",
        "        \"label\": p.get(\"label\", \"neu\"),   # fallback neu\n",
        "        \"domain\": p.get(\"domain\", \"eesa\")\n",
        "    }\n",
        "    for p in pairs\n",
        "    if p.get(\"host\") and p.get(\"label\") in [\"pos\",\"neu\",\"neg\"]\n",
        "]\n",
        "\n",
        "print(\"RL data size:\", len(rl_data))\n",
        "print(\"Label dist:\", Counter([r[\"label\"] for r in rl_data]))\n",
        "print(\"Domain dist:\", Counter([r[\"domain\"] for r in rl_data]) )\n"
      ],
      "metadata": {
        "id": "C-kpO1IrYpf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4Ô∏è‚É£ Step 2 ‚Äì Load all oracles + generator\n",
        "\n",
        "We‚Äôll:\n",
        "\n",
        "reload your supervised generator\n",
        "\n",
        "load sentiment oracle\n",
        "\n",
        "load switch oracle"
      ],
      "metadata": {
        "id": "2AHqFR_lZNVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# --- generator (start from supervised G0) ---\n",
        "G_SUP = BASE / \"models/gan_stage1_generator_supervised\"\n",
        "tok_g = AutoTokenizer.from_pretrained(G_SUP)\n",
        "gen_model = AutoModelForSeq2SeqLM.from_pretrained(G_SUP).to(device)\n",
        "gen_model.eval()  # PPO code will switch to train() later\n",
        "\n",
        "# --- sentiment oracle (frozen) ---\n",
        "SA_DIR = BASE / \"models/sa_mixed_v3_frozen\"\n",
        "tok_sa = AutoTokenizer.from_pretrained(SA_DIR)\n",
        "sa_model = AutoModelForSequenceClassification.from_pretrained(SA_DIR).to(device)\n",
        "sa_model.eval()\n",
        "\n",
        "sa_labels = json.load(open(SA_DIR / \"label_map.json\"))[\"labels\"]  # [\"pos\",\"neg\",\"neu\"] or similar\n",
        "sa_idx = {lab: i for i, lab in enumerate(sa_labels)}\n",
        "\n",
        "# --- switch oracle (binary) ---\n",
        "SW_DIR = BASE / \"models/xlmr_switch_decider_lexsupervised\"\n",
        "tok_sw = AutoTokenizer.from_pretrained(SW_DIR)\n",
        "sw_model = AutoModelForSequenceClassification.from_pretrained(SW_DIR).to(device)\n",
        "sw_model.eval()\n"
      ],
      "metadata": {
        "id": "5WMaiayzZB3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5Ô∏è‚É£ Step 3 ‚Äì Reward functions\n",
        "5.1 Sentiment reward\n",
        "\n",
        "We want a score in [0,1] measuring how confidently the sentiment matches the original label."
      ],
      "metadata": {
        "id": "VyRR2ATvZQi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "@torch.no_grad()\n",
        "def sentiment_reward(texts, target_labels):\n",
        "    \"\"\"\n",
        "    texts: list[str] generated CS sentences\n",
        "    target_labels: list[str] same length, each in {\"pos\",\"neu\",\"neg\"}\n",
        "    returns: np.array of shape (batch,)\n",
        "    \"\"\"\n",
        "    enc = tok_sa(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    logits = sa_model(**enc).logits  # (B,3)\n",
        "    probs = logits.softmax(-1)\n",
        "\n",
        "    idx = torch.tensor([sa_idx[l] for l in target_labels], device=device)\n",
        "    scores = probs[torch.arange(len(texts), device=device), idx]  # prob of correct label\n",
        "\n",
        "    return scores.detach().cpu().numpy()  # values in [0,1]\n"
      ],
      "metadata": {
        "id": "iFSfTauBZOJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.2 Switch reward (how ‚Äúcode-switched‚Äù is it?)\n",
        "\n",
        "The switch-decider was trained at token level, but we can derive a sentence-level score: for each token, probability of ‚Äúswitch‚Äù; then average over tokens."
      ],
      "metadata": {
        "id": "WYNHDYwxZYYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def switch_reward(texts):\n",
        "    \"\"\"\n",
        "    texts: list[str] generated CS sentences\n",
        "    returns: np.array of shape (batch,), higher = more plausible switching\n",
        "    \"\"\"\n",
        "    enc = tok_sw(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    logits = sw_model(**enc).logits  # (B,2)\n",
        "    probs = logits.softmax(-1)       # class 1 = \"switch\"\n",
        "    # Take P(switch) as overall \"switchiness\" proxy.\n",
        "    # If your model was token-level, you might want to adapt this.\n",
        "    p_switch = probs[:, 1]\n",
        "\n",
        "    return p_switch.detach().cpu().numpy()\n"
      ],
      "metadata": {
        "id": "H9842E1kZaQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.3 Combine rewards\n",
        "\n",
        "We combine them with simple weights:"
      ],
      "metadata": {
        "id": "n35FhDVxZgYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combined_reward(gen_texts, target_labels, w_sent=0.7, w_switch=0.3):\n",
        "    r_sent = sentiment_reward(gen_texts, target_labels)   # [0,1]\n",
        "    r_sw   = switch_reward(gen_texts)                     # [0,1]\n",
        "    r_tot  = w_sent * r_sent + w_switch * r_sw\n",
        "    return r_tot, r_sent, r_sw\n"
      ],
      "metadata": {
        "id": "xU4RVOocZeOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6Ô∏è‚É£ Step 4 ‚Äì Generation function for PPO loop\n",
        "\n",
        "We need a helper that, given hosts, generates candidate CS texts from the current generator:"
      ],
      "metadata": {
        "id": "Mv369kQ9Znux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_cs(host_batch, max_new_tokens=40):\n",
        "    inputs = tok_g(\n",
        "        host_batch,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=64,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    outputs = gen_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "\n",
        "    texts = tok_g.batch_decode(outputs, skip_special_tokens=True)\n",
        "    return texts\n"
      ],
      "metadata": {
        "id": "ZqEpl8uYZoex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7Ô∏è‚É£ Step 5 ‚Äì PPO / RL training loop (conceptual skeleton)\n",
        "\n",
        "Here‚Äôs the shape of the PPO loop, without going deeply into all math details (because a full implementation is long):"
      ],
      "metadata": {
        "id": "ErCkH5vGZuQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "# the generator will now be trainable\n",
        "gen_model.train()\n",
        "optimizer = AdamW(gen_model.parameters(), lr=1e-5)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "RL_EPOCHS = 2  # you can increase once stable\n",
        "\n",
        "def get_minibatch():\n",
        "    batch = random.sample(rl_data, BATCH_SIZE)\n",
        "    hosts = [b[\"host\"] for b in batch]\n",
        "    labels = [b[\"label\"] for b in batch]\n",
        "    return hosts, labels\n",
        "\n",
        "for epoch in range(1, RL_EPOCHS+1):\n",
        "    print(f\"\\n=== RL Epoch {epoch} ===\")\n",
        "    for step in range(200):  # number of PPO steps per epoch (tune this)\n",
        "        hosts, labels = get_minibatch()\n",
        "\n",
        "        # 1) generate CS from current generator\n",
        "        gen_model.eval()\n",
        "        with torch.no_grad():\n",
        "            gen_texts = generate_cs(hosts)\n",
        "        gen_model.train()\n",
        "\n",
        "        # 2) compute rewards\n",
        "        r_total, r_sent, r_sw = combined_reward(gen_texts, labels)\n",
        "        r_total_t = torch.tensor(r_total, dtype=torch.float32, device=device)\n",
        "\n",
        "        # 3) re-encode generated texts to compute logprobs\n",
        "        enc = tok_g(\n",
        "            hosts,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=64,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "\n",
        "        with tok_g.as_target_tokenizer():\n",
        "            tgt = tok_g(\n",
        "                gen_texts,\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=64,\n",
        "                return_tensors=\"pt\"\n",
        "            ).input_ids.to(device)\n",
        "\n",
        "        # shift labels etc. for seq2seq cross-entropy\n",
        "        out = gen_model(\n",
        "            **enc,\n",
        "            labels=tgt\n",
        "        )\n",
        "        # out.loss is average NLL over tokens; we want policy gradient approximated as:\n",
        "        # loss_rl = - E[R * logpi] ‚âà out.loss * (-R_normalized)\n",
        "        # So we weight loss by normalized reward:\n",
        "\n",
        "        # normalize rewards (zero mean, unit std) to stabilize\n",
        "        r_norm = (r_total_t - r_total_t.mean()) / (r_total_t.std() + 1e-6)\n",
        "        # we multiply loss by (-r_norm) so high reward -> low loss -> parameters push toward it\n",
        "        loss = out.loss * (-r_norm.mean())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(gen_model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 20 == 0:\n",
        "            print(f\"step {step:03d} | loss={loss.item():.4f} | R={r_total.mean():.3f} | R_sent={r_sent.mean():.3f} | R_sw={r_sw.mean():.3f}\")\n",
        "\n",
        "    # optionally save checkpoint after each RL epoch\n",
        "    OUT_RL = BASE / \"models/gan_stage2_generator_rl\"\n",
        "    OUT_RL.mkdir(parents=True, exist_ok=True)\n",
        "    gen_model.save_pretrained(OUT_RL.as_posix())\n",
        "    tok_g.save_pretrained(OUT_RL.as_posix())\n",
        "    print(\"üíæ Saved RL generator checkpoint to:\", OUT_RL)\n"
      ],
      "metadata": {
        "id": "VkK4-G87ZvuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc, torch\n",
        "\n",
        "# 1) move oracles to CPU\n",
        "sa_model.to(\"cpu\")\n",
        "sw_model.to(\"cpu\")\n",
        "\n",
        "# keep generator on GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "gen_model.to(device)\n",
        "\n",
        "# 2) clear any leftover GPU cache\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"Generator on:\", device)\n",
        "print(\"Sentiment oracle on CPU, switch oracle on CPU.\")\n"
      ],
      "metadata": {
        "id": "Iwx9G7Z3fzcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "@torch.no_grad()\n",
        "def sentiment_reward(texts, target_labels):\n",
        "    \"\"\"\n",
        "    texts: list[str] generated CS sentences\n",
        "    target_labels: list[str] in {\"pos\",\"neu\",\"neg\"}\n",
        "    Uses sa_model on CPU.\n",
        "    \"\"\"\n",
        "    enc = tok_sa(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )  # stays on CPU\n",
        "\n",
        "    logits = sa_model(**enc).logits  # CPU\n",
        "    probs = logits.softmax(-1)\n",
        "\n",
        "    idx = torch.tensor([sa_idx[l] for l in target_labels], dtype=torch.long)\n",
        "    scores = probs[torch.arange(len(texts)), idx]\n",
        "\n",
        "    return scores.detach().numpy()  # [0,1]\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def switch_reward(texts):\n",
        "    \"\"\"\n",
        "    texts: list[str] generated CS sentences\n",
        "    Uses sw_model on CPU.\n",
        "    \"\"\"\n",
        "    enc = tok_sw(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )  # CPU\n",
        "\n",
        "    logits = sw_model(**enc).logits  # (B,2)\n",
        "    probs = logits.softmax(-1)\n",
        "    p_switch = probs[:, 1]\n",
        "\n",
        "    return p_switch.detach().numpy()\n",
        "\n",
        "\n",
        "def combined_reward(gen_texts, target_labels, w_sent=0.7, w_switch=0.3):\n",
        "    r_sent = sentiment_reward(gen_texts, target_labels)   # [0,1]\n",
        "    r_sw   = switch_reward(gen_texts)                     # [0,1]\n",
        "    r_tot  = w_sent * r_sent + w_switch * r_sw\n",
        "    return r_tot, r_sent, r_sw\n"
      ],
      "metadata": {
        "id": "4yDlCcSQf2j-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "gen_model.train()\n",
        "optimizer = AdamW(gen_model.parameters(), lr=1e-5)\n",
        "\n",
        "BATCH_SIZE = 4          # smaller batch to save memory\n",
        "RL_EPOCHS  = 1          # start with 1 just to test\n",
        "RL_STEPS   = 50         # PPO-ish steps per epoch (you can increase later)\n",
        "\n",
        "def get_minibatch():\n",
        "    batch = random.sample(rl_data, BATCH_SIZE)\n",
        "    hosts  = [b[\"host\"]  for b in batch]\n",
        "    labels = [b[\"label\"] for b in batch]\n",
        "    return hosts, labels\n",
        "\n",
        "def generate_cs(host_batch, max_new_tokens=40):\n",
        "    inputs = tok_g(\n",
        "        host_batch,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=64,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)  # generator on GPU\n",
        "\n",
        "    outputs = gen_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "\n",
        "    texts = tok_g.batch_decode(outputs, skip_special_tokens=True)\n",
        "    return texts\n",
        "\n",
        "for epoch in range(1, RL_EPOCHS + 1):\n",
        "    print(f\"\\n=== RL Epoch {epoch} ===\")\n",
        "    for step in range(RL_STEPS):\n",
        "        hosts, labels = get_minibatch()\n",
        "\n",
        "        # 1) generate CS with current generator\n",
        "        gen_model.eval()\n",
        "        with torch.no_grad():\n",
        "            gen_texts = generate_cs(hosts)\n",
        "        gen_model.train()\n",
        "\n",
        "        # 2) compute rewards (on CPU)\n",
        "        r_total, r_sent, r_sw = combined_reward(gen_texts, labels)\n",
        "        r_total_t = torch.tensor(r_total, dtype=torch.float32, device=device)\n",
        "\n",
        "        # 3) compute seq2seq loss for generated outputs\n",
        "        enc = tok_g(\n",
        "            hosts,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=64,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "\n",
        "        tgt = tok_g(\n",
        "            text_target=gen_texts,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=64,\n",
        "            return_tensors=\"pt\"\n",
        "        ).input_ids.to(device)\n",
        "\n",
        "        out = gen_model(\n",
        "            **enc,\n",
        "            labels=tgt\n",
        "        )  # out.loss is average NLL\n",
        "\n",
        "        # normalize rewards\n",
        "        r_norm = (r_total_t - r_total_t.mean()) / (r_total_t.std() + 1e-6)\n",
        "        # scalar surrogate loss: high reward => reduce loss\n",
        "        loss = out.loss * (-r_norm.mean())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(gen_model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 10 == 0:\n",
        "            print(\n",
        "                f\"step {step:03d} | loss={loss.item():.4f} | \"\n",
        "                f\"R={r_total.mean():.3f} | R_sent={r_sent.mean():.3f} | R_sw={r_sw.mean():.3f}\"\n",
        "            )\n",
        "\n",
        "    # save after RL epoch\n",
        "    OUT_RL = BASE / \"models/gan_stage2_generator_rl\"\n",
        "    OUT_RL.mkdir(parents=True, exist_ok=True)\n",
        "    gen_model.save_pretrained(OUT_RL.as_posix())\n",
        "    tok_g.save_pretrained(OUT_RL.as_posix())\n",
        "    print(\"üíæ Saved RL generator checkpoint to:\", OUT_RL)\n"
      ],
      "metadata": {
        "id": "EKSIP0Sxf5OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, json\n",
        "from pathlib import Path\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoModelForSequenceClassification,\n",
        ")\n",
        "\n",
        "BASE   = Path(\"/content/drive/MyDrive/cs-senti\")\n",
        "MODELS = BASE / \"models\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# 1) Generator (mT5) ON GPU\n",
        "GEN_DIR = MODELS / \"gan_stage1_generator_supervised\"\n",
        "tok_g   = AutoTokenizer.from_pretrained(GEN_DIR.as_posix())\n",
        "gen_model = AutoModelForSeq2SeqLM.from_pretrained(GEN_DIR.as_posix()).to(device)\n",
        "\n",
        "# 2) Sentiment oracle ON CPU\n",
        "SA_DIR = MODELS / \"sa_mixed_v3_frozen\"\n",
        "tok_sa = AutoTokenizer.from_pretrained(SA_DIR.as_posix())\n",
        "sa_model = AutoModelForSequenceClassification.from_pretrained(SA_DIR.as_posix())\n",
        "sa_model.eval()     # CPU\n",
        "\n",
        "sa_labels = json.load(open(SA_DIR / \"label_map.json\"))[\"labels\"]\n",
        "sa_idx    = {l:i for i,l in enumerate(sa_labels)}\n",
        "\n",
        "# 3) Switch-decider ON CPU\n",
        "SW_DIR = MODELS / \"xlmr_switch_decider_lexsupervised\"\n",
        "tok_sw = AutoTokenizer.from_pretrained(SW_DIR.as_posix())\n",
        "sw_model = AutoModelForSequenceClassification.from_pretrained(SW_DIR.as_posix())\n",
        "sw_model.eval()     # CPU\n",
        "\n",
        "print(\"‚úÖ Loaded generator on\", device, \"| oracles on CPU\")\n"
      ],
      "metadata": {
        "id": "kN9uXCxnljeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "DATA = BASE / \"data\"\n",
        "LING = DATA / \"ling\"\n",
        "\n",
        "FP_GAN_TR = LING / \"gan_pairs_train.jsonl\"\n",
        "\n",
        "def read_jsonl(fp):\n",
        "    rows = []\n",
        "    with open(fp, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "rl_data = read_jsonl(FP_GAN_TR)\n",
        "print(\"RL data size:\", len(rl_data))\n",
        "print(\"Sample:\", rl_data[0])\n"
      ],
      "metadata": {
        "id": "Y20nnsMmlsHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "@torch.no_grad()\n",
        "def sentiment_reward(texts, target_labels):\n",
        "    enc = tok_sa(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"       # CPU\n",
        "    )\n",
        "    logits = sa_model(**enc).logits\n",
        "    probs  = logits.softmax(-1)\n",
        "\n",
        "    idx = torch.tensor([sa_idx[l] for l in target_labels], dtype=torch.long)\n",
        "    scores = probs[torch.arange(len(texts)), idx]\n",
        "    return scores.detach().numpy()\n",
        "\n",
        "@torch.no_grad()\n",
        "def switch_reward(texts):\n",
        "    enc = tok_sw(\n",
        "        texts,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=128,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    logits = sw_model(**enc).logits\n",
        "    probs  = logits.softmax(-1)\n",
        "    p_switch = probs[:, 1]\n",
        "    return p_switch.detach().numpy()\n",
        "\n",
        "def combined_reward(gen_texts, target_labels, w_sent=0.7, w_switch=0.3):\n",
        "    r_sent = sentiment_reward(gen_texts, target_labels)\n",
        "    r_sw   = switch_reward(gen_texts)\n",
        "    r_tot  = w_sent * r_sent + w_switch * r_sw\n",
        "    return r_tot, r_sent, r_sw\n"
      ],
      "metadata": {
        "id": "1WAIhhsanywY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Adafactor\n",
        "import random\n",
        "\n",
        "gen_model.train()\n",
        "\n",
        "# Adafactor = more memory friendly than AdamW for T5/mT5\n",
        "optimizer = Adafactor(\n",
        "    gen_model.parameters(),\n",
        "    lr=1e-4,\n",
        "    relative_step=False,\n",
        "    scale_parameter=False\n",
        ")\n",
        "\n",
        "BATCH_SIZE = 2\n",
        "RL_EPOCHS  = 1\n",
        "RL_STEPS   = 40   # keep it modest\n",
        "\n",
        "def get_minibatch():\n",
        "    batch = random.sample(rl_data, BATCH_SIZE)\n",
        "    hosts  = [b[\"host\"]  for b in batch]\n",
        "    labels = [b[\"label\"] for b in batch]\n",
        "    return hosts, labels\n",
        "\n",
        "def generate_cs(host_batch, max_new_tokens=32):\n",
        "    inputs = tok_g(\n",
        "        host_batch,\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=48,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)   # only this goes to GPU\n",
        "\n",
        "    outputs = gen_model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "\n",
        "    texts = tok_g.batch_decode(outputs, skip_special_tokens=True)\n",
        "    return texts\n",
        "\n",
        "for epoch in range(1, RL_EPOCHS + 1):\n",
        "    print(f\"\\n=== RL Epoch {epoch} (GPU) ===\")\n",
        "    for step in range(RL_STEPS):\n",
        "        hosts, labels = get_minibatch()\n",
        "\n",
        "        # 1) generate on GPU\n",
        "        with torch.no_grad():\n",
        "            gen_texts = generate_cs(hosts)\n",
        "\n",
        "        # 2) rewards on CPU\n",
        "        r_total, r_sent, r_sw = combined_reward(gen_texts, labels)\n",
        "        r_total_t = torch.tensor(r_total, dtype=torch.float32, device=device)\n",
        "\n",
        "        # 3) teacher-forcing loss on GPU w.r.t generated text\n",
        "        enc = tok_g(\n",
        "            hosts,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=48,\n",
        "            return_tensors=\"pt\"\n",
        "        ).to(device)\n",
        "\n",
        "        tgt_ids = tok_g(\n",
        "            text_target=gen_texts,\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            max_length=48,\n",
        "            return_tensors=\"pt\"\n",
        "        ).input_ids.to(device)\n",
        "\n",
        "        out = gen_model(\n",
        "            **enc,\n",
        "            labels=tgt_ids\n",
        "        )\n",
        "        base_loss = out.loss\n",
        "\n",
        "        # normalize reward\n",
        "        r_norm = (r_total_t - r_total_t.mean()) / (r_total_t.std() + 1e-6)\n",
        "        # simple shaping: encourage higher reward ‚áí smaller loss\n",
        "        loss = base_loss * (1.0 - r_norm.mean())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(gen_model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        if step % 5 == 0:\n",
        "            print(\n",
        "                f\"step {step:02d} | loss={loss.item():.4f} | \"\n",
        "                f\"R={r_total.mean():.3f} | R_sent={r_sent.mean():.3f} | R_sw={r_sw.mean():.3f}\"\n",
        "            )\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "OUT_RL = MODELS / \"gan_stage2_generator_rl_gpu\"\n",
        "OUT_RL.mkdir(parents=True, exist_ok=True)\n",
        "gen_model.save_pretrained(OUT_RL.as_posix())\n",
        "tok_g.save_pretrained(OUT_RL.as_posix())\n",
        "print(\"üíæ Saved RL generator checkpoint to:\", OUT_RL)\n"
      ],
      "metadata": {
        "id": "MU2wYfain1Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: qualitative comparison between:"
      ],
      "metadata": {
        "id": "rkFUDcG4dwGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, random\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "BASE   = Path(\"/content/drive/MyDrive/cs-senti\")\n",
        "DATA   = BASE / \"data\"\n",
        "LING   = DATA / \"ling\"\n",
        "MODELS = BASE / \"models\"\n",
        "\n",
        "FP_GAN_TR = LING / \"gan_pairs_train.jsonl\"\n",
        "\n",
        "GEN_SUP_DIR = MODELS / \"gan_stage1_generator_supervised\"\n",
        "GEN_RL_DIR  = MODELS / \"gan_stage2_generator_rl_gpu\"\n",
        "\n",
        "def read_jsonl(fp: Path):\n",
        "    rows = []\n",
        "    with open(fp, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "pairs = read_jsonl(FP_GAN_TR)\n",
        "print(\"Total train pairs:\", len(pairs))\n",
        "\n",
        "# load tokenizer + both generators\n",
        "tok_g = AutoTokenizer.from_pretrained(GEN_SUP_DIR)\n",
        "\n",
        "gen_sup = AutoModelForSeq2SeqLM.from_pretrained(GEN_SUP_DIR).to(device)\n",
        "gen_rl  = AutoModelForSeq2SeqLM.from_pretrained(GEN_RL_DIR).to(device)\n",
        "\n",
        "gen_sup.eval()\n",
        "gen_rl.eval()\n",
        "\n",
        "def generate_cs(model, tokenizer, host_text, max_src_len=64, max_new_tokens=64):\n",
        "    \"\"\"Generate a CS sentence from a host sentence.\"\"\"\n",
        "    enc = tokenizer(\n",
        "        host_text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=max_src_len\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out_ids = model.generate(\n",
        "            **enc,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            num_beams=4,\n",
        "            do_sample=False\n",
        "        )\n",
        "    text = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
        "    return text.strip()\n"
      ],
      "metadata": {
        "id": "MWDnpt4ldoPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# how many examples to inspect\n",
        "N = 15\n",
        "sampled = random.sample(pairs, min(N, len(pairs)))\n",
        "\n",
        "for i, ex in enumerate(sampled, 1):\n",
        "    host   = ex[\"host\"]\n",
        "    cs_gold = ex[\"cs\"]\n",
        "    label  = ex.get(\"label\", \"neu\")\n",
        "    domain = ex.get(\"domain\", \"unk\")\n",
        "\n",
        "    cs_sup = generate_cs(gen_sup, tok_g, host)\n",
        "    cs_rl  = generate_cs(gen_rl, tok_g, host)\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(f\"[{i}] LABEL: {label} | DOMAIN: {domain}\")\n",
        "    print(f\"HOST : {host}\")\n",
        "    print(f\"GOLD : {cs_gold}\")\n",
        "    print(f\"SUP  : {cs_sup}\")\n",
        "    print(f\"RL   : {cs_rl}\")\n"
      ],
      "metadata": {
        "id": "JztiE8ZId486"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}