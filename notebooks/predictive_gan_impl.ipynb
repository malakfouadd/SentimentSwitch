{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3g208ZRSHPa"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sentence-transformers datasets accelerate -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ju7MtQM_SKBP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pathlib import Path\n",
        "import random, json\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25eXUHWDkfyY"
      },
      "outputs": [],
      "source": [
        "!mkdir -p \"/content/drive/MyDrive/cs-senti/labse\"\n",
        "!git lfs install\n",
        "!git clone https://huggingface.co/sentence-transformers/LaBSE \"/content/drive/MyDrive/cs-senti/labse\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0dmo1L6rYQ0"
      },
      "outputs": [],
      "source": [
        "import json, os\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/cs-senti/repo/data/annotated_with_id\")\n",
        "\n",
        "def load_jsonl(path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "eesa_train_annotated = load_jsonl(BASE/\"eesa_train_annotated.jsonl\")\n",
        "eesa_dev_annotated   = load_jsonl(BASE/\"eesa_dev_annotated.jsonl\")\n",
        "eesa_test_annotated  = load_jsonl(BASE/\"eesa_test_annotated.jsonl\")\n",
        "\n",
        "mr_cs_labeled  = load_jsonl(BASE/\"mr_cs_labeled_annotated.jsonl\")\n",
        "amg_cs_labeled = load_jsonl(BASE/\"amg_cs_labeled_annotated.jsonl\")\n",
        "amg_ar_mono    = load_jsonl(BASE/\"amg_ar_mono_annotated.jsonl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSj5nDsrrb6m"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "LEXICON_PATH = \"/content/drive/MyDrive/cs-senti/data/ling/ar_en_lexicon_expanded_with_synonyms.jsonl\"\n",
        "\n",
        "def load_lexicon(path):\n",
        "    lex = {}\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            item = json.loads(line)\n",
        "            base = item[\"en\"]\n",
        "            syns = item.get(\"syn\", [])\n",
        "            lex[item[\"ar\"]] = [base] + syns   # list\n",
        "    return lex\n",
        "\n",
        "LEXICON = load_lexicon(LEXICON_PATH)\n",
        "print(\"Loaded lexicon entries:\", len(LEXICON))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrHS_igyuy0u"
      },
      "source": [
        "# **new lexicon**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6299oFyuyHL"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "LEXICON_PATH = \"/content/drive/MyDrive/cs-senti/repo/data/ar_en_lexicon_MERGED.jsonl\"\n",
        "\n",
        "def load_lexicon(path):\n",
        "    lex = {}\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            item = json.loads(line)\n",
        "\n",
        "            ar = item[\"ar\"]\n",
        "\n",
        "            # --- ensure EN is always a list ---\n",
        "            en_list = item[\"en\"]\n",
        "            if isinstance(en_list, str):\n",
        "                en_list = [en_list]\n",
        "\n",
        "            # --- ensure SYN is always a list ---\n",
        "            syns = item.get(\"syn\", [])\n",
        "            if isinstance(syns, str):\n",
        "                syns = [syns]\n",
        "\n",
        "            # --- merge + remove duplicates while keeping order ---\n",
        "            final = list(dict.fromkeys(en_list + syns))\n",
        "\n",
        "            lex[ar] = final\n",
        "\n",
        "    return lex\n",
        "\n",
        "LEXICON = load_lexicon(LEXICON_PATH)\n",
        "print(\"Loaded lexicon entries:\", len(LEXICON))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z06V61JnrfbY"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def is_english_word(w):\n",
        "    return bool(re.fullmatch(r\"[A-Za-z]+\", w))\n",
        "\n",
        "def generate_candidates(tokens, mask_positions, lexicon, lang_tags, pos_tags, has_al, top_k=5):\n",
        "    candidates = {}\n",
        "\n",
        "    for pos in mask_positions:\n",
        "        tok = tokens[pos]\n",
        "        al_flag = has_al[pos]\n",
        "\n",
        "        cand_list = []\n",
        "\n",
        "        # 1. Lexicon\n",
        "        if tok in lexicon:\n",
        "            eng_list = lexicon[tok]\n",
        "            eng = eng_list[0]     # keep original behavior\n",
        "            if al_flag and is_english_word(eng):\n",
        "                eng = \"ال \" + eng\n",
        "            cand_list.append(eng)\n",
        "\n",
        "        # 2. Fallback\n",
        "        if is_english_word(tok):\n",
        "            fb = tok.lower()\n",
        "            if al_flag:\n",
        "                fb = \"ال \" + fb\n",
        "            if fb not in cand_list:\n",
        "                cand_list.append(fb)\n",
        "\n",
        "        candidates[pos] = cand_list[:top_k] if cand_list else []\n",
        "\n",
        "    return candidates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQJunyPNoXCS"
      },
      "outputs": [],
      "source": [
        "def get_cand_embeddings(cand_list):\n",
        "    \"\"\"\n",
        "    Returns LaBSE embeddings (768-d) for each candidate.\n",
        "    \"\"\"\n",
        "    emb = similarity_model.encode(cand_list, convert_to_tensor=True)\n",
        "    return emb.to(\"cuda\")  # shape: [N, 768]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZQMFitnrhfF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, hidden=256):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(768, hidden)   # <-- FIXED (300 → 768)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(hidden, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.activation(self.linear1(x))\n",
        "        return self.linear2(h)\n",
        "\n",
        "\n",
        "generator = Generator().to(\"cuda\")\n",
        "optimizer = torch.optim.Adam(generator.parameters(), lr=1e-4)\n",
        "\n",
        "print(\"Generator ready ✓\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d7VxsGAy8wY"
      },
      "outputs": [],
      "source": [
        "!pip uninstall -y transformers tokenizers huggingface_hub sentence-transformers\n",
        "!pip install --upgrade transformers==4.40.2\n",
        "!pip install sentence-transformers==2.6.1\n",
        "!pip install huggingface_hub==0.22.2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjLY-WyNztpz"
      },
      "outputs": [],
      "source": [
        "import transformers, sentence_transformers, huggingface_hub\n",
        "\n",
        "print(\"transformers:\", transformers.__version__)\n",
        "print(\"sentence-transformers:\", sentence_transformers.__version__)\n",
        "print(\"huggingface_hub:\", huggingface_hub.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5FFFl3Qrph6"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "reward_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"/content/drive/MyDrive/cs-senti/baseline_marbert_v1/checkpoint-1920\"\n",
        ")\n",
        "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"/content/drive/MyDrive/cs-senti/baseline_marbert_v1/checkpoint-1920\"\n",
        ").to(\"cuda\")\n",
        "reward_model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5exPraGrwdq"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "disc_tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "\n",
        "disc_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"aubmindlab/bert-base-arabertv2\",\n",
        "    num_labels=2    # REAL = 1, FAKE = 0\n",
        ").to(\"cuda\")\n",
        "\n",
        "disc_optimizer = torch.optim.Adam(disc_model.parameters(), lr=2e-5)\n",
        "\n",
        "print(\"Discriminator ready ✓\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxCo41sUr2Qb"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Correct LaBSE load\n",
        "similarity_model = SentenceTransformer(\"sentence-transformers/LaBSE\")\n",
        "\n",
        "def reward_similarity(orig, gen):\n",
        "    emb_o = similarity_model.encode(orig, convert_to_tensor=True)\n",
        "    emb_g = similarity_model.encode(gen, convert_to_tensor=True)\n",
        "    return float(F.cosine_similarity(emb_o, emb_g, dim=0).item())\n",
        "\n",
        "def semantic_similarity(a, b):\n",
        "    emb_a = similarity_model.encode(a, convert_to_tensor=True)\n",
        "    emb_b = similarity_model.encode(b, convert_to_tensor=True)\n",
        "    return float(F.cosine_similarity(emb_a, emb_b, dim=0).item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46o8e4lAomM1"
      },
      "outputs": [],
      "source": [
        "def train_discriminator(real_text, fake_text):\n",
        "    disc_optimizer.zero_grad()\n",
        "\n",
        "    # REAL\n",
        "    real_in = disc_tokenizer(real_text, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
        "    real_out = disc_model(**real_in).logits\n",
        "    real_loss = torch.nn.functional.cross_entropy(real_out, torch.tensor([1]).to(\"cuda\"))\n",
        "\n",
        "    # FAKE\n",
        "    fake_in = disc_tokenizer(fake_text, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
        "    fake_out = disc_model(**fake_in).logits\n",
        "    fake_loss = torch.nn.functional.cross_entropy(fake_out, torch.tensor([0]).to(\"cuda\"))\n",
        "\n",
        "    loss = real_loss + fake_loss\n",
        "    loss.backward()\n",
        "    disc_optimizer.step()\n",
        "\n",
        "    return float(loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3w7hmXJipWFV"
      },
      "outputs": [],
      "source": [
        "def reinforce_step_combined(\n",
        "    example,\n",
        "    candidates_dict,\n",
        "    generator,\n",
        "    optimizer,\n",
        "    disc_model,\n",
        "    disc_tokenizer,\n",
        "    reward_model,\n",
        "    reward_tokenizer,\n",
        "    sim_model,\n",
        "    alpha=0.5,\n",
        "    beta=0.4,\n",
        "    gamma=0.1,\n",
        "):\n",
        "    \"\"\"\n",
        "    Combined REINFORCE step using:\n",
        "    - Discriminator reward     (alpha)\n",
        "    - Sentiment consistency    (beta)\n",
        "    - Semantic similarity      (gamma)\n",
        "    \"\"\"\n",
        "\n",
        "    tokens = example[\"tokens\"]\n",
        "    sent_label = example.get(\"label\")\n",
        "\n",
        "    # Skip unlabeled samples\n",
        "    if sent_label is None:\n",
        "        return None\n",
        "\n",
        "    idx_map = {\"neg\": 0, \"neu\": 1, \"pos\": 2}\n",
        "\n",
        "    log_probs = []\n",
        "    rewards = []\n",
        "\n",
        "    for pos, cand_list in candidates_dict.items():\n",
        "\n",
        "        # ============================\n",
        "        # 1. Handle EMPTY candidate list\n",
        "        # ============================\n",
        "        if len(cand_list) == 0:\n",
        "            continue\n",
        "\n",
        "        # ============================\n",
        "        # 2. Determine chosen candidate and log_prob (if learning)\n",
        "        # ============================\n",
        "        chosen = None # Initialize chosen\n",
        "\n",
        "        if len(cand_list) == 1:\n",
        "            chosen = cand_list[0]\n",
        "            # If only one candidate, no choice is made by the generator.\n",
        "            # This step does not contribute to generator's policy gradient.\n",
        "        else:\n",
        "            # Dummy embeddings for candidates - MUST require grad for generator to learn\n",
        "            cand_vecs = get_cand_embeddings(cand_list).clone().detach().requires_grad_(True)\n",
        "\n",
        "            logits = generator(cand_vecs).squeeze(-1)\n",
        "\n",
        "            # Fix NaNs\n",
        "            if torch.isnan(logits).any():\n",
        "                logits = torch.zeros_like(logits)\n",
        "\n",
        "            probs = torch.softmax(logits, dim=0)\n",
        "\n",
        "            # Fix zero-prob or NaNs\n",
        "            if probs.sum() == 0 or torch.isnan(probs).any():\n",
        "                probs = torch.ones_like(probs) / len(probs)\n",
        "\n",
        "            # Sample\n",
        "            idx = torch.multinomial(probs, 1).item()\n",
        "            chosen = cand_list[idx]\n",
        "\n",
        "            log_prob = torch.log(probs[idx] + 1e-12)\n",
        "            log_probs.append(log_prob) # Append only if generator made a choice\n",
        "\n",
        "        # ============================\n",
        "        # 3. Build the new switched sentence\n",
        "        # ============================\n",
        "        # This block should be executed for all cases where a chosen candidate exists\n",
        "        if chosen is None:\n",
        "            # This should ideally not happen if cand_list is not empty\n",
        "            continue\n",
        "\n",
        "        replaced = tokens.copy()\n",
        "        replaced[pos] = chosen\n",
        "        new_text = \" \".join(replaced)\n",
        "\n",
        "        # ============================\n",
        "        # 4. DISCRIMINATOR reward (D)\n",
        "        # ============================\n",
        "        disc_inputs = disc_tokenizer(\n",
        "            new_text, return_tensors=\"pt\", truncation=True\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        disc_logits = disc_model(**disc_inputs).logits.softmax(-1)\n",
        "        D_reward = float(disc_logits[0][1].item())  # probability it is \"real\"\n",
        "\n",
        "        # ============================\n",
        "        # 5. SENTIMENT reward (S)\n",
        "        # ============================\n",
        "        sent_inputs = reward_tokenizer(\n",
        "            new_text, return_tensors=\"pt\", truncation=True\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        sent_logits = reward_model(**sent_inputs).logits\n",
        "        S_reward = float(sent_logits[0][idx_map[sent_label]].item())\n",
        "\n",
        "\n",
        "        # ============================\n",
        "        # 6. SEMANTIC SIMILARITY reward (Sim)\n",
        "        # ============================\n",
        "        # ============================\n",
        "# 6. SEMANTIC SIMILARITY reward (Sim) - FIXED\n",
        "# ============================\n",
        "        Sim_reward = max(0, reward_similarity(example[\"original\"], new_text))\n",
        "\n",
        "\n",
        "        # ============================\n",
        "        # 7. Combined reward\n",
        "        # ============================\n",
        "        R = alpha * D_reward + beta * S_reward + gamma * Sim_reward\n",
        "\n",
        "        # Append R only if log_prob was also appended (i.e., generator made a choice)\n",
        "        if len(cand_list) > 1:\n",
        "            rewards.append(R)\n",
        "\n",
        "    # ============================\n",
        "    # 8. Policy gradient optimization\n",
        "    # ============================\n",
        "    if len(log_probs) > 0: # Only optimize if there were actual choices made by the generator\n",
        "        log_probs = torch.stack(log_probs)\n",
        "        rewards = torch.tensor(rewards).to(\"cuda\")\n",
        "\n",
        "        loss = -(log_probs * rewards).mean()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        return float(loss.item())\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyDjbfRHsGy-"
      },
      "outputs": [],
      "source": [
        "def gan_generate_text(example, generator, reward_tokenizer, reward_model):\n",
        "    \"\"\"\n",
        "    Safe GAN sentence generation — now uses LaBSE embeddings (768-d)\n",
        "    instead of random 300-d vectors.\n",
        "    \"\"\"\n",
        "    tokens = example[\"tokens\"].copy()\n",
        "    mask_positions = example[\"mask_positions\"]\n",
        "\n",
        "    if len(mask_positions) == 0:\n",
        "        return example[\"original\"]\n",
        "\n",
        "    candidates_dict = generate_candidates(\n",
        "        tokens=tokens,\n",
        "        mask_positions=mask_positions,\n",
        "        lexicon=LEXICON,\n",
        "        lang_tags=example[\"lang\"],\n",
        "        pos_tags=example[\"pos\"],\n",
        "        has_al=example[\"has_al\"]\n",
        "    )\n",
        "\n",
        "    for pos, cand_list in candidates_dict.items():\n",
        "\n",
        "        # skip empty candidate list\n",
        "        if len(cand_list) == 0:\n",
        "            continue\n",
        "\n",
        "        # === FIX: use LaBSE embeddings (768-d) ===\n",
        "        cand_vecs = similarity_model.encode(\n",
        "            cand_list,\n",
        "            convert_to_tensor=True\n",
        "        ).to(\"cuda\")  # shape: [N, 768]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = generator(cand_vecs).squeeze(-1)\n",
        "            probs = torch.softmax(logits, dim=0)\n",
        "\n",
        "        if probs.numel() == 0:\n",
        "            continue\n",
        "\n",
        "        best_idx = torch.argmax(probs).item()\n",
        "        chosen = cand_list[best_idx]\n",
        "\n",
        "        tokens[pos] = chosen\n",
        "\n",
        "    generated_text = \" \".join(tokens).replace(\"  \", \" \").strip()\n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9VZwGQjuhsq"
      },
      "source": [
        "training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "005PKb5Vui7K"
      },
      "outputs": [],
      "source": [
        "import json, os\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/cs-senti/repo/data/annotated_with_id\")\n",
        "\n",
        "def load_jsonl(path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "# --------- MAIN DATASETS ----------\n",
        "eesa_train_annotated = load_jsonl(BASE/\"eesa_train_annotated.jsonl\")\n",
        "mr_cs_labeled        = load_jsonl(BASE/\"mr_cs_labeled_annotated.jsonl\")\n",
        "amg_cs_labeled       = load_jsonl(BASE/\"amg_cs_labeled_annotated.jsonl\")\n",
        "#amg_ar_mono          = load_jsonl(BASE/\"amg_ar_mono_annotated.jsonl\")\n",
        "\n",
        "print(\"EESA:\", len(eesa_train_annotated))\n",
        "print(\"MR-CS:\", len(mr_cs_labeled))\n",
        "print(\"AMG-CS:\", len(amg_cs_labeled))\n",
        "#print(\"AMG-MONO:\", len(amg_ar_mono))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSM7aU-1vsA7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# =====================================================\n",
        "# 1. Load the original AMG-AR-MONO file\n",
        "# =====================================================\n",
        "\n",
        "AMG_MONO_PATH = \"/content/drive/MyDrive/cs-senti/repo/data/annotated_with_id/amg_ar_mono_annotated.jsonl\"\n",
        "\n",
        "amg_mono = []\n",
        "with open(AMG_MONO_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        amg_mono.append(json.loads(line))\n",
        "\n",
        "print(\"Loaded AMG-AR-MONO samples:\", len(amg_mono))\n",
        "\n",
        "# =====================================================\n",
        "# 2. Define function to compute sentiment label\n",
        "#    USING YOUR EXISTING SENTIMENT MODEL\n",
        "# =====================================================\n",
        "\n",
        "def predict_sentiment(text):\n",
        "    inputs = reward_tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    preds = reward_model(**inputs).logits.softmax(-1)\n",
        "    label_index = preds.argmax(-1).item()\n",
        "\n",
        "    return [\"neg\", \"neu\", \"pos\"][label_index]\n",
        "\n",
        "# =====================================================\n",
        "# 3. Auto-label entire AMG-AR-MONO dataset\n",
        "# =====================================================\n",
        "\n",
        "amg_mono_labeled = []\n",
        "\n",
        "for ex in tqdm(amg_mono, desc=\"Auto-labeling AMG-AR-MONO\"):\n",
        "    text = ex[\"original\"]\n",
        "\n",
        "    # predict sentiment using your reward model\n",
        "    auto_label = predict_sentiment(text)\n",
        "\n",
        "    # attach new label\n",
        "    ex[\"label\"] = auto_label\n",
        "\n",
        "    amg_mono_labeled.append(ex)\n",
        "\n",
        "print(\"Done. Example labeled sample:\")\n",
        "print(amg_mono_labeled[0])\n",
        "\n",
        "# =====================================================\n",
        "# 4. Save NEW labeled version\n",
        "# =====================================================\n",
        "\n",
        "SAVE_PATH = \"/content/drive/MyDrive/cs-senti/repo/data/annotated_with_id/amg_ar_mono_auto_labeled.jsonl\"\n",
        "\n",
        "with open(SAVE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    for ex in amg_mono_labeled:\n",
        "        f.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"Saved auto-labeled AMG-MONO to:\")\n",
        "print(SAVE_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WrjM7Hgwzs6"
      },
      "outputs": [],
      "source": [
        "amg_ar_mono = load_jsonl(BASE/\"amg_ar_mono_auto_labeled.jsonl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMy5SV_OrjQn"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# ---- create reproducible subset ----\n",
        "random.seed(42)\n",
        "\n",
        "amg_ar_subset = random.sample(amg_ar_mono, 2000)\n",
        "\n",
        "print(\"Subset size:\", len(amg_ar_subset))\n",
        "print(\"Original size:\", len(amg_ar_mono))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6BQMldTunr9"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_gan(\n",
        "    dataset,\n",
        "    epochs,\n",
        "    batch_size,\n",
        "    generator,\n",
        "    optimizer,\n",
        "    disc_model,\n",
        "    disc_tokenizer,\n",
        "    reward_model,\n",
        "    reward_tokenizer,\n",
        "    sim_model,\n",
        "    alpha=0.5,\n",
        "    beta=0.4,\n",
        "    gamma=0.1\n",
        "):\n",
        "    \"\"\"\n",
        "    Full GAN training loop with:\n",
        "    ✓ Generator REINFORCE (policy gradient)\n",
        "    ✓ Discriminator training (real vs fake)\n",
        "    ✓ Sentiment critic (MARBERT)\n",
        "    ✓ Semantic similarity critic (LaBSE)\n",
        "    \"\"\"\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        print(f\"\\n===== EPOCH {ep+1}/{epochs} =====\")\n",
        "        random.shuffle(dataset)\n",
        "\n",
        "        gen_losses = []\n",
        "        disc_losses = []\n",
        "\n",
        "        # ==== Iterate over dataset in batches ====\n",
        "        for i in tqdm(range(0, len(dataset), batch_size)):\n",
        "            batch = dataset[i:i+batch_size]\n",
        "\n",
        "            for ex in batch:\n",
        "\n",
        "                # ---------- Skip unlabeled samples ----------\n",
        "                if ex.get(\"label\") is None:\n",
        "                    continue\n",
        "\n",
        "                tokens = ex[\"tokens\"]\n",
        "                mask_positions = ex[\"mask_positions\"]\n",
        "\n",
        "                # ---------- Candidate generation ----------\n",
        "                candidates = generate_candidates(\n",
        "                    tokens=tokens,\n",
        "                    mask_positions=mask_positions,\n",
        "                    lexicon=LEXICON,\n",
        "                    lang_tags=ex[\"lang\"],\n",
        "                    pos_tags=ex[\"pos\"],\n",
        "                    has_al=ex[\"has_al\"]\n",
        "                )\n",
        "\n",
        "                # ---------- GENERATOR UPDATE (RL) ----------\n",
        "                loss_g = reinforce_step_combined(\n",
        "                    ex,\n",
        "                    candidates,\n",
        "                    generator,\n",
        "                    optimizer,\n",
        "                    disc_model,\n",
        "                    disc_tokenizer,\n",
        "                    reward_model,\n",
        "                    reward_tokenizer,\n",
        "                    sim_model,\n",
        "                    alpha,\n",
        "                    beta,\n",
        "                    gamma\n",
        "                )\n",
        "\n",
        "                if loss_g is not None:\n",
        "                    gen_losses.append(loss_g)\n",
        "\n",
        "                # ---------- DISCRIMINATOR UPDATE ----------\n",
        "                fake_text = gan_generate_text(ex, generator, reward_tokenizer, reward_model)\n",
        "                real_text = ex[\"original\"]\n",
        "\n",
        "                disc_optimizer.zero_grad()\n",
        "\n",
        "                # Real example: target = 1\n",
        "                real_inp = disc_tokenizer(real_text, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
        "                real_logits = disc_model(**real_inp).logits\n",
        "                real_loss = torch.nn.functional.cross_entropy(\n",
        "                    real_logits,\n",
        "                    torch.tensor([1]).to(\"cuda\")\n",
        "                )\n",
        "\n",
        "                # Fake example: target = 0\n",
        "                fake_inp = disc_tokenizer(fake_text, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
        "                fake_logits = disc_model(**fake_inp).logits\n",
        "                fake_loss = torch.nn.functional.cross_entropy(\n",
        "                    fake_logits,\n",
        "                    torch.tensor([0]).to(\"cuda\")\n",
        "                )\n",
        "\n",
        "                loss_d = real_loss + fake_loss\n",
        "                loss_d.backward()\n",
        "                disc_optimizer.step()\n",
        "\n",
        "                disc_losses.append(float(loss_d.item()))\n",
        "\n",
        "        # ===== END OF EPOCH: PRINT LOSSES =====\n",
        "        if gen_losses:\n",
        "            print(\"GEN avg loss:\", sum(gen_losses)/len(gen_losses))\n",
        "        else:\n",
        "            print(\"GEN avg loss: no updates\")\n",
        "\n",
        "        if disc_losses:\n",
        "            print(\"DISC avg loss:\", sum(disc_losses)/len(disc_losses))\n",
        "        else:\n",
        "            print(\"DISC avg loss: no updates\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld63FZFHfviv"
      },
      "source": [
        "# **new training V2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6VJQp9Iv3FC"
      },
      "outputs": [],
      "source": [
        "SAVE_DIR = \"/content/drive/MyDrive/cs-senti/gan_synthetic2\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wi7FNUaXBe6k"
      },
      "outputs": [],
      "source": [
        "#lolll\n",
        "print(\"====== TRAINING ON EESA ======\")\n",
        "\n",
        "train_gan(\n",
        "    eesa_train_annotated,\n",
        "    epochs=3,\n",
        "    batch_size=8,\n",
        "    generator=generator,\n",
        "    optimizer=optimizer,\n",
        "    disc_model=disc_model,\n",
        "    disc_tokenizer=disc_tokenizer,\n",
        "    reward_model=reward_model,\n",
        "    reward_tokenizer=reward_tokenizer,\n",
        "    sim_model=similarity_model\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1Jp7MnfBsSE"
      },
      "outputs": [],
      "source": [
        "eesa_gan_samples2 = generate_all_samples(\n",
        "    eesa_train_annotated,\n",
        "    generator,\n",
        "    reward_tokenizer,\n",
        "    reward_model\n",
        ")\n",
        "\n",
        "with open(f\"{SAVE_DIR}/eesa_gan_samples.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for row in eesa_gan_samples2:\n",
        "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"Saved eesa-CS GAN synthetic data.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvn2YXD2B4_E"
      },
      "outputs": [],
      "source": [
        "print(\"====== ALIGN AMG-CS ======\")\n",
        "align_domain(det_rows, eesa_gan_samples2, domain=\"eesa\", limit=20)\n",
        "def show_examples(samples, title=\"Examples\"):\n",
        "    \"\"\"\n",
        "    Display ALL generated GAN samples for a domain.\n",
        "    No max_show limit.\n",
        "    \"\"\"\n",
        "    print(f\"\\n========== {title} ==========\\n\")\n",
        "\n",
        "    for ex in samples:\n",
        "        print(f\"ID        : {ex.get('id', 'N/A')}\")\n",
        "        print(f\"ORIGINAL  : {ex['original']}\")\n",
        "        print(f\"GAN       : {ex['generated']}\")\n",
        "        print(f\"LABEL     : {ex['label']}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "show_examples(eesa_gan_samples2,  \"AMG Synthetic GAN Data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O92K8YKCrc0K"
      },
      "outputs": [],
      "source": [
        "#lolll\n",
        "print(\"====== TRAINING ON AMG-CS ======\")\n",
        "\n",
        "train_gan(\n",
        "    amg_cs_labeled,\n",
        "    epochs=3,\n",
        "    batch_size=8,\n",
        "    generator=generator,\n",
        "    optimizer=optimizer,\n",
        "    disc_model=disc_model,\n",
        "    disc_tokenizer=disc_tokenizer,\n",
        "    reward_model=reward_model,\n",
        "    reward_tokenizer=reward_tokenizer,\n",
        "    sim_model=similarity_model\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-zRbc25v7sS"
      },
      "outputs": [],
      "source": [
        "def generate_all_samples(dataset, generator, reward_tokenizer, reward_model):\n",
        "    \"\"\"\n",
        "    Generate GAN synthetic data for the entire dataset.\n",
        "    Includes the original sample ID for alignment.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "\n",
        "    for ex in dataset:\n",
        "        gan_text = gan_generate_text(ex, generator, reward_tokenizer, reward_model)\n",
        "        out.append({\n",
        "            \"id\": ex[\"id\"],\n",
        "            \"original\": ex[\"original\"],\n",
        "            \"generated\": gan_text,\n",
        "            \"label\": ex[\"label\"]\n",
        "        })\n",
        "\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Et-VazOwBmE"
      },
      "outputs": [],
      "source": [
        "amg_gan_samples2 = generate_all_samples(\n",
        "    amg_cs_labeled,\n",
        "    generator,\n",
        "    reward_tokenizer,\n",
        "    reward_model\n",
        ")\n",
        "\n",
        "with open(f\"{SAVE_DIR}/amg_gan_samples.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for row in amg_gan_samples2:\n",
        "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"Saved AMG-CS GAN synthetic data.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSkVkTkSwUgA"
      },
      "outputs": [],
      "source": [
        "DET_PATH = \"/content/drive/MyDrive/cs-senti/data/ling/host_train_switched_lex_llm.jsonl\"\n",
        "\n",
        "det_rows = []\n",
        "with open(DET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            det_rows.append(json.loads(line))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "print(\"Loaded deterministic:\", len(det_rows))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPuBDy-iw90F"
      },
      "outputs": [],
      "source": [
        "def align_domain(det_rows, gan_samples, domain=None, limit=20):\n",
        "    # Build dict original → deterministic\n",
        "    det_map = {row[\"orig_text\"]: row[\"switched_text\"] for row in det_rows if (domain is None or row.get(\"domain\") == domain)}\n",
        "\n",
        "    gan_map = {row[\"original\"]: row for row in gan_samples}\n",
        "\n",
        "    aligned = []\n",
        "    for orig, gan_row in gan_map.items():\n",
        "        det_sw = det_map.get(orig, None)\n",
        "        aligned.append({\n",
        "            \"id\": gan_row[\"id\"],\n",
        "            \"original\": orig,\n",
        "            \"deterministic\": det_sw,\n",
        "            \"gan\": gan_row[\"generated\"],\n",
        "            \"label\": gan_row[\"label\"]\n",
        "        })\n",
        "\n",
        "    # print first N\n",
        "    for row in aligned[:limit]:\n",
        "        print(\"ID:\", row[\"id\"])\n",
        "        print(\"ORIGINAL:\", row[\"original\"])\n",
        "        print(\"DETERMINISTIC:\", row[\"deterministic\"])\n",
        "        print(\"GAN:\", row[\"gan\"])\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "    return aligned\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jefWR_iwxCB1"
      },
      "outputs": [],
      "source": [
        "print(\"====== ALIGN AMG-CS ======\")\n",
        "align_domain(det_rows, amg_gan_samples2, domain=\"amg\", limit=20)\n",
        "def show_examples(samples, title=\"Examples\"):\n",
        "    \"\"\"\n",
        "    Display ALL generated GAN samples for a domain.\n",
        "    No max_show limit.\n",
        "    \"\"\"\n",
        "    print(f\"\\n========== {title} ==========\\n\")\n",
        "\n",
        "    for ex in samples:\n",
        "        print(f\"ID        : {ex.get('id', 'N/A')}\")\n",
        "        print(f\"ORIGINAL  : {ex['original']}\")\n",
        "        print(f\"GAN       : {ex['generated']}\")\n",
        "        print(f\"LABEL     : {ex['label']}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "show_examples(amg_gan_samples2,  \"AMG Synthetic GAN Data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPRk9QzqyC9U"
      },
      "outputs": [],
      "source": [
        "evaluate_domain(\"AMG-CS\", amg_gan_samples2,\n",
        "                reward_tokenizer, reward_model,\n",
        "                similarity_model,\n",
        "                disc_tokenizer, disc_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPlYqeAN0-uQ"
      },
      "outputs": [],
      "source": [
        "print(\"====== TRAINING ON MR-CS ======\")\n",
        "\n",
        "train_gan(\n",
        "    mr_cs_labeled,\n",
        "    epochs=3,\n",
        "    batch_size=8,\n",
        "    generator=generator,\n",
        "    optimizer=optimizer,\n",
        "    disc_model=disc_model,\n",
        "    disc_tokenizer=disc_tokenizer,\n",
        "    reward_model=reward_model,\n",
        "    reward_tokenizer=reward_tokenizer,\n",
        "    sim_model=similarity_model\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7-LND3P03wH"
      },
      "outputs": [],
      "source": [
        "\n",
        "mr_gan_samples2 = generate_all_samples(\n",
        "    mr_cs_labeled,\n",
        "    generator,\n",
        "    reward_tokenizer,\n",
        "    reward_model\n",
        ")\n",
        "\n",
        "with open(f\"{SAVE_DIR}/mr_gan_samples.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for row in mr_gan_samples2:\n",
        "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"Saved MR-CS GAN synthetic data.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66iJJGzI1FCW"
      },
      "outputs": [],
      "source": [
        "evaluate_domain(\"AMG-CS\", mr_gan_samples2,\n",
        "                reward_tokenizer, reward_model,\n",
        "                similarity_model,\n",
        "                disc_tokenizer, disc_model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def normalize(t):\n",
        "    if t is None:\n",
        "        return \"\"\n",
        "    t = t.strip()\n",
        "    t = re.sub(r\"\\s+\", \" \", t)  # collapse spaces\n",
        "    t = re.sub(r\"[ـ]+\", \"\", t)  # remove tatweel or filler chars\n",
        "    t = re.sub(r\"[^\\w\\s\\u0600-\\u06FF]+$\", \"\", t)  # strip trailing punctuation\n",
        "    return t\n",
        "\n",
        "def load_gan(path):\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            obj = json.loads(line)\n",
        "            rows.append({\n",
        "                \"id\": obj.get(\"id\"),\n",
        "                \"original\": normalize(obj.get(\"original\", \"\")),\n",
        "                \"text\": normalize(obj.get(\"generated\", \"\")),\n",
        "                \"label\": obj.get(\"label\"),\n",
        "            })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "eesa_gan = load_gan(\"/content/drive/MyDrive/cs-senti/gan_synthetic2/eesa_gan_samples.jsonl\")\n",
        "amg_gan  = load_gan(\"/content/drive/MyDrive/cs-senti/gan_synthetic2/amg_gan_samples.jsonl\")\n",
        "mr_gan   = load_gan(\"/content/drive/MyDrive/cs-senti/gan_synthetic2/mr_gan_samples.jsonl\")\n"
      ],
      "metadata": {
        "id": "sadNPjmVZBh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_no_change(df):\n",
        "    before = len(df)\n",
        "    df = df[df[\"text\"] != df[\"original\"]].reset_index(drop=True)\n",
        "    print(f\"Removed {before - len(df)} unchanged samples out of {before}\")\n",
        "    return df\n",
        "\n",
        "eesa_gan_f = remove_no_change(eesa_gan)\n",
        "amg_gan_f  = remove_no_change(amg_gan)\n",
        "mr_gan_f   = remove_no_change(mr_gan)\n"
      ],
      "metadata": {
        "id": "2hLvz5y_ZDV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/cs-senti/repo/data/cleaned\")\n",
        "\n",
        "def load_clean(path):\n",
        "    return pd.DataFrame([json.loads(l) for l in open(path, \"r\", encoding=\"utf-8\")])\n",
        "\n",
        "eesa = load_clean(BASE / \"eesa_train_clean.jsonl\")\n",
        "mr   = load_clean(BASE / \"mr_cs_clean.jsonl\")\n",
        "amg  = load_clean(BASE / \"amg_cs_clean.jsonl\")\n",
        "\n",
        "print(len(eesa), len(mr), len(amg))\n"
      ],
      "metadata": {
        "id": "sUBlM_LPZPPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"EESA columns:\", eesa.columns.tolist())\n",
        "print(\"MR-CS columns:\", mr.columns.tolist())\n",
        "print(\"AMG-CS columns:\", amg.columns.tolist())\n"
      ],
      "metadata": {
        "id": "DIboveHyaJGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"EESA-GAN columns:\", eesa_gan_f.columns.tolist())\n",
        "print(\"AMG-GAN columns:\", amg_gan_f.columns.tolist())\n",
        "print(\"MR-GAN columns:\", mr_gan_f.columns.tolist())\n"
      ],
      "metadata": {
        "id": "WHWc0Cl0aL1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the original text column (not needed)\n",
        "eesa_gan_clean = eesa_gan_f.drop(columns=[\"original\"])\n",
        "amg_gan_clean  = amg_gan_f.drop(columns=[\"original\"])\n",
        "mr_gan_clean   = mr_gan_f.drop(columns=[\"original\"])\n",
        "\n",
        "print(eesa_gan_clean.columns)\n",
        "print(amg_gan_clean.columns)\n",
        "print(mr_gan_clean.columns)\n"
      ],
      "metadata": {
        "id": "hQGB1G3yagRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eesa_orig_small = eesa[['text','label']]\n",
        "mr_orig_small   = mr[['text','label']]\n",
        "amg_orig_small  = amg[['text','label']]\n",
        "\n",
        "eesa_gan_small = eesa_gan_clean[['text','label']]\n",
        "amg_gan_small  = amg_gan_clean[['text','label']]\n",
        "mr_gan_small   = mr_gan_clean[['text','label']]\n"
      ],
      "metadata": {
        "id": "iCjyoZf4azeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eesa_merged = pd.concat([eesa_orig_small, eesa_gan_small], ignore_index=True)\n",
        "mr_merged   = pd.concat([mr_orig_small, mr_gan_small], ignore_index=True)\n",
        "amg_merged  = pd.concat([amg_orig_small, amg_gan_small], ignore_index=True)\n"
      ],
      "metadata": {
        "id": "UFquxBAea1tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eesa_merged = eesa_merged.drop_duplicates(subset=\"text\")\n",
        "mr_merged   = mr_merged.drop_duplicates(subset=\"text\")\n",
        "amg_merged  = amg_merged.drop_duplicates(subset=\"text\")\n"
      ],
      "metadata": {
        "id": "FEzBYkZqa4tD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unified_gan_augmented = pd.concat(\n",
        "    [eesa_merged, mr_merged, amg_merged],\n",
        "    ignore_index=True\n",
        ").drop_duplicates(subset=\"text\")\n",
        "\n",
        "print(\"Final unified size:\", len(unified_gan_augmented))\n",
        "unified_gan_augmented.head()\n"
      ],
      "metadata": {
        "id": "VVyyb7lNa6My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to save\n",
        "out_path_jsonl = \"/content/drive/MyDrive/cs-senti/gan_synthetic2/unified_gan_augmented.jsonl\"\n",
        "out_path_csv   = \"/content/drive/MyDrive/cs-senti/gan_synthetic2/unified_gan_augmented.csv\"\n",
        "\n",
        "# Save as JSONL\n",
        "unified_gan_augmented.to_json(\n",
        "    out_path_jsonl,\n",
        "    orient=\"records\",\n",
        "    lines=True,\n",
        "    force_ascii=False\n",
        ")\n",
        "\n",
        "# Save as CSV\n",
        "unified_gan_augmented.to_csv(\n",
        "    out_path_csv,\n",
        "    index=False\n",
        ")\n",
        "\n",
        "print(\"Saved JSONL to:\", out_path_jsonl)\n",
        "print(\"Saved CSV to:\", out_path_csv)\n",
        "print(\"Final dataset size:\", len(unified_gan_augmented))\n",
        "unified_gan_augmented.head()\n"
      ],
      "metadata": {
        "id": "xxOLW7A4bbCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/cs-senti/gan_synthetic2/unified_gan_augmented.csv\")\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "pPRTC0s7bxLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df[\"label\"].isna() | ~df[\"label\"].isin([\"neg\", \"neu\", \"pos\"])].head(20)\n"
      ],
      "metadata": {
        "id": "44TMXwFqcXqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df[\"label\"].isin([\"neg\", \"neu\", \"pos\"])]\n",
        "df = df.dropna(subset=[\"label\"])\n"
      ],
      "metadata": {
        "id": "LRwwFgi8cbP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label2id = {\"neg\":0, \"neu\":1, \"pos\":2}\n",
        "id2label = {v:k for k,v in label2id.items()}\n",
        "\n",
        "df[\"label_id\"] = df[\"label\"].map(label2id)\n"
      ],
      "metadata": {
        "id": "GZStyq7Ecdfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/cs-senti/repo/data/cleaned\")\n",
        "GAN  = Path(\"/content/drive/MyDrive/cs-senti/gan_synthetic2\")\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# 1. LOAD ORIGINAL CLEAN DATA\n",
        "# ---------------------------\n",
        "def load_jsonl(path):\n",
        "    return [json.loads(l) for l in open(path, \"r\", encoding=\"utf-8\")]\n",
        "\n",
        "# Original datasets\n",
        "eesa_train = pd.DataFrame(load_jsonl(BASE/\"eesa_train_clean.jsonl\"))\n",
        "eesa_dev   = pd.DataFrame(load_jsonl(BASE/\"eesa_dev_clean.jsonl\"))\n",
        "eesa_test  = pd.DataFrame(load_jsonl(BASE/\"eesa_test_clean.jsonl\"))\n",
        "\n",
        "mr_df  = pd.DataFrame(load_jsonl(BASE/\"mr_cs_clean.jsonl\"))\n",
        "amg_df = pd.DataFrame(load_jsonl(BASE/\"amg_cs_clean.jsonl\"))\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 2. LOAD UNIFIED GAN-AUGMENTED\n",
        "# -------------------------------\n",
        "unified_gan = pd.read_json(GAN/\"unified_gan_augmented.jsonl\", lines=True)\n",
        "\n",
        "print(\"Train (GAN-augmented) size:\", len(unified_gan))\n",
        "print(\"Dev (EESA) size:\", len(eesa_dev))\n",
        "print(\"Test (EESA) size:\", len(eesa_test))\n",
        "print(\"Test (MR) size:\", len(mr_df))\n",
        "print(\"Test (AMG) size:\", len(amg_df))\n",
        "\n",
        "\n",
        "# --------------------------------------\n",
        "# 3. ASSIGN TRAIN / DEV / TEST SPLITS\n",
        "# --------------------------------------\n",
        "\n",
        "train_df = unified_gan.copy()   # GAN-augmented unified training set\n",
        "dev_df   = eesa_dev.copy()      # EESA dev for validation\n",
        "test_in  = eesa_test.copy()     # EESA test (in-domain)\n",
        "test_mr  = mr_df.copy()         # MR test (cross-domain)\n",
        "test_amg = amg_df.copy()        # AMG test (generalization)\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# 4. SHOW FINAL SHAPES\n",
        "# -------------------------------\n",
        "print(\"\\n===== FINAL SPLITS =====\")\n",
        "print(\"TRAIN:\", len(train_df))\n",
        "print(\"DEV:\", len(dev_df))\n",
        "print(\"TEST-IN-DOMAIN (EESA):\", len(test_in))\n",
        "print(\"TEST-MR:\", len(test_mr))\n",
        "print(\"TEST-AMG:\", len(test_amg))\n"
      ],
      "metadata": {
        "id": "FJPjvbTocisf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Paths\n",
        "base = \"/content/drive/MyDrive/cs-senti\"\n",
        "\n",
        "train_path = f\"{base}/gan_synthetic2/unified_gan_augmented.csv\"\n",
        "eesa_dev_path = f\"{base}/repo/data/cleaned/eesa_dev_clean.jsonl\"\n",
        "eesa_test_path = f\"{base}/repo/data/cleaned/eesa_test_clean.jsonl\"\n",
        "mr_path = f\"{base}/repo/data/cleaned/mr_cs_clean.jsonl\"\n",
        "amg_path = f\"{base}/repo/data/cleaned/amg_cs_clean.jsonl\"\n",
        "\n",
        "# Load\n",
        "train_df = pd.read_csv(train_path)\n",
        "dev_df   = pd.read_json(eesa_dev_path, lines=True)\n",
        "test_df  = pd.read_json(eesa_test_path, lines=True)\n",
        "mr_df    = pd.read_json(mr_path, lines=True)\n",
        "amg_df   = pd.read_json(amg_path, lines=True)\n",
        "\n",
        "print(len(train_df), len(dev_df), len(test_df), len(mr_df), len(amg_df))\n"
      ],
      "metadata": {
        "id": "qIDavM0Xfm4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "base = \"/content/drive/MyDrive/cs-senti\"\n",
        "\n",
        "# Paths\n",
        "train_path = f\"{base}/gan_synthetic2/unified_gan_augmented.csv\"\n",
        "eesa_dev_path = f\"{base}/repo/data/cleaned/eesa_dev_clean.jsonl\"\n",
        "eesa_test_path = f\"{base}/repo/data/cleaned/eesa_test_clean.jsonl\"\n",
        "mr_path = f\"{base}/repo/data/cleaned/mr_cs_clean.jsonl\"\n",
        "amg_path = f\"{base}/repo/data/cleaned/amg_cs_clean.jsonl\"\n",
        "\n",
        "# ----------------------------\n",
        "# Helper: load + standardize\n",
        "# ----------------------------\n",
        "def load_and_fix(path, filetype=\"jsonl\"):\n",
        "    if filetype == \"jsonl\":\n",
        "        df = pd.read_json(path, lines=True)\n",
        "    else:\n",
        "        df = pd.read_csv(path)\n",
        "\n",
        "    # Standardize column names\n",
        "    if \"text\" not in df.columns:\n",
        "        raise ValueError(f\"Missing 'text' in {path}\")\n",
        "\n",
        "    if \"label\" not in df.columns:\n",
        "        raise ValueError(f\"Missing 'label' in {path}\")\n",
        "\n",
        "    # Keep only text + label\n",
        "    df = df[[\"text\", \"label\"]]\n",
        "\n",
        "    # Drop NaN labels if any\n",
        "    df = df.dropna(subset=[\"label\"])\n",
        "\n",
        "    # Strip text\n",
        "    df[\"text\"] = df[\"text\"].astype(str).str.strip()\n",
        "\n",
        "    return df\n",
        "\n",
        "# ----------------------------\n",
        "# Load all datasets\n",
        "# ----------------------------\n",
        "train_df = load_and_fix(train_path, filetype=\"csv\")\n",
        "dev_df   = load_and_fix(eesa_dev_path)\n",
        "test_df  = load_and_fix(eesa_test_path)\n",
        "mr_df    = load_and_fix(mr_path)\n",
        "amg_df   = load_and_fix(amg_path)\n",
        "\n",
        "print(\n",
        "    \"Train:\", len(train_df),\n",
        "    \"\\nDev:\", len(dev_df),\n",
        "    \"\\nTest:\", len(test_df),\n",
        "    \"\\nMR:\", len(mr_df),\n",
        "    \"\\nAMG:\", len(amg_df)\n",
        ")\n"
      ],
      "metadata": {
        "id": "hac99ShjjPN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {\"neg\":0, \"neu\":1, \"pos\":2}\n",
        "\n",
        "def encode_df(df):\n",
        "    df = df.copy()\n",
        "    df[\"y\"] = df[\"label\"].map(label_map)\n",
        "    return df\n",
        "\n",
        "train_df = encode_df(train_df)\n",
        "dev_df   = encode_df(dev_df)\n",
        "test_df  = encode_df(test_df)\n",
        "mr_df    = encode_df(mr_df)\n",
        "amg_df   = encode_df(amg_df)\n"
      ],
      "metadata": {
        "id": "D-p39FzHgYuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    ngram_range=(1,2),\n",
        "    min_df=3,\n",
        "    max_df=0.95,\n",
        "    sublinear_tf=True\n",
        ")\n",
        "\n",
        "X_train = vectorizer.fit_transform(train_df[\"text\"])\n",
        "X_dev   = vectorizer.transform(dev_df[\"text\"])\n",
        "X_test  = vectorizer.transform(test_df[\"text\"])\n",
        "X_mr    = vectorizer.transform(mr_df[\"text\"])\n",
        "X_amg   = vectorizer.transform(amg_df[\"text\"])\n",
        "\n",
        "y_train = train_df[\"y\"]\n",
        "y_dev   = dev_df[\"y\"]\n",
        "y_test  = test_df[\"y\"]\n",
        "y_mr    = mr_df[\"y\"]\n",
        "y_amg   = amg_df[\"y\"]\n"
      ],
      "metadata": {
        "id": "c3oZLTHDgahV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {\"neg\":0, \"neu\":1, \"pos\":2}\n",
        "train_df[\"y\"] = train_df[\"label\"].map(label_map)\n",
        "\n",
        "# Check again\n",
        "train_df[\"y\"].isna().sum()\n"
      ],
      "metadata": {
        "id": "UhXgw88Vhhbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"label\"].value_counts(dropna=False)\n"
      ],
      "metadata": {
        "id": "2lgcbkPZhjNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {\"neg\":0, \"neu\":1, \"pos\":2}\n",
        "train_df[\"y\"] = train_df[\"label\"].map(label_map)\n",
        "dev_df[\"y\"]   = dev_df[\"label\"].map(label_map)\n",
        "test_df[\"y\"]  = test_df[\"label\"].map(label_map)\n"
      ],
      "metadata": {
        "id": "waW3_LschvUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    sublinear_tf=True,\n",
        "    max_features=50000,\n",
        "    ngram_range=(1,2),\n",
        "    min_df=2\n",
        ")\n",
        "\n",
        "X_train = vectorizer.fit_transform(train_df[\"text\"])\n",
        "y_train = train_df[\"y\"]\n",
        "\n",
        "X_dev = vectorizer.transform(dev_df[\"text\"])\n",
        "y_dev = dev_df[\"y\"]\n",
        "\n",
        "X_test = vectorizer.transform(test_df[\"text\"])\n",
        "y_test = test_df[\"y\"]\n"
      ],
      "metadata": {
        "id": "xtxFW4Dihwql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(\n",
        "    max_iter=3000,\n",
        "    class_weight=\"balanced\",\n",
        "    n_jobs=-1,\n",
        "    C=3.0\n",
        ")\n",
        "\n",
        "lr.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "P_efmFAbhyfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "pred_dev = lr.predict(X_dev)\n",
        "pred_test = lr.predict(X_test)\n",
        "\n",
        "print(\"DEV F1:\", f1_score(y_dev, pred_dev, average=\"macro\"))\n",
        "print(\"TEST F1:\", f1_score(y_test, pred_test, average=\"macro\"))\n",
        "\n",
        "print(\"\\nTest Classification Report:\\n\")\n",
        "print(classification_report(y_test, pred_test, target_names=[\"neg\",\"neu\",\"pos\"]))\n"
      ],
      "metadata": {
        "id": "Yr32k7oBh02Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "svm = LinearSVC(\n",
        "    C=1.0,\n",
        "    class_weight=\"balanced\"\n",
        ")\n",
        "\n",
        "svm.fit(X_train, y_train)\n",
        "pred_dev = svm.predict(X_dev)\n",
        "print(\"DEV F1:\", f1_score(y_dev, pred_dev, average=\"macro\"))\n",
        "pred_test = svm.predict(X_test)\n",
        "print(\"EESA TEST F1:\", f1_score(y_test, pred_test, average=\"macro\"))\n",
        "print(\"\\nClassification Report (EESA TEST):\\n\")\n",
        "print(classification_report(y_test, pred_test, target_names=[\"neg\",\"neu\",\"pos\"]))\n"
      ],
      "metadata": {
        "id": "dzhwZNJViKQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_mr = vectorizer.transform(mr_test_df[\"text\"])\n",
        "y_mr = mr_test_df[\"label\"].map(label_map)\n",
        "\n",
        "pred_mr = svm.predict(X_mr)\n",
        "print(\"MR Generalization F1:\", f1_score(y_mr, pred_mr, average=\"macro\"))\n",
        "X_amg = vectorizer.transform(amg_test_df[\"text\"])\n",
        "y_amg = amg_test_df[\"label\"].map(label_map)\n",
        "\n",
        "pred_amg = svm.predict(X_amg)\n",
        "print(\"AMG Generalization F1:\", f1_score(y_amg, pred_amg, average=\"macro\"))\n"
      ],
      "metadata": {
        "id": "CHVn2Ds8iQAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Paths\n",
        "base = \"/content/drive/MyDrive/cs-senti\"\n",
        "\n",
        "train_path = f\"{base}/gan_synthetic2/unified_gan_augmented.csv\"\n",
        "eesa_dev_path = f\"{base}/repo/data/cleaned/eesa_dev_clean.jsonl\"\n",
        "eesa_test_path = f\"{base}/repo/data/cleaned/eesa_test_clean.jsonl\"\n",
        "mr_path = f\"{base}/repo/data/cleaned/mr_cs_clean.jsonl\"\n",
        "amg_path = f\"{base}/repo/data/cleaned/amg_cs_clean.jsonl\"\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv(train_path)\n",
        "dev_df   = pd.read_json(eesa_dev_path, lines=True)\n",
        "test_df  = pd.read_json(eesa_test_path, lines=True)\n",
        "mr_df    = pd.read_json(mr_path, lines=True)\n",
        "amg_df   = pd.read_json(amg_path, lines=True)\n",
        "\n",
        "print(len(train_df), len(dev_df), len(test_df), len(mr_df), len(amg_df))\n"
      ],
      "metadata": {
        "id": "dGZpT3pFj1SQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {\"pos\": 2, \"neu\": 1, \"neg\": 0}\n",
        "\n",
        "train_df[\"y\"] = train_df[\"label\"].map(label_map)\n",
        "dev_df[\"y\"]   = dev_df[\"label\"].map(label_map)\n",
        "test_df[\"y\"]  = test_df[\"label\"].map(label_map)\n",
        "mr_df[\"y\"]    = mr_df[\"label\"].map(label_map)\n",
        "amg_df[\"y\"]   = amg_df[\"label\"].map(label_map)\n"
      ],
      "metadata": {
        "id": "KNjN8akWkCZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=20000,\n",
        "    ngram_range=(1,2),\n",
        "    sublinear_tf=True\n",
        ")\n",
        "\n",
        "X_train = vectorizer.fit_transform(train_df[\"text\"])\n",
        "y_train = train_df[\"y\"]\n"
      ],
      "metadata": {
        "id": "Tpz96hiekMGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression(max_iter=5000, class_weight=\"balanced\")\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# SVM\n",
        "svm = LinearSVC(class_weight=\"balanced\")\n",
        "svm.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "naZ7TS9QkNtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[train_df[\"label\"].isna()].head()\n",
        "bad_labels = train_df[train_df[\"y\"].isna()]\n",
        "bad_labels\n",
        "train_df[\"y\"].isna().sum()\n"
      ],
      "metadata": {
        "id": "nny0pb5kxCsQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove any rows with missing labels\n",
        "train_df = train_df.dropna(subset=[\"label\"])\n",
        "\n",
        "# Ensure labels belong to {pos, neu, neg}\n",
        "valid = {\"pos\", \"neu\", \"neg\"}\n",
        "train_df = train_df[train_df[\"label\"].isin(valid)]\n",
        "\n",
        "# Remap labels after cleaning\n",
        "train_df[\"y\"] = train_df[\"label\"].map({\"pos\":2, \"neu\":1, \"neg\":0})\n"
      ],
      "metadata": {
        "id": "ONOLyy3SxMGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"y\"].isna().sum()\n"
      ],
      "metadata": {
        "id": "BauW93LexNoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = vectorizer.fit_transform(train_df[\"text\"])\n",
        "y_train = train_df[\"y\"]\n"
      ],
      "metadata": {
        "id": "lE74JacCxP4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def eval_model(model, df, name):\n",
        "    X = vectorizer.transform(df[\"text\"])\n",
        "    y = df[\"y\"]\n",
        "    preds = model.predict(X)\n",
        "    f1 = f1_score(y, preds, average=\"macro\")\n",
        "    print(f\"{name}: {f1:.4f}\")\n",
        "    return f1\n"
      ],
      "metadata": {
        "id": "J6uwkhomxXLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"====== LOGISTIC REGRESSION RESULTS ======\")\n",
        "eval_model(lr, dev_df,  \"EESA Dev\")\n",
        "eval_model(lr, test_df, \"EESA Test\")\n",
        "eval_model(lr, mr_df,   \"MR Generalization\")\n",
        "eval_model(lr, amg_df,  \"AMG Generalization\")\n",
        "\n",
        "print(\"\\n====== SVM RESULTS ======\")\n",
        "eval_model(svm, dev_df,  \"EESA Dev\")\n",
        "eval_model(svm, test_df, \"EESA Test\")\n",
        "eval_model(svm, mr_df,   \"MR Generalization\")\n",
        "eval_model(svm, amg_df,  \"AMG Generalization\")\n"
      ],
      "metadata": {
        "id": "gbrSvShnxaQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "neural"
      ],
      "metadata": {
        "id": "fyrWeKoHyHMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Confirm train/dev/test\n",
        "print(len(train_df), len(dev_df), len(test_df), len(mr_df), len(amg_df))\n",
        "\n",
        "# Extract text + labels\n",
        "train_texts = train_df[\"text\"].astype(str).tolist()\n",
        "dev_texts   = dev_df[\"text\"].astype(str).tolist()\n",
        "test_texts  = test_df[\"text\"].astype(str).tolist()\n",
        "mr_texts    = mr_df[\"text\"].astype(str).tolist()\n",
        "amg_texts   = amg_df[\"text\"].astype(str).tolist()\n",
        "\n",
        "train_labels = train_df[\"label\"].tolist()\n",
        "dev_labels   = dev_df[\"label\"].tolist()\n",
        "test_labels  = test_df[\"label\"].tolist()\n",
        "mr_labels    = mr_df[\"label\"].tolist()\n",
        "amg_labels   = amg_df[\"label\"].tolist()\n",
        "\n",
        "# Label mapping\n",
        "label_map = {\"neg\":0, \"neu\":1, \"pos\":2}\n",
        "\n",
        "y_train = np.array([label_map[x] for x in train_labels])\n",
        "y_dev   = np.array([label_map[x] for x in dev_labels])\n",
        "y_test  = np.array([label_map[x] for x in test_labels])\n",
        "y_mr    = np.array([label_map[x] for x in mr_labels])\n",
        "y_amg   = np.array([label_map[x] for x in amg_labels])\n"
      ],
      "metadata": {
        "id": "hp80I6geyGlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_WORDS = 30000\n",
        "MAX_LEN = 40   # tuned for short CS sentences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_texts)\n",
        "\n",
        "def encode(texts):\n",
        "    seq = tokenizer.texts_to_sequences(texts)\n",
        "    return pad_sequences(seq, maxlen=MAX_LEN, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "X_train = encode(train_texts)\n",
        "X_dev   = encode(dev_texts)\n",
        "X_test  = encode(test_texts)\n",
        "X_mr    = encode(mr_texts)\n",
        "X_amg   = encode(amg_texts)\n",
        "\n",
        "print(X_train.shape, X_dev.shape, X_test.shape)\n"
      ],
      "metadata": {
        "id": "QjncFpqdyKwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "EMB_DIM = 128\n",
        "LSTM_UNITS = 128\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Embedding(MAX_WORDS, EMB_DIM, input_length=MAX_LEN),\n",
        "    layers.Bidirectional(layers.LSTM(LSTM_UNITS, return_sequences=False)),\n",
        "    layers.Dense(128, activation=\"relu\"),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(3, activation=\"softmax\"),\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "C2mKX8OqyNGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets evaluate accelerate -q\n"
      ],
      "metadata": {
        "id": "amxmd4Ne3sa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# Label mapping\n",
        "label_map = {\"neg\": 0, \"neu\": 1, \"pos\": 2}\n",
        "\n",
        "train_df[\"label\"] = train_df[\"label\"].map(label_map)\n",
        "dev_df[\"label\"]   = dev_df[\"label\"].map(label_map)\n",
        "test_df[\"label\"]  = test_df[\"label\"].map(label_map)\n",
        "mr_df[\"label\"]    = mr_df[\"label\"].map(label_map)\n",
        "amg_df[\"label\"]   = amg_df[\"label\"].map(label_map)\n",
        "\n",
        "# Convert to HF datasets\n",
        "ds_train = Dataset.from_pandas(train_df[[\"text\", \"label\"]])\n",
        "ds_dev   = Dataset.from_pandas(dev_df[[\"text\", \"label\"]])\n",
        "ds_test  = Dataset.from_pandas(test_df[[\"text\", \"label\"]])\n",
        "ds_mr    = Dataset.from_pandas(mr_df[[\"text\", \"label\"]])\n",
        "ds_amg   = Dataset.from_pandas(amg_df[[\"text\", \"label\"]])\n"
      ],
      "metadata": {
        "id": "kqojL7L83vGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"UBC-NLP/MARBERTv2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "MAX_LEN = 64\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN\n",
        "    )\n",
        "\n",
        "ds_train_enc = ds_train.map(tokenize, batched=True)\n",
        "ds_dev_enc   = ds_dev.map(tokenize, batched=True)\n",
        "ds_test_enc  = ds_test.map(tokenize, batched=True)\n",
        "ds_mr_enc    = ds_mr.map(tokenize, batched=True)\n",
        "ds_amg_enc   = ds_amg.map(tokenize, batched=True)\n",
        "\n",
        "cols = [\"input_ids\", \"attention_mask\", \"label\"]\n",
        "ds_train_enc.set_format(\"torch\", columns=cols)\n",
        "ds_dev_enc.set_format(\"torch\", columns=cols)\n",
        "ds_test_enc.set_format(\"torch\", columns=cols)\n",
        "ds_mr_enc.set_format(\"torch\", columns=cols)\n",
        "ds_amg_enc.set_format(\"torch\", columns=cols)\n"
      ],
      "metadata": {
        "id": "cKRanFua31Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers datasets evaluate accelerate -q\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import Dataset, DatasetDict\n",
        "import json\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from sklearn.metrics import classification_report\n",
        "import os\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Helper: Load JSONL\n",
        "# ============================================================\n",
        "def load_jsonl(path):\n",
        "    return [json.loads(l) for l in open(path, \"r\", encoding=\"utf-8\")]\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# FIX: unify all augmented data to {text, label}\n",
        "# ============================================================\n",
        "clean_train = []\n",
        "for item in combined_aug:\n",
        "    obj = {}\n",
        "\n",
        "    # choose generated text if available, else text\n",
        "    if \"generated\" in item and item[\"generated\"]:\n",
        "        obj[\"text\"] = item[\"generated\"].strip()\n",
        "    else:\n",
        "        obj[\"text\"] = item[\"text\"].strip()\n",
        "\n",
        "    obj[\"label\"] = item[\"label\"]\n",
        "    clean_train.append(obj)\n",
        "\n",
        "label2id = {\"neg\":0, \"neu\":1, \"pos\":2}\n",
        "id2label = {0:\"neg\", 1:\"neu\", 2:\"pos\"}\n",
        "\n",
        "def encode(example):\n",
        "    example[\"label\"] = label2id[example[\"label\"]]\n",
        "    return example\n",
        "\n",
        "train_ds = Dataset.from_list(clean_train).map(encode)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# DEV + TEST (EESA)\n",
        "# ============================================================\n",
        "DEV = \"/content/drive/MyDrive/cs-senti/repo/data/cleaned/eesa_dev_clean.jsonl\"\n",
        "TEST = \"/content/drive/MyDrive/cs-senti/repo/data/cleaned/eesa_test_clean.jsonl\"\n",
        "\n",
        "eesa_dev = Dataset.from_list(load_jsonl(DEV)).map(encode)\n",
        "eesa_test = Dataset.from_list(load_jsonl(TEST)).map(encode)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# DatasetDict\n",
        "# ============================================================\n",
        "dataset = DatasetDict({\n",
        "    \"train\": train_ds,\n",
        "    \"dev\": eesa_dev,\n",
        "    \"test\": eesa_test\n",
        "})\n",
        "\n",
        "print(dataset)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TRAINING FUNCTION (fixed tokenizer handling)\n",
        "# ============================================================\n",
        "def train_any_model(model_name, output_dir):\n",
        "\n",
        "    print(f\"\\n============== Training {model_name} ==============\\n\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "    def tokenize(batch):\n",
        "        return tokenizer(\n",
        "            batch[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=128\n",
        "        )\n",
        "\n",
        "    tokenized = dataset.map(tokenize, batched=True)\n",
        "    tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
        "    tokenized.set_format(\"torch\")\n",
        "\n",
        "    # ---------------- Model ----------------\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=3,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    )\n",
        "\n",
        "    f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "    def compute_metrics(pred):\n",
        "        logits, labels = pred\n",
        "        preds = np.argmax(logits, axis=-1)\n",
        "        f1 = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")\n",
        "        return {\"macro_f1\": f1[\"f1\"]}\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=4,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        learning_rate=2e-5,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        save_total_limit=2,\n",
        "        logging_strategy=\"epoch\",\n",
        "        report_to=[]\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=tokenized[\"train\"],\n",
        "        eval_dataset=tokenized[\"dev\"],\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"\\n===== FINAL TEST RESULTS =====\")\n",
        "    test_out = trainer.evaluate(tokenized[\"test\"])\n",
        "    print(test_out)\n",
        "\n",
        "    # Full classification report\n",
        "    preds = trainer.predict(tokenized[\"test\"]).predictions\n",
        "    pred_labels = np.argmax(preds, axis=1)\n",
        "    true_labels = np.array(tokenized[\"test\"][\"labels\"])\n",
        "\n",
        "    print(\"\\n===== CLASSIFICATION REPORT =====\")\n",
        "    print(classification_report(\n",
        "        true_labels, pred_labels,\n",
        "        target_names=[\"neg\", \"neu\", \"pos\"]\n",
        "    ))\n",
        "\n",
        "    return trainer\n"
      ],
      "metadata": {
        "id": "dpX4cOCP360S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "base = \"/content/drive/MyDrive/cs-senti\"\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 1. Helper Functions\n",
        "# ============================================================\n",
        "\n",
        "def load_jsonl(path):\n",
        "    \"\"\"Reads a JSONL file into a list of dicts.\"\"\"\n",
        "    return [json.loads(l) for l in open(path, \"r\", encoding=\"utf-8\")]\n",
        "\n",
        "\n",
        "def normalize_df(df, source_name=\"\"):\n",
        "    \"\"\"\n",
        "    Normalize any dataset to have columns:\n",
        "        text, label, id\n",
        "    Remove rows with missing or invalid entries.\n",
        "    \"\"\"\n",
        "    # Standardize text column\n",
        "    if \"generated\" in df.columns:\n",
        "        df[\"text\"] = df[\"generated\"]\n",
        "    elif \"text\" in df.columns:\n",
        "        df[\"text\"] = df[\"text\"]\n",
        "    elif \"original\" in df.columns:\n",
        "        df[\"text\"] = df[\"original\"]\n",
        "    else:\n",
        "        raise ValueError(f\"No text field found in {source_name}\")\n",
        "\n",
        "    # Standardize ID\n",
        "    if \"id\" not in df.columns:\n",
        "        df[\"id\"] = range(len(df))\n",
        "\n",
        "    # Ensure correct label column\n",
        "    if \"label\" not in df.columns:\n",
        "        raise ValueError(f\"No label field found in {source_name}\")\n",
        "\n",
        "    # Clean text + label\n",
        "    df[\"text\"] = df[\"text\"].astype(str).str.strip()\n",
        "    df[\"label\"] = df[\"label\"].astype(str).str.strip()\n",
        "\n",
        "    # Filter valid labels\n",
        "    valid = {\"neg\", \"neu\", \"pos\"}\n",
        "    df = df[df[\"label\"].isin(valid)]\n",
        "\n",
        "    # Remove rows with missing text\n",
        "    df = df[df[\"text\"].notna() & (df[\"text\"] != \"\")]\n",
        "\n",
        "    # Drop unused columns\n",
        "    drop_cols = [c for c in [\"generated\", \"original\"] if c in df.columns]\n",
        "    df = df.drop(columns=drop_cols, errors=\"ignore\")\n",
        "\n",
        "    return df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2. Load ORIGINAL CLEAN DATASETS\n",
        "# ============================================================\n",
        "\n",
        "# Unified GAN-train data\n",
        "train_path = f\"{base}/gan_synthetic2/unified_gan_augmented.csv\"\n",
        "train_df = pd.read_csv(train_path)\n",
        "train_df = normalize_df(train_df, \"GAN-TRAIN\")\n",
        "\n",
        "# EESA dev/test\n",
        "eesa_dev = pd.DataFrame(load_jsonl(f\"{base}/repo/data/cleaned/eesa_dev_clean.jsonl\"))\n",
        "eesa_dev = normalize_df(eesa_dev, \"EESA-DEV\")\n",
        "\n",
        "eesa_test = pd.DataFrame(load_jsonl(f\"{base}/repo/data/cleaned/eesa_test_clean.jsonl\"))\n",
        "eesa_test = normalize_df(eesa_test, \"EESA-TEST\")\n",
        "\n",
        "# MR and AMG for generalization\n",
        "mr_df = pd.DataFrame(load_jsonl(f\"{base}/repo/data/cleaned/mr_cs_clean.jsonl\"))\n",
        "mr_df = normalize_df(mr_df, \"MR-CS\")\n",
        "\n",
        "amg_df = pd.DataFrame(load_jsonl(f\"{base}/repo/data/cleaned/amg_cs_clean.jsonl\"))\n",
        "amg_df = normalize_df(amg_df, \"AMG-CS\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3. Load GAN SYNTHETIC DATA\n",
        "# ============================================================\n",
        "\n",
        "gan_eesa = pd.DataFrame(load_jsonl(f\"{base}/gan_synthetic2/eesa_gan_samples.jsonl\"))\n",
        "gan_eesa = normalize_df(gan_eesa, \"GAN-EESA\")\n",
        "\n",
        "gan_mr = pd.DataFrame(load_jsonl(f\"{base}/gan_synthetic2/mr_gan_samples.jsonl\"))\n",
        "gan_mr = normalize_df(gan_mr, \"GAN-MR\")\n",
        "\n",
        "gan_amg = pd.DataFrame(load_jsonl(f\"{base}/gan_synthetic2/amg_gan_samples.jsonl\"))\n",
        "gan_amg = normalize_df(gan_amg, \"GAN-AMG\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4. Print Dataset Sizes\n",
        "# ============================================================\n",
        "\n",
        "print(\"TRAIN (GAN-augmented unified):\", len(train_df))\n",
        "print(\"EESA DEV:\", len(eesa_dev))\n",
        "print(\"EESA TEST:\", len(eesa_test))\n",
        "print(\"MR-CS (Generalization):\", len(mr_df))\n",
        "print(\"AMG-CS (Generalization):\", len(amg_df))\n",
        "\n",
        "print(\"GAN-EESA:\", len(gan_eesa))\n",
        "print(\"GAN-MR:\", len(gan_mr))\n",
        "print(\"GAN-AMG:\", len(gan_amg))\n",
        "\n",
        "\n",
        "# Show first rows to confirm structure\n",
        "train_df.head()\n"
      ],
      "metadata": {
        "id": "U2qSMKOS7_W1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# -------------------------------\n",
        "# LABEL MAPPING\n",
        "# -------------------------------\n",
        "label2id = {\"neg\": 0, \"neu\": 1, \"pos\": 2}\n",
        "id2label = {v:k for k,v in label2id.items()}\n",
        "\n",
        "def encode_labels(batch):\n",
        "    batch[\"label\"] = label2id[batch[\"label\"]]\n",
        "    return batch\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Convert Pandas → HF Dataset\n",
        "# -------------------------------\n",
        "train_ds = Dataset.from_pandas(train_df)\n",
        "dev_ds   = Dataset.from_pandas(eesa_dev)\n",
        "test_ds  = Dataset.from_pandas(eesa_test)\n",
        "\n",
        "# -------------------------------\n",
        "# Keep only text + label columns\n",
        "# -------------------------------\n",
        "for col in train_ds.column_names:\n",
        "    if col not in [\"text\", \"label\"]:\n",
        "        train_ds = train_ds.remove_columns(col)\n",
        "\n",
        "for col in dev_ds.column_names:\n",
        "    if col not in [\"text\", \"label\"]:\n",
        "        dev_ds = dev_ds.remove_columns(col)\n",
        "\n",
        "for col in test_ds.column_names:\n",
        "    if col not in [\"text\", \"label\"]:\n",
        "        test_ds = test_ds.remove_columns(col)\n",
        "\n",
        "# -------------------------------\n",
        "# Encode labels\n",
        "# -------------------------------\n",
        "train_ds = train_ds.map(encode_labels)\n",
        "dev_ds   = dev_ds.map(encode_labels)\n",
        "test_ds  = test_ds.map(encode_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Final dataset structure\n",
        "# -------------------------------\n",
        "dataset = DatasetDict({\n",
        "    \"train\": train_ds,\n",
        "    \"dev\":   dev_ds,\n",
        "    \"test\":  test_ds\n",
        "})\n",
        "\n",
        "dataset\n"
      ],
      "metadata": {
        "id": "xX4OnrKn8r9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_NAME = \"UBC-NLP/MARBERTv2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "tokenized_ds = dataset.map(tokenize, batched=True)\n",
        "\n",
        "# Remove original text column\n",
        "tokenized_ds = tokenized_ds.remove_columns([\"text\"])\n",
        "\n",
        "# Rename label → labels automatically by Trainer\n",
        "tokenized_ds = tokenized_ds.rename_column(\"label\", \"labels\")\n",
        "\n",
        "tokenized_ds.set_format(\"torch\")\n",
        "\n",
        "tokenized_ds\n"
      ],
      "metadata": {
        "id": "AJYKbgM19ZNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    logits, labels = pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    f1 = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")\n",
        "    return {\"macro_f1\": f1[\"f1\"]}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=3,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"/content/marbertv2-gan\",\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=2e-5,\n",
        "    save_total_limit=2,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    logging_strategy=\"epoch\",\n",
        "    report_to=[]\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized_ds[\"train\"],\n",
        "    eval_dataset=tokenized_ds[\"dev\"],\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "n79ntz379eUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== FINAL TEST RESULTS (EESA Test) ===\")\n",
        "print(trainer.evaluate(tokenized_ds[\"test\"]))\n",
        "\n",
        "preds = trainer.predict(tokenized_ds[\"test\"]).predictions\n",
        "pred_labels = np.argmax(preds, axis=1)\n",
        "true_labels = np.array(tokenized_ds[\"test\"][\"labels\"])\n",
        "\n",
        "print(classification_report(\n",
        "    true_labels,\n",
        "    pred_labels,\n",
        "    target_names=[\"neg\", \"neu\", \"pos\"]\n",
        "))\n"
      ],
      "metadata": {
        "id": "yv4TLHl-AZDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# -----------------------------\n",
        "# Load clean external test sets\n",
        "# -----------------------------\n",
        "mr_path  = \"/content/drive/MyDrive/cs-senti/repo/data/cleaned/mr_cs_clean.jsonl\"\n",
        "amg_path = \"/content/drive/MyDrive/cs-senti/repo/data/cleaned/amg_cs_clean.jsonl\"\n",
        "\n",
        "mr_df  = pd.read_json(mr_path, lines=True)\n",
        "amg_df = pd.read_json(amg_path, lines=True)\n",
        "\n",
        "# Ensure consistent columns\n",
        "mr_df  = mr_df[[\"text\", \"label\"]]\n",
        "amg_df = amg_df[[\"text\", \"label\"]]\n",
        "\n",
        "# -----------------------------\n",
        "# Encode labels\n",
        "# -----------------------------\n",
        "def encode_labels(example):\n",
        "    example[\"label\"] = label2id[example[\"label\"]]\n",
        "    return example\n",
        "\n",
        "mr_ds  = Dataset.from_pandas(mr_df).map(encode_labels)\n",
        "amg_ds = Dataset.from_pandas(amg_df).map(encode_labels)\n",
        "\n",
        "# -----------------------------\n",
        "# Tokenize with existing tokenizer\n",
        "# -----------------------------\n",
        "def tokenize_eval(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "mr_tok  = mr_ds.map(tokenize_eval, batched=True)\n",
        "amg_tok = amg_ds.map(tokenize_eval, batched=True)\n",
        "\n",
        "mr_tok  = mr_tok.remove_columns([\"text\"])\n",
        "amg_tok = amg_tok.remove_columns([\"text\"])\n",
        "\n",
        "mr_tok  = mr_tok.rename_column(\"label\", \"labels\")\n",
        "amg_tok = amg_tok.rename_column(\"label\", \"labels\")\n",
        "\n",
        "mr_tok.set_format(\"torch\")\n",
        "amg_tok.set_format(\"torch\")\n",
        "\n",
        "# -----------------------------\n",
        "# Evaluate with trainer\n",
        "# -----------------------------\n",
        "print(\"\\n================= MR GENERALIZATION =================\")\n",
        "mr_results = trainer.evaluate(mr_tok)\n",
        "print(mr_results)\n",
        "\n",
        "mr_preds = np.argmax(trainer.predict(mr_tok).predictions, axis=1)\n",
        "mr_true  = np.array(mr_tok[\"labels\"])\n",
        "\n",
        "print(\"\\nMR Classification Report:\")\n",
        "print(classification_report(mr_true, mr_preds, target_names=[\"neg\",\"neu\",\"pos\"]))\n",
        "\n",
        "# Individual macro F1\n",
        "mr_f1 = f1_score(mr_true, mr_preds, average=\"macro\")\n",
        "print(\"MR Macro-F1:\", mr_f1)\n",
        "\n",
        "\n",
        "print(\"\\n================= AMG GENERALIZATION =================\")\n",
        "amg_results = trainer.evaluate(amg_tok)\n",
        "print(amg_results)\n",
        "\n",
        "amg_preds = np.argmax(trainer.predict(amg_tok).predictions, axis=1)\n",
        "amg_true  = np.array(amg_tok[\"labels\"])\n",
        "\n",
        "print(\"\\nAMG Classification Report:\")\n",
        "print(classification_report(amg_true, amg_preds, target_names=[\"neg\",\"neu\",\"pos\"]))\n",
        "\n",
        "# Individual macro F1\n",
        "amg_f1 = f1_score(amg_true, amg_preds, average=\"macro\")\n",
        "print(\"AMG Macro-F1:\", amg_f1)\n"
      ],
      "metadata": {
        "id": "KCIKzBeXBB6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# A R A B E R T   T R A I N I N G  C E L L\n",
        "# ============================================\n",
        "\n",
        "model_name = \"aubmindlab/bert-base-arabertv2\"\n",
        "\n",
        "print(f\"\\n============== Training {model_name} ==============\\n\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "# Tokenization function — same as MARBERT\n",
        "def tokenize(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "# Convert Pandas → HF Dataset\n",
        "train_ds = Dataset.from_pandas(train_df[[\"text\", \"label\"]]).map(encode_labels)\n",
        "dev_ds   = Dataset.from_pandas(eesa_dev[[\"text\", \"label\"]]).map(encode_labels)\n",
        "test_ds  = Dataset.from_pandas(eesa_test[[\"text\", \"label\"]]).map(encode_labels)\n",
        "\n",
        "# Build DatasetDict\n",
        "dataset = DatasetDict({\n",
        "    \"train\": train_ds,\n",
        "    \"dev\": dev_ds,\n",
        "    \"test\": test_ds\n",
        "})\n",
        "\n",
        "# Apply tokenizer\n",
        "tokenized = dataset.map(tokenize, batched=True)\n",
        "tokenized = tokenized.remove_columns([\"text\"])\n",
        "tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
        "tokenized.set_format(\"torch\")\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=3,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# Metrics\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    logits, labels = pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    f1 = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")\n",
        "    return {\"macro_f1\": f1[\"f1\"]}\n",
        "\n",
        "# Training settings\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/cs-senti/arabert_augmented\",\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=2e-5,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    report_to=[]\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"dev\"],\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# ================================\n",
        "# Final Test Evaluation\n",
        "# ================================\n",
        "print(\"\\n===== FINAL TEST RESULTS (EESA) =====\")\n",
        "print(trainer.evaluate(tokenized[\"test\"]))\n",
        "\n",
        "preds = trainer.predict(tokenized[\"test\"]).predictions\n",
        "pred_labels = np.argmax(preds, axis=1)\n",
        "true_labels = np.array(tokenized[\"test\"][\"labels\"])\n",
        "\n",
        "print(\"\\n===== CLASSIFICATION REPORT =====\")\n",
        "print(classification_report(\n",
        "    true_labels, pred_labels,\n",
        "    target_names=[\"neg\", \"neu\", \"pos\"]\n",
        "))\n"
      ],
      "metadata": {
        "id": "Ec_yFsOQCDF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n================= MR GENERALIZATION =================\")\n",
        "\n",
        "# Convert MR dataframe → HF dataset\n",
        "mr_ds = Dataset.from_pandas(mr_df[[\"text\", \"label\"]]).map(encode_labels)\n",
        "\n",
        "# Tokenize\n",
        "mr_tok = mr_ds.map(lambda b: tokenizer(\n",
        "    b[\"text\"],\n",
        "    truncation=True,\n",
        "    padding=\"max_length\",\n",
        "    max_length=128\n",
        "), batched=True)\n",
        "\n",
        "mr_tok = mr_tok.remove_columns([\"text\"])\n",
        "mr_tok = mr_tok.rename_column(\"label\", \"labels\")\n",
        "mr_tok.set_format(\"torch\")\n",
        "\n",
        "# Evaluate\n",
        "mr_results = trainer.evaluate(mr_tok)\n",
        "print(mr_results)\n",
        "\n",
        "# Classification report\n",
        "preds = trainer.predict(mr_tok).predictions\n",
        "pred_labels = np.argmax(preds, axis=1)\n",
        "true_labels = np.array(mr_tok[\"labels\"])\n",
        "\n",
        "print(\"\\nMR Classification Report:\")\n",
        "print(classification_report(\n",
        "    true_labels, pred_labels,\n",
        "    target_names=[\"neg\", \"neu\", \"pos\"]\n",
        "))\n",
        "\n",
        "\n",
        "print(\"\\n\\n================= AMG GENERALIZATION =================\")\n",
        "\n",
        "# Convert AMG → HF dataset\n",
        "amg_ds = Dataset.from_pandas(amg_df[[\"text\", \"label\"]]).map(encode_labels)\n",
        "\n",
        "# Tokenize\n",
        "amg_tok = amg_ds.map(lambda b: tokenizer(\n",
        "    b[\"text\"],\n",
        "    truncation=True,\n",
        "    padding=\"max_length\",\n",
        "    max_length=128\n",
        "), batched=True)\n",
        "\n",
        "amg_tok = amg_tok.remove_columns([\"text\"])\n",
        "amg_tok = amg_tok.rename_column(\"label\", \"labels\")\n",
        "amg_tok.set_format(\"torch\")\n",
        "\n",
        "# Evaluate\n",
        "amg_results = trainer.evaluate(amg_tok)\n",
        "print(amg_results)\n",
        "\n",
        "# Classification report\n",
        "preds = trainer.predict(amg_tok).predictions\n",
        "pred_labels = np.argmax(preds, axis=1)\n",
        "true_labels = np.array(amg_tok[\"labels\"])\n",
        "\n",
        "print(\"\\nAMG Classification Report:\")\n",
        "print(classification_report(\n",
        "    true_labels, pred_labels,\n",
        "    target_names=[\"neg\", \"neu\", \"pos\"]\n",
        "))\n"
      ],
      "metadata": {
        "id": "xSX045wZFwLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# XLM-R TRAINING (same pipeline you used for MARBERT/AraBERT)\n",
        "# ============================================================\n",
        "\n",
        "model_name = \"xlm-roberta-base\"\n",
        "output_dir = \"/content/drive/MyDrive/cs-senti/models/xlm-r-gan\"\n",
        "\n",
        "print(f\"\\n============== Training {model_name} ==============\\n\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "def tokenize(batch):\n",
        "    # Use \"text\" always (all your datasets were normalized to have this field)\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "tokenized = dataset.map(tokenize, batched=True)\n",
        "\n",
        "# Remove irrelevant columns\n",
        "for col in [\"id\", \"original\", \"generated\"]:\n",
        "    if col in tokenized[\"train\"].column_names:\n",
        "        tokenized = tokenized.remove_columns(col)\n",
        "\n",
        "tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
        "tokenized.set_format(\"torch\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=3,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# ---- Metrics ----\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    logits, labels = pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    return {\"macro_f1\": f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]}\n",
        "\n",
        "# ---- Training Args ----\n",
        "args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=2e-5,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    report_to=[]\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"dev\"],\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# ========== TEST (EESA) ==========\n",
        "print(\"\\n===== FINAL TEST RESULTS (EESA) =====\")\n",
        "print(trainer.evaluate(tokenized[\"test\"]))\n",
        "\n",
        "preds = trainer.predict(tokenized[\"test\"]).predictions\n",
        "pred_labels = np.argmax(preds, axis=1)\n",
        "true_labels = np.array([x[\"labels\"] for x in tokenized[\"test\"]])\n",
        "\n",
        "print(\"\\n===== CLASSIFICATION REPORT =====\")\n",
        "print(classification_report(true_labels, pred_labels, target_names=[\"neg\", \"neu\", \"pos\"]))\n"
      ],
      "metadata": {
        "id": "peq5-uHtJ-eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n================= MR GENERALIZATION =================\")\n",
        "\n",
        "mr_tokenized = mr_ds.map(tokenize, batched=True)\n",
        "mr_tokenized = mr_tokenized.rename_column(\"label\", \"labels\")\n",
        "mr_tokenized.set_format(\"torch\")\n",
        "\n",
        "print(trainer.evaluate(mr_tokenized))\n",
        "\n",
        "preds = trainer.predict(mr_tokenized).predictions\n",
        "pred_labels = np.argmax(preds, axis=1)\n",
        "true_labels = np.array([x[\"labels\"] for x in mr_tokenized])\n",
        "\n",
        "print(\"\\nMR Classification Report:\")\n",
        "print(classification_report(true_labels, pred_labels, target_names=[\"neg\", \"neu\", \"pos\"]))\n"
      ],
      "metadata": {
        "id": "mjlXslKfSreb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n================= AMG GENERALIZATION =================\")\n",
        "\n",
        "amg_tokenized = amg_ds.map(tokenize, batched=True)\n",
        "amg_tokenized = amg_tokenized.rename_column(\"label\", \"labels\")\n",
        "amg_tokenized.set_format(\"torch\")\n",
        "\n",
        "print(trainer.evaluate(amg_tokenized))\n",
        "\n",
        "preds = trainer.predict(amg_tokenized).predictions\n",
        "pred_labels = np.argmax(preds, axis=1)\n",
        "true_labels = np.array([x[\"labels\"] for x in amg_tokenized])\n",
        "\n",
        "print(\"\\nAMG Classification Report:\")\n",
        "print(classification_report(true_labels, pred_labels, target_names=[\"neg\", \"neu\", \"pos\"]))\n"
      ],
      "metadata": {
        "id": "lo16k6P9Ss7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_df.dropna(subset=[\"label\"])\n"
      ],
      "metadata": {
        "id": "9nR_QvKjwEJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import f1_score, classification_report\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 1. LOAD ALL DATASETS\n",
        "# ----------------------------------------------------------\n",
        "base = \"/content/drive/MyDrive/cs-senti\"\n",
        "\n",
        "train_path = f\"{base}/gan_synthetic2/unified_gan_augmented.csv\"\n",
        "eesa_dev_path = f\"{base}/repo/data/cleaned/eesa_dev_clean.jsonl\"\n",
        "eesa_test_path = f\"{base}/repo/data/cleaned/eesa_test_clean.jsonl\"\n",
        "mr_path = f\"{base}/repo/data/cleaned/mr_cs_clean.jsonl\"\n",
        "amg_path = f\"{base}/repo/data/cleaned/amg_cs_clean.jsonl\"\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "eesa_dev = pd.read_json(eesa_dev_path, lines=True)\n",
        "eesa_test = pd.read_json(eesa_test_path, lines=True)\n",
        "mr_df = pd.read_json(mr_path, lines=True)\n",
        "amg_df = pd.read_json(amg_path, lines=True)\n",
        "\n",
        "print(\"Loaded sizes:\")\n",
        "print(len(train_df), len(eesa_dev), len(eesa_test), len(mr_df), len(amg_df))\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 2. CLEAN LABELS (THE IMPORTANT PART)\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "def clean_labels(df):\n",
        "    df = df.copy()\n",
        "    df[\"label\"] = df[\"label\"].astype(str).str.strip()\n",
        "    df = df[df[\"label\"].isin([\"neg\", \"neu\", \"pos\"])]\n",
        "    df[\"label\"] = df[\"label\"].map({\"neg\":0, \"neu\":1, \"pos\":2})\n",
        "    return df\n",
        "\n",
        "train_df = clean_labels(train_df)\n",
        "eesa_dev = clean_labels(eesa_dev)\n",
        "eesa_test = clean_labels(eesa_test)\n",
        "mr_df = clean_labels(mr_df)\n",
        "amg_df = clean_labels(amg_df)\n",
        "\n",
        "print(\"\\nLabel distribution (train):\")\n",
        "print(train_df[\"label\"].value_counts())\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 3. BUILD TRAIN/DEV/TEST MATRICES\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=6000, ngram_range=(1,2))\n",
        "\n",
        "X_train = vectorizer.fit_transform(train_df[\"text\"])\n",
        "y_train = train_df[\"label\"]\n",
        "\n",
        "X_dev = vectorizer.transform(eesa_dev[\"text\"])\n",
        "y_dev = eesa_dev[\"label\"]\n",
        "\n",
        "X_test = vectorizer.transform(eesa_test[\"text\"])\n",
        "y_test = eesa_test[\"label\"]\n",
        "\n",
        "X_mr = vectorizer.transform(mr_df[\"text\"])\n",
        "y_mr = mr_df[\"label\"]\n",
        "\n",
        "X_amg = vectorizer.transform(amg_df[\"text\"])\n",
        "y_amg = amg_df[\"label\"]\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# SAFETY CHECK — ENSURE NO NaN EXISTS\n",
        "# ----------------------------------------------------------\n",
        "print(\"\\nAny NaN in y_train?\", y_train.isna().sum())\n",
        "print(\"Any NaN in X_train?\", pd.isna(X_train.data).sum())\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 4. TRAIN LOGISTIC REGRESSION\n",
        "# ----------------------------------------------------------\n",
        "print(\"\\n================ Logistic Regression ================\")\n",
        "lr = LogisticRegression(max_iter=5000, class_weight=\"balanced\")\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nEESA Test F1:\", f1_score(y_test, lr.predict(X_test), average=\"macro\"))\n",
        "print(\"MR F1:\", f1_score(y_mr, lr.predict(X_mr), average=\"macro\"))\n",
        "print(\"AMG F1:\", f1_score(y_amg, lr.predict(X_amg), average=\"macro\"))\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 5. TRAIN SVM\n",
        "# ----------------------------------------------------------\n",
        "print(\"\\n====================== SVM ======================\")\n",
        "svm = LinearSVC(class_weight=\"balanced\")\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nEESA Test F1:\", f1_score(y_test, svm.predict(X_test), average=\"macro\"))\n",
        "print(\"MR F1:\", f1_score(y_mr, svm.predict(X_mr), average=\"macro\"))\n",
        "print(\"AMG F1:\", f1_score(y_amg, svm.predict(X_amg), average=\"macro\"))\n"
      ],
      "metadata": {
        "id": "du87xYFEx3bY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# 1. Imports\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# ============================================================\n",
        "# 2. Load datasets\n",
        "# ============================================================\n",
        "base = \"/content/drive/MyDrive/cs-senti\"\n",
        "\n",
        "train_path = f\"{base}/gan_synthetic2/unified_gan_augmented.csv\"\n",
        "eesa_dev_path = f\"{base}/repo/data/cleaned/eesa_dev_clean.jsonl\"\n",
        "eesa_test_path = f\"{base}/repo/data/cleaned/eesa_test_clean.jsonl\"\n",
        "mr_path = f\"{base}/repo/data/cleaned/mr_cs_clean.jsonl\"\n",
        "amg_path = f\"{base}/repo/data/cleaned/amg_cs_clean.jsonl\"\n",
        "\n",
        "train_df = pd.read_csv(train_path)\n",
        "dev_df   = pd.read_json(eesa_dev_path, lines=True)\n",
        "test_df  = pd.read_json(eesa_test_path, lines=True)\n",
        "mr_df    = pd.read_json(mr_path, lines=True)\n",
        "amg_df   = pd.read_json(amg_path, lines=True)\n",
        "\n",
        "print(\"Dataset sizes:\")\n",
        "print(len(train_df), len(dev_df), len(test_df), len(mr_df), len(amg_df))\n",
        "\n",
        "# Ensure consistent columns\n",
        "train_df = train_df[[\"text\", \"label\"]]\n",
        "dev_df   = dev_df[[\"text\", \"label\"]]\n",
        "test_df  = test_df[[\"text\", \"label\"]]\n",
        "mr_df    = mr_df[[\"text\", \"label\"]]\n",
        "amg_df   = amg_df[[\"text\", \"label\"]]\n",
        "\n",
        "# ============================================================\n",
        "# Label mapping\n",
        "# ============================================================\n",
        "label_map = {\"neg\": 0, \"neu\": 1, \"pos\": 2}\n",
        "\n",
        "train_df[\"label\"] = train_df[\"label\"].map(label_map)\n",
        "dev_df[\"label\"]   = dev_df[\"label\"].map(label_map)\n",
        "test_df[\"label\"]  = test_df[\"label\"].map(label_map)\n",
        "mr_df[\"label\"]    = mr_df[\"label\"].map(label_map)\n",
        "amg_df[\"label\"]   = amg_df[\"label\"].map(label_map)\n",
        "\n",
        "# ============================================================\n",
        "# 3. TF-IDF Vectorizer (shared across classical models)\n",
        "# ============================================================\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=40000,\n",
        "    ngram_range=(1,2),\n",
        "    min_df=2\n",
        ")\n",
        "\n",
        "X_train = vectorizer.fit_transform(train_df[\"text\"])\n",
        "y_train = train_df[\"label\"]\n",
        "\n",
        "X_dev  = vectorizer.transform(dev_df[\"text\"])\n",
        "y_dev  = dev_df[\"label\"]\n",
        "\n",
        "X_test = vectorizer.transform(test_df[\"text\"])\n",
        "y_test = test_df[\"label\"]\n",
        "\n",
        "X_mr   = vectorizer.transform(mr_df[\"text\"])\n",
        "y_mr   = mr_df[\"label\"]\n",
        "\n",
        "X_amg  = vectorizer.transform(amg_df[\"text\"])\n",
        "y_amg  = amg_df[\"label\"]\n",
        "\n",
        "# ============================================================\n",
        "# 4. Logistic Regression\n",
        "# ============================================================\n",
        "print(\"\\n================ Logistic Regression ================\")\n",
        "lr = LogisticRegression(max_iter=5000, class_weight=\"balanced\")\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "pred_test = lr.predict(X_test)\n",
        "pred_mr   = lr.predict(X_mr)\n",
        "pred_amg  = lr.predict(X_amg)\n",
        "\n",
        "print(\"\\nEESA Test F1:\", f1_score(y_test, pred_test, average=\"macro\"))\n",
        "print(classification_report(y_test, pred_test))\n",
        "\n",
        "print(\"\\nMR Generalization F1:\", f1_score(y_mr, pred_mr, average=\"macro\"))\n",
        "print(classification_report(y_mr, pred_mr))\n",
        "\n",
        "print(\"\\nAMG Generalization F1:\", f1_score(y_amg, pred_amg, average=\"macro\"))\n",
        "print(classification_report(y_amg, pred_amg))\n",
        "\n",
        "# ============================================================\n",
        "# 5. Linear SVM\n",
        "# ============================================================\n",
        "print(\"\\n================ Linear SVM ================\")\n",
        "svm = LinearSVC(class_weight=\"balanced\")\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "pred_test = svm.predict(X_test)\n",
        "pred_mr   = svm.predict(X_mr)\n",
        "pred_amg  = svm.predict(X_amg)\n",
        "\n",
        "print(\"\\nEESA Test F1:\", f1_score(y_test, pred_test, average=\"macro\"))\n",
        "print(classification_report(y_test, pred_test))\n",
        "\n",
        "print(\"\\nMR Generalization F1:\", f1_score(y_mr, pred_mr, average=\"macro\"))\n",
        "print(classification_report(y_mr, pred_mr))\n",
        "\n",
        "print(\"\\nAMG Generalization F1:\", f1_score(y_amg, pred_amg, average=\"macro\"))\n",
        "print(classification_report(y_amg, pred_amg))\n",
        "\n",
        "# ============================================================\n",
        "# 6. BiLSTM Neural Baseline\n",
        "# ============================================================\n",
        "print(\"\\n================ BiLSTM ================\")\n",
        "\n",
        "# ----------------------------\n",
        "# Tokenization\n",
        "# ----------------------------\n",
        "tokenizer = Tokenizer(num_words=50000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_df[\"text\"])\n",
        "\n",
        "def tokenize_series(series, max_len=64):\n",
        "    seq = tokenizer.texts_to_sequences(series)\n",
        "    return pad_sequences(seq, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "X_train_nn = tokenize_series(train_df[\"text\"])\n",
        "X_dev_nn   = tokenize_series(dev_df[\"text\"])\n",
        "X_test_nn  = tokenize_series(test_df[\"text\"])\n",
        "X_mr_nn    = tokenize_series(mr_df[\"text\"])\n",
        "X_amg_nn   = tokenize_series(amg_df[\"text\"])\n",
        "\n",
        "# ----------------------------\n",
        "# Build model\n",
        "# ----------------------------\n",
        "model = tf.keras.Sequential([\n",
        "    Embedding(50000, 128, input_length=64),\n",
        "    Bidirectional(LSTM(64, return_sequences=False)),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation=\"relu\"),\n",
        "    Dense(3, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=\"adam\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# ----------------------------\n",
        "# Train\n",
        "# ----------------------------\n",
        "history = model.fit(\n",
        "    X_train_nn, y_train,\n",
        "    validation_data=(X_dev_nn, y_dev),\n",
        "    epochs=5,\n",
        "    batch_size=64,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ----------------------------\n",
        "# Evaluate BiLSTM\n",
        "# ----------------------------\n",
        "def evaluate_nn(split_name, X, y):\n",
        "    pred = model.predict(X)\n",
        "    pred = np.argmax(pred, axis=1)\n",
        "    print(f\"\\n=== {split_name} F1 ===\")\n",
        "    print(f1_score(y, pred, average='macro'))\n",
        "    print(classification_report(y, pred))\n",
        "\n",
        "evaluate_nn(\"EESA Test\", X_test_nn, y_test)\n",
        "evaluate_nn(\"MR Generalization\", X_mr_nn, y_mr)\n",
        "evaluate_nn(\"AMG Generalization\", X_amg_nn, y_amg)\n",
        "\n"
      ],
      "metadata": {
        "id": "okjTaH5nvVbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLOk52Nmf58R"
      },
      "source": [
        "# **old training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xuKwrF92vATf"
      },
      "outputs": [],
      "source": [
        "#still needs to run\n",
        "print(\"====== TRAINING ON EESA ======\")\n",
        "\n",
        "train_gan(\n",
        "    eesa_train_annotated,\n",
        "    epochs=3,\n",
        "    batch_size=8,\n",
        "    generator=generator,\n",
        "    optimizer=optimizer,\n",
        "    disc_model=disc_model,\n",
        "    disc_tokenizer=disc_tokenizer,\n",
        "    reward_model=reward_model,\n",
        "    reward_tokenizer=reward_tokenizer,\n",
        "    sim_model=similarity_model\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIUUqu8Szam5"
      },
      "outputs": [],
      "source": [
        "print(\"====== TRAINING ON MR-CS ======\")\n",
        "\n",
        "train_gan(\n",
        "    mr_cs_labeled,\n",
        "    epochs=3,\n",
        "    batch_size=8,\n",
        "    generator=generator,\n",
        "    optimizer=optimizer,\n",
        "    disc_model=disc_model,\n",
        "    disc_tokenizer=disc_tokenizer,\n",
        "    reward_model=reward_model,\n",
        "    reward_tokenizer=reward_tokenizer,\n",
        "    sim_model=similarity_model\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCae2Eu7twfi"
      },
      "outputs": [],
      "source": [
        "print(\"====== TRAINING ON AMG-CS ======\")\n",
        "\n",
        "train_gan(\n",
        "    amg_cs_labeled,\n",
        "    epochs=3,\n",
        "    batch_size=8,\n",
        "    generator=generator,\n",
        "    optimizer=optimizer,\n",
        "    disc_model=disc_model,\n",
        "    disc_tokenizer=disc_tokenizer,\n",
        "    reward_model=reward_model,\n",
        "    reward_tokenizer=reward_tokenizer,\n",
        "    sim_model=similarity_model\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMnFQ8s2tzRg"
      },
      "outputs": [],
      "source": [
        "#still needs to run\n",
        "print(\"====== TRAINING ON AMG-mono ======\")\n",
        "\n",
        "train_gan(\n",
        "    amg_ar_subset,\n",
        "    epochs=3,\n",
        "    batch_size=8,\n",
        "    generator=generator,\n",
        "    optimizer=optimizer,\n",
        "    disc_model=disc_model,\n",
        "    disc_tokenizer=disc_tokenizer,\n",
        "    reward_model=reward_model,\n",
        "    reward_tokenizer=reward_tokenizer,\n",
        "    sim_model=similarity_model\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w06nOnoctaDZ"
      },
      "source": [
        "generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pq1hGpZSuIhf"
      },
      "outputs": [],
      "source": [
        "SAVE_DIR = \"/content/drive/MyDrive/cs-senti/gan_synthetic\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IL74bIIY90Wi"
      },
      "outputs": [],
      "source": [
        "def generate_all_samples(dataset, generator, reward_tokenizer, reward_model):\n",
        "    \"\"\"\n",
        "    Generate GAN synthetic data for the entire dataset.\n",
        "    Includes the original sample ID for alignment.\n",
        "    \"\"\"\n",
        "    out = []\n",
        "\n",
        "    for ex in dataset:\n",
        "        gan_text = gan_generate_text(ex, generator, reward_tokenizer, reward_model)\n",
        "        out.append({\n",
        "            \"id\": ex[\"id\"],\n",
        "            \"original\": ex[\"original\"],\n",
        "            \"generated\": gan_text,\n",
        "            \"label\": ex[\"label\"]\n",
        "        })\n",
        "\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UipbsFRHux4V"
      },
      "outputs": [],
      "source": [
        "#still needs to run\n",
        "\n",
        "eesa_gan_samples = generate_all_samples(\n",
        "    eesa_train_annotated,\n",
        "    generator,\n",
        "    reward_tokenizer,\n",
        "    reward_model\n",
        ")\n",
        "\n",
        "with open(f\"{SAVE_DIR}/eesa_gan_samples.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for row in eesa_gan_samples:\n",
        "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"Saved EESA GAN synthetic data.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dr5kfpWHu2hh"
      },
      "outputs": [],
      "source": [
        "\n",
        "mr_gan_samples = generate_all_samples(\n",
        "    mr_cs_labeled,\n",
        "    generator,\n",
        "    reward_tokenizer,\n",
        "    reward_model\n",
        ")\n",
        "\n",
        "with open(f\"{SAVE_DIR}/mr_gan_samples.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for row in mr_gan_samples:\n",
        "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"Saved MR-CS GAN synthetic data.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZppPoC1u36M"
      },
      "outputs": [],
      "source": [
        "amg_gan_samples = generate_all_samples(\n",
        "    amg_cs_labeled,\n",
        "    generator,\n",
        "    reward_tokenizer,\n",
        "    reward_model\n",
        ")\n",
        "\n",
        "with open(f\"{SAVE_DIR}/amg_gan_samples.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for row in amg_gan_samples:\n",
        "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"Saved AMG-CS GAN synthetic data.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hepvqJNpyBDW"
      },
      "outputs": [],
      "source": [
        "#still needs to run\n",
        "amg_ar_gan_samples = generate_all_samples(\n",
        "    amg_ar_subset,\n",
        "    generator,\n",
        "    reward_tokenizer,\n",
        "    reward_model\n",
        ")\n",
        "\n",
        "with open(f\"{SAVE_DIR}/amg_ar_gan_samplesV2.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for row in amg_ar_gan_samples:\n",
        "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"Saved AMG-ar-CS GAN synthetic data.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_jEK4enAsEA"
      },
      "outputs": [],
      "source": [
        "# =======================================\n",
        "#       TRAINING ON AMG-AR MONO\n",
        "# =======================================\n",
        "\n",
        "print(\"====== TRAINING ON AMG-AR ======\")\n",
        "\n",
        "train_gan(\n",
        "    amg_ar_subset,            # ← your 2000-sentence auto-labeled subset\n",
        "    epochs=3,                 # same as EESA\n",
        "    batch_size=8,             # same as EESA\n",
        "    generator=generator,\n",
        "    optimizer=optimizer,\n",
        "    disc_model=disc_model,\n",
        "    disc_tokenizer=disc_tokenizer,\n",
        "    reward_model=reward_model,\n",
        "    reward_tokenizer=reward_tokenizer,\n",
        "    sim_model=similarity_model\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ra8TU68BkMu"
      },
      "outputs": [],
      "source": [
        "# =======================================================\n",
        "#        GAN GENERATION FOR AMG-AR SYNTHETIC DATA\n",
        "# =======================================================\n",
        "\n",
        "print(\"====== GENERATING AMG-AR SYNTHETIC DATA ======\")\n",
        "\n",
        "amg_ar_gan_samples = generate_all_samples(\n",
        "    amg_ar_subset,        # ← your 2000 auto-labeled mono subset\n",
        "    generator,            # trained generator\n",
        "    reward_tokenizer,     # sentiment reward tokenizer\n",
        "    reward_model          # sentiment reward model\n",
        ")\n",
        "\n",
        "# Save output\n",
        "OUTPUT_PATH = f\"{SAVE_DIR}/amg_ar_gan_samplesV3.jsonl\"\n",
        "\n",
        "with open(OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    for row in amg_ar_gan_samples:\n",
        "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"Saved AMG-AR GAN synthetic data to:\")\n",
        "print(OUTPUT_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVFN7AglB2Bs"
      },
      "outputs": [],
      "source": [
        "show_examples(amg_ar_gan_samples,  \"AMG ar Synthetic GAN Data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6csXcXw_vVKC"
      },
      "source": [
        "comparison with deterministic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uQBIby4vUgR"
      },
      "outputs": [],
      "source": [
        "DET_PATH = \"/content/drive/MyDrive/cs-senti/data/ling/host_train_switched_lex_llm.jsonl\"\n",
        "\n",
        "det_rows = []\n",
        "with open(DET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            det_rows.append(json.loads(line))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "print(\"Loaded deterministic:\", len(det_rows))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HoAMTMp1vbQu"
      },
      "outputs": [],
      "source": [
        "def align_domain(det_rows, gan_samples, domain=None, limit=20):\n",
        "    # Build dict original → deterministic\n",
        "    det_map = {row[\"orig_text\"]: row[\"switched_text\"] for row in det_rows if (domain is None or row.get(\"domain\") == domain)}\n",
        "\n",
        "    gan_map = {row[\"original\"]: row for row in gan_samples}\n",
        "\n",
        "    aligned = []\n",
        "    for orig, gan_row in gan_map.items():\n",
        "        det_sw = det_map.get(orig, None)\n",
        "        aligned.append({\n",
        "            \"id\": gan_row[\"id\"],\n",
        "            \"original\": orig,\n",
        "            \"deterministic\": det_sw,\n",
        "            \"gan\": gan_row[\"generated\"],\n",
        "            \"label\": gan_row[\"label\"]\n",
        "        })\n",
        "\n",
        "    # print first N\n",
        "    for row in aligned[:limit]:\n",
        "        print(\"ID:\", row[\"id\"])\n",
        "        print(\"ORIGINAL:\", row[\"original\"])\n",
        "        print(\"DETERMINISTIC:\", row[\"deterministic\"])\n",
        "        print(\"GAN:\", row[\"gan\"])\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "    return aligned\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQzMShuZvdCP"
      },
      "outputs": [],
      "source": [
        "print(\"====== ALIGN EESA ======\")\n",
        "align_domain(det_rows, eesa_gan_samples, domain=\"eesa\", limit=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HipzV_JxveFx"
      },
      "outputs": [],
      "source": [
        "print(\"====== ALIGN MR-CS ======\")\n",
        "align_domain(det_rows, mr_gan_samples, domain=\"mr\", limit=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RASVBS_ovfZw"
      },
      "outputs": [],
      "source": [
        "print(\"====== ALIGN AMG-CS ======\")\n",
        "align_domain(det_rows, amg_gan_samples, domain=\"amg\", limit=20)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uM_O7pyZZjbT"
      },
      "source": [
        "# **showing samples**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzv1JRf86pci"
      },
      "outputs": [],
      "source": [
        "def show_examples(samples, title=\"Examples\"):\n",
        "    \"\"\"\n",
        "    Display ALL generated GAN samples for a domain.\n",
        "    No max_show limit.\n",
        "    \"\"\"\n",
        "    print(f\"\\n========== {title} ==========\\n\")\n",
        "\n",
        "    for ex in samples:\n",
        "        print(f\"ID        : {ex.get('id', 'N/A')}\")\n",
        "        print(f\"ORIGINAL  : {ex['original']}\")\n",
        "        print(f\"GAN       : {ex['generated']}\")\n",
        "        print(f\"LABEL     : {ex['label']}\")\n",
        "        print(\"-\" * 70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZiyRUv-z8JR"
      },
      "outputs": [],
      "source": [
        "show_examples(eesa_gan_samples, \"EESA Synthetic GAN Data\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFar_8Ou0gDQ"
      },
      "outputs": [],
      "source": [
        "show_examples(mr_gan_samples,   \"MR-CS Synthetic GAN Data\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyHjv-SY0hBf"
      },
      "outputs": [],
      "source": [
        "show_examples(amg_gan_samples,  \"AMG Synthetic GAN Data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qn57dD8PzDbp"
      },
      "outputs": [],
      "source": [
        "show_examples(amg_ar_gan_samples,  \"AMG ar Synthetic GAN Data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vm0grA1U2SZY"
      },
      "outputs": [],
      "source": [
        "#version2\n",
        "show_examples(amg_ar_gan_samples,  \"AMG ar Synthetic GAN Data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31uW899j2LTw"
      },
      "source": [
        "trials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frSLAXTK3zX2"
      },
      "outputs": [],
      "source": [
        "def show_examples(samples, title=\"Examples\", max_show=None):\n",
        "    print(f\"\\n====== {title} ======\\n\")\n",
        "    count = 0\n",
        "    for ex in samples:\n",
        "        print(\"ORIGINAL :\", ex[\"original\"])\n",
        "        print(\"GAN      :\", ex[\"generated\"])\n",
        "        print(\"LABEL    :\", ex[\"label\"])\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        count += 1\n",
        "        if max_show and count >= max_show:\n",
        "            break\n",
        "\n",
        "\n",
        "# Example usage\n",
        "show_examples(mr_gan_samples, \"MR-CS GAN\", max_show=50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cChEX5-P7nNg"
      },
      "outputs": [],
      "source": [
        "def save_samples(path, samples):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for row in samples:\n",
        "            f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "    print(\"Saved:\", path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2u33P_A77qvE"
      },
      "outputs": [],
      "source": [
        "save_samples(\"/content/drive/MyDrive/cs-senti/repo/data/gan_outputs/mr_gan_samples.jsonl\", mr_gan_samples)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtDBIPWX7oR7"
      },
      "source": [
        "# **intrinsic evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "guI1aE-G2Q4o"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from statistics import mean\n",
        "\n",
        "def get_sentiment(text, tokenizer, model):\n",
        "    inp = tokenizer(\n",
        "        text, return_tensors=\"pt\",\n",
        "        truncation=True, padding=True\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    pred = model(**inp).logits.softmax(-1).argmax(-1).item()\n",
        "    return [\"neg\", \"neu\", \"pos\"][pred]\n",
        "\n",
        "def evaluate_sentiment(samples, reward_tokenizer, reward_model):\n",
        "    matches = 0\n",
        "    total = 0\n",
        "\n",
        "    for row in samples:\n",
        "        true_label = row[\"label\"]         # dataset label\n",
        "        gen_label  = get_sentiment(row[\"generated\"], reward_tokenizer, reward_model)\n",
        "\n",
        "        if true_label == gen_label:\n",
        "            matches += 1\n",
        "        total += 1\n",
        "\n",
        "    return matches / total if total > 0 else 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fTM8ONiP2U7J"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def cosine(a, b):\n",
        "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-12))\n",
        "\n",
        "def evaluate_similarity(samples, sim_model):\n",
        "    sims = []\n",
        "\n",
        "    for row in samples:\n",
        "        emb = sim_model.encode(\n",
        "            [row[\"original\"], row[\"generated\"]],\n",
        "            convert_to_numpy=True\n",
        "        )\n",
        "        sims.append(cosine(emb[0], emb[1]))\n",
        "\n",
        "    return float(np.mean(sims)) if sims else 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Eir0bSQF2XT8"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def is_english(tok):\n",
        "    return bool(re.fullmatch(r\"[A-Za-z]+\", tok))\n",
        "\n",
        "def evaluate_switch_rate(samples):\n",
        "    switch_ratios = []\n",
        "\n",
        "    for row in samples:\n",
        "        orig_tokens = row[\"original\"].split()\n",
        "        gen_tokens  = row[\"generated\"].split()\n",
        "\n",
        "        if len(orig_tokens) != len(gen_tokens):\n",
        "            continue\n",
        "\n",
        "        total = len(orig_tokens)\n",
        "        switched = sum(\n",
        "            1 for o,g in zip(orig_tokens, gen_tokens)\n",
        "            if is_english(o) != is_english(g)\n",
        "        )\n",
        "\n",
        "        switch_ratios.append(switched / total)\n",
        "\n",
        "    return float(np.mean(switch_ratios)) if switch_ratios else 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ktLqU_Ea2Y77"
      },
      "outputs": [],
      "source": [
        "def evaluate_realness(samples, disc_tokenizer, disc_model):\n",
        "    scores = []\n",
        "\n",
        "    for row in samples:\n",
        "        text = row[\"generated\"]\n",
        "\n",
        "        inputs = disc_tokenizer(\n",
        "            text, return_tensors=\"pt\",\n",
        "            truncation=True, padding=True\n",
        "        ).to(\"cuda\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = disc_model(**inputs).logits.softmax(-1)\n",
        "\n",
        "        prob_real = float(logits[0][1])     # class 1 = \"real\"\n",
        "        scores.append(prob_real)\n",
        "\n",
        "    return float(np.mean(scores)) if scores else 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Z30xeLcZ2e0t"
      },
      "outputs": [],
      "source": [
        "def evaluate_domain(name, samples, reward_tokenizer, reward_model, sim_model, disc_tokenizer, disc_model):\n",
        "    print(f\"\\n\\n========== EVALUATION FOR: {name} ==========\\n\")\n",
        "\n",
        "    S = evaluate_sentiment(samples, reward_tokenizer, reward_model)\n",
        "    M = evaluate_similarity(samples, sim_model)\n",
        "    L = evaluate_switch_rate(samples)\n",
        "    D = evaluate_realness(samples, disc_tokenizer, disc_model)\n",
        "\n",
        "    print(f\"Sentiment Consistency : {S:.4f}\")\n",
        "    print(f\"Meaning Similarity    : {M:.4f}\")\n",
        "    print(f\"Language Switch Rate  : {L:.4f}\")\n",
        "    print(f\"Discriminator Realness: {D:.4f}\")\n",
        "\n",
        "    return {\n",
        "        \"sentiment\": S,\n",
        "        \"similarity\": M,\n",
        "        \"switch_rate\": L,\n",
        "        \"realness\": D\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fg0idstr2gUY"
      },
      "outputs": [],
      "source": [
        "\n",
        "evaluate_domain(\"AMG-CS\", amg_gan_samples,\n",
        "                reward_tokenizer, reward_model,\n",
        "                similarity_model,\n",
        "                disc_tokenizer, disc_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YW02GwBZ2iAU"
      },
      "outputs": [],
      "source": [
        "evaluate_domain(\"EESA\", eesa_gan_samples,\n",
        "                reward_tokenizer, reward_model,\n",
        "                similarity_model,\n",
        "                disc_tokenizer, disc_model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYcH7Y4Bu232"
      },
      "outputs": [],
      "source": [
        "evaluate_domain(\"MR-CS\", mr_gan_samples,\n",
        "                reward_tokenizer, reward_model,\n",
        "                similarity_model,\n",
        "                disc_tokenizer, disc_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZqG_QJczhHs"
      },
      "outputs": [],
      "source": [
        "\n",
        "evaluate_domain(\"AMG-ar-CS\", amg_ar_gan_samples,\n",
        "                reward_tokenizer, reward_model,\n",
        "                similarity_model,\n",
        "                disc_tokenizer, disc_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37k4brjZ3CdC"
      },
      "outputs": [],
      "source": [
        "\n",
        "evaluate_domain(\"AMG-ar-CS\", amg_ar_gan_samples,\n",
        "                reward_tokenizer, reward_model,\n",
        "                similarity_model,\n",
        "                disc_tokenizer, disc_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNkKHhucReOp"
      },
      "source": [
        "# **augmentation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kFMmvo4RgLq"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def load_jsonl(path):\n",
        "    data=[]\n",
        "    with open(path,\"r\",encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "# ORIGINAL CLEANED DATASETS\n",
        "BASE_CLEAN = \"/content/drive/MyDrive/cs-senti/repo/data/cleaned\"\n",
        "\n",
        "eesa_train = load_jsonl(f\"{BASE_CLEAN}/eesa_train_clean.jsonl\")\n",
        "mr_clean   = load_jsonl(f\"{BASE_CLEAN}/mr_cs_clean.jsonl\")\n",
        "amg_clean  = load_jsonl(f\"{BASE_CLEAN}/amg_cs_clean.jsonl\")\n",
        "\n",
        "# EXISTING GAN SYNTHETIC DATA\n",
        "BASE_GAN = \"/content/drive/MyDrive/cs-senti/gan_synthetic\"\n",
        "\n",
        "eesa_gan = load_jsonl(f\"{BASE_GAN}/eesa_gan_samples.jsonl\")\n",
        "mr_gan   = load_jsonl(f\"{BASE_GAN}/mr_gan_samples.jsonl\")\n",
        "amg_gan  = load_jsonl(f\"{BASE_GAN}/amg_gan_samples.jsonl\")\n",
        "\n",
        "print(len(eesa_train), len(eesa_gan))\n",
        "print(len(mr_clean), len(mr_gan))\n",
        "print(len(amg_clean), len(amg_gan))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wE2xx-LbSB09"
      },
      "outputs": [],
      "source": [
        "def convert_gan_to_training(gan_list):\n",
        "    out=[]\n",
        "    for row in gan_list:\n",
        "        out.append({\n",
        "            \"id\": row[\"id\"],\n",
        "            \"text\": row[\"generated\"],\n",
        "            \"label\": row[\"label\"]\n",
        "        })\n",
        "    return out\n",
        "\n",
        "eesa_gan_conv = convert_gan_to_training(eesa_gan)\n",
        "mr_gan_conv   = convert_gan_to_training(mr_gan)\n",
        "amg_gan_conv  = convert_gan_to_training(amg_gan)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIEa-5YJSD_8"
      },
      "outputs": [],
      "source": [
        "aug_eesa = eesa_train + eesa_gan_conv\n",
        "aug_mr   = mr_clean + mr_gan_conv\n",
        "aug_amg  = amg_clean + amg_gan_conv\n",
        "\n",
        "print(\"NEW SIZES:\")\n",
        "print(len(aug_eesa), len(aug_mr), len(aug_amg))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tv6MOx1SahH"
      },
      "outputs": [],
      "source": [
        "SAVE_OUT = \"/content/drive/MyDrive/cs-senti/augmented_datasets\"\n",
        "os.makedirs(SAVE_OUT, exist_ok=True)\n",
        "\n",
        "def save_jsonl(data, path):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for row in data:\n",
        "            f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "save_jsonl(aug_eesa, f\"{SAVE_OUT}/eesa_augmented.jsonl\")\n",
        "save_jsonl(aug_mr,   f\"{SAVE_OUT}/mr_augmented.jsonl\")\n",
        "save_jsonl(aug_amg,  f\"{SAVE_OUT}/amg_augmented.jsonl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhlFkaN0S3eF"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "import json\n",
        "\n",
        "def load_jsonl(path):\n",
        "    data=[]\n",
        "    with open(path,\"r\",encoding=\"utf-8\") as f:\n",
        "        for l in f:\n",
        "            data.append(json.loads(l))\n",
        "    return data\n",
        "\n",
        "AUG_PATH = \"/content/drive/MyDrive/cs-senti/augmented_datasets\"\n",
        "\n",
        "eesa_aug = load_jsonl(f\"{AUG_PATH}/eesa_augmented.jsonl\")\n",
        "mr_aug   = load_jsonl(f\"{AUG_PATH}/mr_augmented.jsonl\")\n",
        "amg_aug  = load_jsonl(f\"{AUG_PATH}/amg_augmented.jsonl\")\n",
        "\n",
        "print(len(eesa_aug), len(mr_aug), len(amg_aug))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgtTzvAxS5Ue"
      },
      "outputs": [],
      "source": [
        "# COMBINE ALL AUGMENTED DATA\n",
        "combined_aug = eesa_aug + mr_aug + amg_aug\n",
        "\n",
        "len(combined_aug)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJ-EjHGlTPj_"
      },
      "outputs": [],
      "source": [
        "bad = [x for x in combined_aug if x[\"label\"] is None]\n",
        "len(bad)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiQxOF92TSU7"
      },
      "outputs": [],
      "source": [
        "combined_aug = [x for x in combined_aug if x[\"label\"] is not None]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rOFJJw9S8P7"
      },
      "outputs": [],
      "source": [
        "label2id = {\"neg\":0, \"neu\":1, \"pos\":2}\n",
        "id2label = {0:\"neg\", 1:\"neu\", 2:\"pos\"}\n",
        "\n",
        "def encode_label(row):\n",
        "    row[\"label\"] = label2id[row[\"label\"]]\n",
        "    return row\n",
        "\n",
        "ds = Dataset.from_list(combined_aug)\n",
        "ds = ds.map(encode_label)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iigwpCqTazX"
      },
      "outputs": [],
      "source": [
        "DEV = \"/content/drive/MyDrive/cs-senti/repo/data/cleaned/eesa_dev_clean.jsonl\"\n",
        "TEST = \"/content/drive/MyDrive/cs-senti/repo/data/cleaned/eesa_test_clean.jsonl\"\n",
        "\n",
        "eesa_dev = load_jsonl(DEV)\n",
        "eesa_test = load_jsonl(TEST)\n",
        "\n",
        "eesa_dev = Dataset.from_list(eesa_dev).map(encode_label)\n",
        "eesa_test = Dataset.from_list(eesa_test).map(encode_label)\n",
        "\n",
        "print(len(eesa_dev), len(eesa_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "if0z64_ZTdcc"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_NAME = \"UBC-NLP/MARBERTv2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "train_ds = ds.map(tokenize, batched=True)\n",
        "dev_ds   = eesa_dev.map(tokenize, batched=True)\n",
        "test_ds  = eesa_test.map(tokenize, batched=True)\n",
        "\n",
        "train_ds = train_ds.remove_columns([\"text\", \"id\"])\n",
        "dev_ds   = dev_ds.remove_columns([\"text\", \"id\"])\n",
        "test_ds  = test_ds.remove_columns([\"text\", \"id\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYE3z6w2Tf7_"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers datasets evaluate accelerate -q\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import Dataset, DatasetDict\n",
        "import json\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from sklearn.metrics import classification_report\n",
        "import os\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Helper: Load JSONL\n",
        "# ============================================================\n",
        "def load_jsonl(path):\n",
        "    return [json.loads(l) for l in open(path, \"r\", encoding=\"utf-8\")]\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# FIX: unify all augmented data to {text, label}\n",
        "# ============================================================\n",
        "clean_train = []\n",
        "for item in combined_aug:\n",
        "    obj = {}\n",
        "\n",
        "    # choose generated text if available, else text\n",
        "    if \"generated\" in item and item[\"generated\"]:\n",
        "        obj[\"text\"] = item[\"generated\"].strip()\n",
        "    else:\n",
        "        obj[\"text\"] = item[\"text\"].strip()\n",
        "\n",
        "    obj[\"label\"] = item[\"label\"]\n",
        "    clean_train.append(obj)\n",
        "\n",
        "label2id = {\"neg\":0, \"neu\":1, \"pos\":2}\n",
        "id2label = {0:\"neg\", 1:\"neu\", 2:\"pos\"}\n",
        "\n",
        "def encode(example):\n",
        "    example[\"label\"] = label2id[example[\"label\"]]\n",
        "    return example\n",
        "\n",
        "train_ds = Dataset.from_list(clean_train).map(encode)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# DEV + TEST (EESA)\n",
        "# ============================================================\n",
        "DEV = \"/content/drive/MyDrive/cs-senti/repo/data/cleaned/eesa_dev_clean.jsonl\"\n",
        "TEST = \"/content/drive/MyDrive/cs-senti/repo/data/cleaned/eesa_test_clean.jsonl\"\n",
        "\n",
        "eesa_dev = Dataset.from_list(load_jsonl(DEV)).map(encode)\n",
        "eesa_test = Dataset.from_list(load_jsonl(TEST)).map(encode)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# DatasetDict\n",
        "# ============================================================\n",
        "dataset = DatasetDict({\n",
        "    \"train\": train_ds,\n",
        "    \"dev\": eesa_dev,\n",
        "    \"test\": eesa_test\n",
        "})\n",
        "\n",
        "print(dataset)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# TRAINING FUNCTION (fixed tokenizer handling)\n",
        "# ============================================================\n",
        "def train_any_model(model_name, output_dir):\n",
        "\n",
        "    print(f\"\\n============== Training {model_name} ==============\\n\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "    def tokenize(batch):\n",
        "        return tokenizer(\n",
        "            batch[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=128\n",
        "        )\n",
        "\n",
        "    tokenized = dataset.map(tokenize, batched=True)\n",
        "    tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
        "    tokenized.set_format(\"torch\")\n",
        "\n",
        "    # ---------------- Model ----------------\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=3,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    )\n",
        "\n",
        "    f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "    def compute_metrics(pred):\n",
        "        logits, labels = pred\n",
        "        preds = np.argmax(logits, axis=-1)\n",
        "        f1 = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")\n",
        "        return {\"macro_f1\": f1[\"f1\"]}\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=4,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        learning_rate=2e-5,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        save_total_limit=2,\n",
        "        logging_strategy=\"epoch\",\n",
        "        report_to=[]\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=tokenized[\"train\"],\n",
        "        eval_dataset=tokenized[\"dev\"],\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"\\n===== FINAL TEST RESULTS =====\")\n",
        "    test_out = trainer.evaluate(tokenized[\"test\"])\n",
        "    print(test_out)\n",
        "\n",
        "    # Full classification report\n",
        "    preds = trainer.predict(tokenized[\"test\"]).predictions\n",
        "    pred_labels = np.argmax(preds, axis=1)\n",
        "    true_labels = np.array(tokenized[\"test\"][\"labels\"])\n",
        "\n",
        "    print(\"\\n===== CLASSIFICATION REPORT =====\")\n",
        "    print(classification_report(\n",
        "        true_labels, pred_labels,\n",
        "        target_names=[\"neg\", \"neu\", \"pos\"]\n",
        "    ))\n",
        "\n",
        "    return trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWB7afnJVpq4"
      },
      "outputs": [],
      "source": [
        "trainer = train_any_model(\n",
        "    model_name=\"UBC-NLP/MARBERT\",\n",
        "    output_dir=\"/content/drive/MyDrive/cs-senti/marbert_gan_aug\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcnBlqsg_fUw"
      },
      "source": [
        "# **version 2 GAN Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JS9y-iZL_jZd"
      },
      "outputs": [],
      "source": [
        "!pip install transformers sentence-transformers -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNnvnhfyL-gH"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/cs-senti/repo/data/annotated_with_id\")\n",
        "\n",
        "def load_jsonl(path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "# Load your GAN-ready annotated datasets\n",
        "eesa_train_annotated = load_jsonl(BASE/\"eesa_train_annotated.jsonl\")\n",
        "eesa_dev_annotated   = load_jsonl(BASE/\"eesa_dev_annotated.jsonl\")\n",
        "eesa_test_annotated  = load_jsonl(BASE/\"eesa_test_annotated.jsonl\")\n",
        "\n",
        "mr_cs_labeled  = load_jsonl(BASE/\"mr_cs_labeled_annotated.jsonl\")\n",
        "amg_cs_labeled = load_jsonl(BASE/\"amg_cs_labeled_annotated.jsonl\")\n",
        "amg_ar_mono    = load_jsonl(BASE/\"amg_ar_mono_annotated.jsonl\")\n",
        "\n",
        "print(\"EESA train:\", len(eesa_train_annotated))\n",
        "print(\"MR-CS:\", len(mr_cs_labeled))\n",
        "print(\"AMG-CS:\", len(amg_cs_labeled))\n",
        "print(\"AMG-mono:\", len(amg_ar_mono))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUWo8yTTMMym"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "LEXICON_PATH = \"/content/drive/MyDrive/cs-senti/repo/data/ar_en_lexicon_MERGED.jsonl\"\n",
        "\n",
        "def load_lexicon(path):\n",
        "    lex = {}\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            item = json.loads(line)\n",
        "\n",
        "            ar = item[\"ar\"]\n",
        "\n",
        "            # Ensure English is always a list\n",
        "            en_list = item[\"en\"]\n",
        "            if isinstance(en_list, str):\n",
        "                en_list = [en_list]\n",
        "\n",
        "            # Ensure synonyms exist and are a list\n",
        "            syns = item.get(\"syn\", [])\n",
        "            if isinstance(syns, str):\n",
        "                syns = [syns]\n",
        "\n",
        "            # Merge and dedupe while preserving order\n",
        "            merged = list(dict.fromkeys(en_list + syns))\n",
        "\n",
        "            lex[ar] = merged\n",
        "\n",
        "    return lex\n",
        "\n",
        "LEXICON = load_lexicon(LEXICON_PATH)\n",
        "print(\"✓ Loaded lexicon entries:\", len(LEXICON))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSiS9kY6Ew3y"
      },
      "outputs": [],
      "source": [
        "DEVICE = \"cuda\"\n",
        "\n",
        "# ============================\n",
        "# (1) XLM-R contextual encoder\n",
        "# ============================\n",
        "xlmr_tok = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "xlmr = AutoModel.from_pretrained(\"xlm-roberta-base\").to(DEVICE)\n",
        "xlmr.eval()\n",
        "\n",
        "# ============================\n",
        "# (2) NEW MARBERT sentiment reward model\n",
        "# ============================\n",
        "reward_path = \"/content/drive/MyDrive/cs-senti/baseline_marbert_v1/checkpoint-1920\"\n",
        "\n",
        "reward_tokenizer = AutoTokenizer.from_pretrained(reward_path)\n",
        "reward_model = AutoModelForSequenceClassification.from_pretrained(reward_path).to(DEVICE)\n",
        "reward_model.eval()\n",
        "\n",
        "print(\"✓ Loaded NEW MARBERT reward model for GAN sentiment reward\")\n",
        "\n",
        "# ============================\n",
        "# (3) ArabicBERT discriminator (REAL vs FAKE)\n",
        "# ============================\n",
        "disc_tok = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "disc_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"aubmindlab/bert-base-arabertv2\",\n",
        "    num_labels=2\n",
        ").to(DEVICE)\n",
        "\n",
        "disc_model.eval()\n",
        "print(\"✓ Loaded Discriminator (ArabicBERT REAL/FAKE classifier)\")\n",
        "\n",
        "# ============================\n",
        "# (4) LaBSE semantic similarity model\n",
        "# ============================\n",
        "from sentence_transformers import SentenceTransformer\n",
        "labse = SentenceTransformer(\"sentence-transformers/LaBSE\").to(DEVICE)\n",
        "\n",
        "print(\"✓ Loaded LaBSE for semantic similarity\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pu3Eb22ZJw0C"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "\n",
        "DEVICE = \"cuda\"\n",
        "\n",
        "# Already loaded in Block 1\n",
        "# xlmr_tok\n",
        "# xlmr\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_xlmr_embeddings(tokens):\n",
        "    \"\"\"\n",
        "    Returns contextual embedding per token (768-dim).\n",
        "    \"\"\"\n",
        "    encoded = xlmr_tok(tokens, return_tensors=\"pt\", is_split_into_words=True, truncation=True).to(DEVICE)\n",
        "\n",
        "    outputs = xlmr(**encoded)\n",
        "    hidden = outputs.last_hidden_state[0]  # (seq_len, 768)\n",
        "\n",
        "    # align subwords → token-level embeddings\n",
        "    token_embs = []\n",
        "    word_ids = encoded.word_ids()\n",
        "    current = []\n",
        "\n",
        "    for hid, wid in zip(hidden, word_ids):\n",
        "        if wid is None:\n",
        "            continue\n",
        "        if len(token_embs) <= wid:\n",
        "            token_embs.append([hid])\n",
        "        else:\n",
        "            token_embs[wid].append(hid)\n",
        "\n",
        "    final_embs = [torch.stack(w).mean(dim=0) for w in token_embs]\n",
        "    return final_embs  # list of 768-dim tensors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtwrcrUEJy7h"
      },
      "outputs": [],
      "source": [
        "# POS tag set you used during annotation:\n",
        "POS_TAGS = sorted(list(set([\n",
        "    \"NOUN\",\"VERB\",\"ADJ\",\"ADV\",\"PRON\",\"DET\",\"ADP\",\"CONJ\",\n",
        "    \"PART\",\"NUM\",\"PROPN\",\"X\",\"PUNCT\",\"SYM\"\n",
        "])))\n",
        "\n",
        "pos2id = {p:i for i,p in enumerate(POS_TAGS)}\n",
        "\n",
        "# trainable projection\n",
        "proj = nn.Linear(768, 300).to(DEVICE)\n",
        "\n",
        "def token_to_vector(tok_emb, pos, lang, maskable, has_al):\n",
        "    pos_vec = torch.zeros(len(POS_TAGS), device=DEVICE)\n",
        "    pos_vec[pos2id.get(pos, 0)] = 1.0\n",
        "\n",
        "    lang_val = 1.0 if lang == \"en\" else 0.0\n",
        "\n",
        "    return torch.cat([\n",
        "        proj(tok_emb),                 # 300 dim\n",
        "        pos_vec,                       # 14 dim\n",
        "        torch.tensor([lang_val], device=DEVICE),\n",
        "        torch.tensor([float(maskable)], device=DEVICE),\n",
        "        torch.tensor([float(has_al)], device=DEVICE)\n",
        "    ])  # total = 317 dim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OYKtr5SgJ19u"
      },
      "outputs": [],
      "source": [
        "class SwitchGenerator(nn.Module):\n",
        "    def __init__(self, feature_size=FEATURE_SIZE, hidden=128):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(feature_size, hidden)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear2 = nn.Linear(hidden, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.linear2(self.relu(self.linear1(x)))  # raw scores\n",
        "        probs = torch.sigmoid(logits)                      # convert to probabilities\n",
        "        return probs.squeeze(-1), logits.squeeze(-1)\n",
        "\n",
        "\n",
        "generator = SwitchGenerator().to(DEVICE)\n",
        "gen_optimizer = torch.optim.Adam(generator.parameters(), lr=1e-4)\n",
        "\n",
        "print(\"✓ Generator updated with FEATURE_SIZE =\", FEATURE_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_34yogN5J4Aq"
      },
      "outputs": [],
      "source": [
        "def reinforce_step(example):\n",
        "    tokens = example[\"tokens\"]\n",
        "    mask_positions = example[\"mask_positions\"]\n",
        "    pos = example[\"pos\"]\n",
        "    lang = example[\"lang\"]\n",
        "    has_al = example[\"has_al\"]\n",
        "    label = example[\"label\"]\n",
        "\n",
        "    if len(mask_positions)==0:\n",
        "        return None\n",
        "\n",
        "    xlmr_embs = extract_xlmr_embeddings(tokens)\n",
        "\n",
        "    # Build feature vectors for mask positions only\n",
        "    vecs = []\n",
        "    for i in mask_positions:\n",
        "        vec = token_to_vector(\n",
        "            xlmr_embs[i],\n",
        "            pos[i],\n",
        "            lang[i],\n",
        "            True,          # mask positions are maskable\n",
        "            has_al[i]\n",
        "        )\n",
        "        vecs.append(vec)\n",
        "    vecs = torch.stack(vecs)\n",
        "\n",
        "    # forward pass\n",
        "    probs, logits = generator(vecs)\n",
        "    dist = torch.distributions.Bernoulli(probs)\n",
        "    actions = dist.sample()            # 1 = switch, 0 = keep\n",
        "    log_probs = dist.log_prob(actions)\n",
        "\n",
        "    # apply switches\n",
        "    new_tokens = tokens.copy()\n",
        "    for act, pos_idx in zip(actions, mask_positions):\n",
        "        if act.item()==1:\n",
        "            # choose a replacement (lexicon)\n",
        "            cands = generate_candidates_for_single(example, pos_idx)\n",
        "            if cands:\n",
        "                new_tokens[pos_idx] = cands[0]  # best candidate\n",
        "    new_text = \" \".join(new_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M49tIE17J6Df"
      },
      "outputs": [],
      "source": [
        "def compute_reward(original, new_text, gold_label):\n",
        "    # Sentiment reward\n",
        "    lab2id = {\"neg\":0,\"neu\":1,\"pos\":2}\n",
        "    sent_in = reward_tokenizer(new_text, return_tensors=\"pt\", truncation=True).to(DEVICE)\n",
        "    sent_probs = reward_model(**sent_in).logits.softmax(-1).detach()\n",
        "    S_reward = sent_probs[0][lab2id[gold_label]].item()\n",
        "\n",
        "    # Discriminator\n",
        "    disc_in = disc_tok(new_text, return_tensors=\"pt\", truncation=True).to(DEVICE)\n",
        "    D_reward = disc_model(**disc_in).logits.softmax(-1)[0][1].item()\n",
        "\n",
        "    # Semantic similarity\n",
        "    emb_a = labse.encode(original, convert_to_tensor=True)\n",
        "    emb_b = labse.encode(new_text, convert_to_tensor=True)\n",
        "    Sim = torch.nn.functional.cosine_similarity(emb_a, emb_b, dim=0).item()\n",
        "\n",
        "    # weights\n",
        "    return 0.5*D_reward + 0.4*S_reward + 0.1*max(Sim,0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTZpIbdEKCJf"
      },
      "outputs": [],
      "source": [
        "def gan_generate(example):\n",
        "    tokens = example[\"tokens\"].copy()\n",
        "    xlmr_embs = extract_xlmr_embeddings(tokens)\n",
        "\n",
        "    for mp in example[\"mask_positions\"]:\n",
        "        vec = token_to_vector(\n",
        "            xlmr_embs[mp],\n",
        "            example[\"pos\"][mp],\n",
        "            example[\"lang\"][mp],\n",
        "            True,\n",
        "            example[\"has_al\"][mp]\n",
        "        ).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            prob, _ = generator(vec)\n",
        "        if prob.item() < 0.5:\n",
        "            continue\n",
        "\n",
        "        cands = generate_candidates_for_single(example, mp)\n",
        "        if cands:\n",
        "            tokens[mp] = cands[0]\n",
        "\n",
        "    return \" \".join(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YK_bTjcLrSa"
      },
      "outputs": [],
      "source": [
        "def generate_candidates_for_single(example, pos_idx):\n",
        "    tok = example[\"tokens\"][pos_idx]\n",
        "    return LEXICON.get(tok, [])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lpi0BmtLubp"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "def train_gan_amg(dataset, epochs=3, batch_size=8):\n",
        "    for ep in range(epochs):\n",
        "        print(f\"\\n===== AMG-CS — EPOCH {ep+1}/{epochs} =====\")\n",
        "        random.shuffle(dataset)\n",
        "\n",
        "        total_loss = []\n",
        "        total_reward = []\n",
        "\n",
        "        for i in tqdm(range(0, len(dataset), batch_size)):\n",
        "            batch = dataset[i:i+batch_size]\n",
        "\n",
        "            for ex in batch:\n",
        "\n",
        "                # --- skip if no maskable positions ---\n",
        "                if len(ex[\"mask_positions\"]) == 0:\n",
        "                    continue\n",
        "\n",
        "                # -----------------------------------------\n",
        "                # Step 1: Extract token embeddings\n",
        "                # -----------------------------------------\n",
        "                xlmr_embs = extract_xlmr_embeddings(ex[\"tokens\"])\n",
        "\n",
        "                # -----------------------------------------\n",
        "                # Step 2: Build feature vectors for mask positions\n",
        "                # -----------------------------------------\n",
        "                vecs = []\n",
        "                for mp in ex[\"mask_positions\"]:\n",
        "                    vec = token_to_vector(\n",
        "                        xlmr_embs[mp],\n",
        "                        ex[\"pos\"][mp],\n",
        "                        ex[\"lang\"][mp],\n",
        "                        True,\n",
        "                        ex[\"has_al\"][mp]\n",
        "                    )\n",
        "                    vecs.append(vec)\n",
        "\n",
        "                vecs = torch.stack(vecs)  # (num_masked, 317)\n",
        "\n",
        "                # -----------------------------------------\n",
        "                # Step 3: Generator forward → probs & log_probs\n",
        "                # -----------------------------------------\n",
        "                probs, logits = generator(vecs)\n",
        "                dist = torch.distributions.Bernoulli(probs)\n",
        "                actions = dist.sample()\n",
        "                log_probs = dist.log_prob(actions)\n",
        "\n",
        "                # -----------------------------------------\n",
        "                # Step 4: Build new sentence according to actions\n",
        "                # -----------------------------------------\n",
        "                new_tokens = ex[\"tokens\"].copy()\n",
        "                for act, pos_idx in zip(actions, ex[\"mask_positions\"]):\n",
        "                    if act.item() == 1:\n",
        "                        cands = generate_candidates_for_single(ex, pos_idx)\n",
        "                        if cands:\n",
        "                            new_tokens[pos_idx] = cands[0]\n",
        "\n",
        "                new_text = \" \".join(new_tokens)\n",
        "\n",
        "                # -----------------------------------------\n",
        "                # Step 5: Compute reward\n",
        "                # -----------------------------------------\n",
        "                reward = compute_reward(\n",
        "                    original=ex[\"original\"],\n",
        "                    new_text=new_text,\n",
        "                    gold_label=ex[\"label\"]\n",
        "                )\n",
        "                total_reward.append(reward)\n",
        "\n",
        "                # -----------------------------------------\n",
        "                # Step 6: Policy gradient update\n",
        "                # -----------------------------------------\n",
        "                loss = -(log_probs * reward).mean()\n",
        "\n",
        "                gen_optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                gen_optimizer.step()\n",
        "\n",
        "                total_loss.append(loss.item())\n",
        "\n",
        "        # ----- end of epoch -----\n",
        "        avg_loss = sum(total_loss) / len(total_loss) if total_loss else 0\n",
        "        avg_reward = sum(total_reward) / len(total_reward) if total_reward else 0\n",
        "\n",
        "        print(f\"Epoch {ep+1} — Avg Loss = {avg_loss:.4f}, Avg Reward = {avg_reward:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKi4GncnLxbk"
      },
      "outputs": [],
      "source": [
        "print(\"🚀 Training GAN Switch Predictor on AMG-CS…\")\n",
        "train_gan_amg(amg_cs_labeled, epochs=3, batch_size=8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpS1zU5rNnOn"
      },
      "outputs": [],
      "source": [
        "gan_amg_outputs = []\n",
        "\n",
        "for ex in amg_cs_labeled:\n",
        "    out = gan_generate(ex)\n",
        "    gan_amg_outputs.append({\n",
        "        \"id\": ex[\"id\"],\n",
        "        \"original\": ex[\"original\"],\n",
        "        \"gan\": out,\n",
        "        \"label\": ex[\"label\"]\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sN2Gy_iNNr4S"
      },
      "outputs": [],
      "source": [
        "SAVE_DIR = \"/content/drive/MyDrive/cs-senti/gan_synthetic\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "with open(f\"{SAVE_DIR}/amg_gan_samplesV3.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for row in gan_amg_outputs:\n",
        "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"✓ Saved AMG GAN synthetic samples.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "482v5CFpOOrF"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def show_all_gan_samples(path):\n",
        "    print(\"📌 Displaying ALL GAN-generated samples:\\n\")\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            row = json.loads(line)\n",
        "\n",
        "            print(\"=\"*100)\n",
        "            print(f\"🆔 ID: {row['id']}   |   🏷️ Label: {row['label']}\")\n",
        "            print(\"\\n🔹 Original:\")\n",
        "            print(row[\"original\"])\n",
        "\n",
        "            print(\"\\n🔹 GAN Generated:\")\n",
        "            print(row[\"gan\"])\n",
        "            print(\"\\n\")\n",
        "    print(\"=\"*100)\n",
        "    print(\"✓ Finished displaying all samples.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIuOjSW0OQrO"
      },
      "outputs": [],
      "source": [
        "show_all_gan_samples(\n",
        "    \"/content/drive/MyDrive/cs-senti/gan_synthetic/amg_gan_samplesV3.jsonl\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_Btmh8jGL3F"
      },
      "source": [
        "# **extrinsic evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el6xq2JhSfiO"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate accelerate -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mw8OrHDOShnJ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import Dataset, DatasetDict\n",
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from sklearn.metrics import classification_report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8lPzrExSrVU"
      },
      "outputs": [],
      "source": [
        "def load_jsonl(path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9At5JiNDSu3e"
      },
      "outputs": [],
      "source": [
        "BASE = Path(\"/content/drive/MyDrive/cs-senti/repo/data/cleaned\")\n",
        "\n",
        "eesa_train = load_jsonl(BASE/\"eesa_train_clean.jsonl\")\n",
        "eesa_dev   = load_jsonl(BASE/\"eesa_dev_clean.jsonl\")\n",
        "eesa_test  = load_jsonl(BASE/\"eesa_test_clean.jsonl\")\n",
        "\n",
        "mr_cs      = load_jsonl(BASE/\"mr_cs_clean.jsonl\")\n",
        "amg_cs     = load_jsonl(BASE/\"amg_cs_clean.jsonl\")\n",
        "\n",
        "print(len(eesa_train), len(eesa_dev), len(eesa_test))\n",
        "print(len(mr_cs), len(amg_cs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOtGPzEwSyvO"
      },
      "outputs": [],
      "source": [
        "train_data = eesa_train + mr_cs + amg_cs\n",
        "dev_data   = eesa_dev\n",
        "test_data  = eesa_test\n",
        "\n",
        "print(\"Final train size:\", len(train_data))\n",
        "print(\"Final dev size:\", len(dev_data))\n",
        "print(\"Final test size:\", len(test_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDgEzVhuSz25"
      },
      "outputs": [],
      "source": [
        "label2id = {\"neg\": 0, \"neu\": 1, \"pos\": 2}\n",
        "id2label = {v: k for k, v in label2id.items()}\n",
        "\n",
        "def encode_label(example):\n",
        "    example[\"label\"] = label2id[example[\"label\"]]\n",
        "    return example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV-NZwvgS30m"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset.from_list(train_data).map(encode_label)\n",
        "dev_dataset   = Dataset.from_list(dev_data).map(encode_label)\n",
        "test_dataset  = Dataset.from_list(test_data).map(encode_label)\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"dev\": dev_dataset,\n",
        "    \"test\": test_dataset\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1FgLzZqTEak"
      },
      "outputs": [],
      "source": [
        "MODEL = \"xlm-roberta-base\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFdmN4loS6MU"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "tokenized = dataset.map(tokenize_function, batched=True)\n",
        "tokenized = tokenized.remove_columns([\"text\", \"id\"])\n",
        "tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
        "tokenized.set_format(\"torch\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utkw6SbuTIV6"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL,\n",
        "    num_labels=3,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROP9v1PCTL3Z"
      },
      "outputs": [],
      "source": [
        "metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    f1 = metric.compute(predictions=preds, references=labels, average=\"macro\")\n",
        "    return {\"macro_f1\": f1[\"f1\"]}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeVn0AI6TOBJ"
      },
      "outputs": [],
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/cs-senti/baseline_xlmr_v0\",\n",
        "    num_train_epochs=4,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_steps=50,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "\n",
        "    # 🔥 Disable ALL logging integrations\n",
        "    report_to=[],         # <-- THIS TURNS OFF wandb COMPLETELY\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFbiV7wuTqTR"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"dev\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSCq0jMvT8xz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_API_KEY\"] = \"dummy\"\n",
        "os.environ[\"WANDB_MODE\"] = \"offline\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kL38sKOUBJm"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71PsNYeAXBww"
      },
      "outputs": [],
      "source": [
        "results = trainer.evaluate(tokenized[\"test\"])\n",
        "results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3B2o14tdXFly"
      },
      "outputs": [],
      "source": [
        "preds = trainer.predict(tokenized[\"test\"]).predictions\n",
        "pred_labels = np.argmax(preds, axis=1)\n",
        "\n",
        "true_labels = np.array([x[\"labels\"] for x in tokenized[\"test\"]])\n",
        "\n",
        "print(classification_report(true_labels, pred_labels, target_names=[\"neg\", \"neu\", \"pos\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xKUQk41XMxL"
      },
      "outputs": [],
      "source": [
        "mr_dataset = Dataset.from_list(mr_cs).map(encode_label)\n",
        "amg_dataset = Dataset.from_list(amg_cs).map(encode_label)\n",
        "\n",
        "mr_tokenized = mr_dataset.map(tokenize_function, batched=True)\n",
        "mr_tokenized = mr_tokenized.remove_columns([\"text\", \"id\"])\n",
        "mr_tokenized.set_format(\"torch\")\n",
        "\n",
        "amg_tokenized = amg_dataset.map(tokenize_function, batched=True)\n",
        "amg_tokenized = amg_tokenized.remove_columns([\"text\", \"id\"])\n",
        "amg_tokenized.set_format(\"torch\")\n",
        "\n",
        "print(\"MR-CS:\", trainer.evaluate(mr_tokenized))\n",
        "print(\"AMG-CS:\", trainer.evaluate(amg_tokenized))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI3G89oAlxDq"
      },
      "source": [
        "# **REAL FINAL transformer based sentiment classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbSrNJmhl4WW"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers datasets evaluate accelerate -q\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "from datasets import Dataset, DatasetDict\n",
        "import json\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from pathlib import Path\n",
        "from sklearn.metrics import classification_report\n",
        "import os\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# 1. Load Datasets\n",
        "# ======================================================================\n",
        "def load_jsonl(path):\n",
        "    return [json.loads(l) for l in open(path, \"r\", encoding=\"utf-8\")]\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/cs-senti/repo/data/cleaned\")\n",
        "\n",
        "eesa_train = load_jsonl(BASE/\"eesa_train_clean.jsonl\")\n",
        "eesa_dev   = load_jsonl(BASE/\"eesa_dev_clean.jsonl\")\n",
        "eesa_test  = load_jsonl(BASE/\"eesa_test_clean.jsonl\")\n",
        "\n",
        "mr_cs  = load_jsonl(BASE/\"mr_cs_clean.jsonl\")\n",
        "amg_cs = load_jsonl(BASE/\"amg_cs_clean.jsonl\")\n",
        "\n",
        "train_data = eesa_train + mr_cs + amg_cs\n",
        "dev_data   = eesa_dev\n",
        "test_data  = eesa_test\n",
        "\n",
        "label2id = {\"neg\": 0, \"neu\": 1, \"pos\": 2}\n",
        "id2label = {v: k for k, v in label2id.items()}\n",
        "\n",
        "def encode_label(example):\n",
        "    example[\"label\"] = label2id[example[\"label\"]]\n",
        "    return example\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_list(train_data).map(encode_label),\n",
        "    \"dev\":   Dataset.from_list(dev_data).map(encode_label),\n",
        "    \"test\":  Dataset.from_list(test_data).map(encode_label),\n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "# ======================================================================\n",
        "# 2. Main training function\n",
        "# ======================================================================\n",
        "def train_any_model(model_name, output_dir):\n",
        "\n",
        "    print(f\"\\n============== Training {model_name} ==============\\n\")\n",
        "\n",
        "    # ---------------- Tokenizer ----------------\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "    def tokenize(batch):\n",
        "        return tokenizer(\n",
        "            batch[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=128\n",
        "        )\n",
        "\n",
        "    tokenized = dataset.map(tokenize, batched=True)\n",
        "\n",
        "    # remove missing columns\n",
        "    for col in [\"text\", \"id\"]:\n",
        "        if col in tokenized[\"train\"].column_names:\n",
        "            tokenized = tokenized.remove_columns(col)\n",
        "\n",
        "    tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
        "    tokenized.set_format(\"torch\")\n",
        "\n",
        "    # ---------------- Model ----------------\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=3,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    )\n",
        "\n",
        "    # ---------------- Metrics ----------------\n",
        "    f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "    def compute_metrics(pred):\n",
        "        logits, labels = pred\n",
        "        preds = np.argmax(logits, axis=-1)\n",
        "        f1 = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")\n",
        "        return {\"macro_f1\": f1[\"f1\"]}\n",
        "\n",
        "    # ---------------- Arguments ----------------\n",
        "    args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=4,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        learning_rate=2e-5,\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True,\n",
        "        # Corrected argument names\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_strategy=\"epoch\",\n",
        "        report_to=[]\n",
        "    )\n",
        "\n",
        "    # ---------------- Trainer ----------------\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=tokenized[\"train\"],\n",
        "        eval_dataset=tokenized[\"dev\"],\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # final test set eval\n",
        "    print(trainer.evaluate(tokenized[\"test\"]))\n",
        "\n",
        "    preds = trainer.predict(tokenized[\"test\"]).predictions\n",
        "    pred_labels = np.argmax(preds, axis=1)\n",
        "    true_labels = np.array([x[\"labels\"] for x in tokenized[\"test\"]])\n",
        "\n",
        "    print(classification_report(\n",
        "        true_labels, pred_labels,\n",
        "        target_names=[\"neg\", \"neu\", \"pos\"]\n",
        "    ))\n",
        "\n",
        "    return trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPM0zAHrnk_T"
      },
      "outputs": [],
      "source": [
        "trainer_xlmr = train_any_model(\n",
        "    \"xlm-roberta-base\",\n",
        "    \"/content/drive/MyDrive/cs-senti/baseline_xlmr_fixed\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxY1Pt-x36Bp"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\", use_fast=True)\n",
        "def tokenize(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "mr_dataset = Dataset.from_list(mr_cs).map(encode_label)\n",
        "amg_dataset = Dataset.from_list(amg_cs).map(encode_label)\n",
        "\n",
        "mr_tokenized = mr_dataset.map(tokenize, batched=True)\n",
        "amg_tokenized = amg_dataset.map(tokenize, batched=True)\n",
        "\n",
        "# Remove unused columns if they exist\n",
        "for col in [\"text\", \"id\"]:\n",
        "    if col in mr_tokenized.column_names:\n",
        "        mr_tokenized = mr_tokenized.remove_columns(col)\n",
        "    if col in amg_tokenized.column_names:\n",
        "        amg_tokenized = amg_tokenized.remove_columns(col)\n",
        "\n",
        "mr_tokenized.set_format(\"torch\")\n",
        "amg_tokenized.set_format(\"torch\")\n",
        "print(\"MR-CS:\", trainer_xlmr.evaluate(mr_tokenized))\n",
        "print(\"AMG-CS:\", trainer_xlmr.evaluate(amg_tokenized))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fJC7kXymcBI"
      },
      "outputs": [],
      "source": [
        "trainer_arabert = train_any_model(\n",
        "    \"aubmindlab/bert-base-arabertv2\",\n",
        "    \"/content/drive/MyDrive/cs-senti/baseline_arabert_v1\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AE_d30Z3PkI"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\", use_fast=True)\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "mr_tokenized = mr_dataset.map(tokenize, batched=True)\n",
        "amg_tokenized = amg_dataset.map(tokenize, batched=True)\n",
        "print(\"MR-CS:\", trainer_arabert.evaluate(mr_tokenized))\n",
        "print(\"AMG-CS:\", trainer_arabert.evaluate(amg_tokenized))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMa9gmuPmkD0"
      },
      "outputs": [],
      "source": [
        "trainer_marbert = train_any_model(\n",
        "    \"UBC-NLP/MARBERTv2\",\n",
        "    \"/content/drive/MyDrive/cs-senti/baseline_marbert_v1\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGW3ZBPU3naV"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"UBC-NLP/MARBERTv2\", use_fast=True)\n",
        "def tokenize(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "mr_dataset = Dataset.from_list(mr_cs).map(encode_label)\n",
        "amg_dataset = Dataset.from_list(amg_cs).map(encode_label)\n",
        "\n",
        "mr_tokenized = mr_dataset.map(tokenize, batched=True)\n",
        "amg_tokenized = amg_dataset.map(tokenize, batched=True)\n",
        "\n",
        "# Remove unused columns if they exist\n",
        "for col in [\"text\", \"id\"]:\n",
        "    if col in mr_tokenized.column_names:\n",
        "        mr_tokenized = mr_tokenized.remove_columns(col)\n",
        "    if col in amg_tokenized.column_names:\n",
        "        amg_tokenized = amg_tokenized.remove_columns(col)\n",
        "\n",
        "mr_tokenized.set_format(\"torch\")\n",
        "amg_tokenized.set_format(\"torch\")\n",
        "print(\"MR-CS:\", trainer_marbert.evaluate(mr_tokenized))\n",
        "print(\"AMG-CS:\", trainer_marbert.evaluate(amg_tokenized))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl5v-F8nkSWF"
      },
      "source": [
        "# **Classical Baselines (TF-IDF + SVM + Logistic Regression)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjO1WXPtiM5b"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/cs-senti/repo/data/cleaned\"\n",
        "\n",
        "def load_jsonl(path):\n",
        "    data = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            data.append(json.loads(line))\n",
        "    return data\n",
        "\n",
        "eesa_train = load_jsonl(f\"{BASE}/eesa_train_clean.jsonl\")\n",
        "eesa_dev   = load_jsonl(f\"{BASE}/eesa_dev_clean.jsonl\")\n",
        "eesa_test  = load_jsonl(f\"{BASE}/eesa_test_clean.jsonl\")\n",
        "\n",
        "mr_cs      = load_jsonl(f\"{BASE}/mr_cs_clean.jsonl\")\n",
        "amg_cs     = load_jsonl(f\"{BASE}/amg_cs_clean.jsonl\")\n",
        "\n",
        "print(len(eesa_train), len(eesa_dev), len(eesa_test))\n",
        "print(len(mr_cs), len(amg_cs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_unXzR9kgAR"
      },
      "outputs": [],
      "source": [
        "train_data = eesa_train + mr_cs + amg_cs\n",
        "dev_data   = eesa_dev\n",
        "test_data  = eesa_test\n",
        "\n",
        "print(\"TRAIN:\", len(train_data))\n",
        "print(\"DEV:\", len(dev_data))\n",
        "print(\"TEST:\", len(test_data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D_ZRDTsXkpgR"
      },
      "outputs": [],
      "source": [
        "def prepare_xy(data):\n",
        "    X = [d[\"text\"] for d in data]\n",
        "    y = [d[\"label\"] for d in data]\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = prepare_xy(train_data)\n",
        "X_dev,   y_dev   = prepare_xy(dev_data)\n",
        "X_test,  y_test  = prepare_xy(test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jf-7nlYkvb6"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_train_enc = le.fit_transform(y_train)\n",
        "y_dev_enc   = le.transform(y_dev)\n",
        "y_test_enc  = le.transform(y_test)\n",
        "\n",
        "le.classes_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqIcdc8vkxPf"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=30000,\n",
        "    ngram_range=(1,2),\n",
        "    min_df=2\n",
        ")\n",
        "\n",
        "X_train_vec = tfidf.fit_transform(X_train)\n",
        "X_dev_vec   = tfidf.transform(X_dev)\n",
        "X_test_vec  = tfidf.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHQvyq1BkzGA"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(\n",
        "    max_iter=2000,\n",
        "    C=3.0,\n",
        "    class_weight=\"balanced\",\n",
        ")\n",
        "\n",
        "lr.fit(X_train_vec, y_train_enc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Xsjfo6Tk1gF"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svm = LinearSVC(\n",
        "    C=1.0,\n",
        "    class_weight=\"balanced\"\n",
        ")\n",
        "\n",
        "svm.fit(X_train_vec, y_train_enc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hi3P55XTk4CZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "def evaluate(model, X, y_true, title=\"\"):\n",
        "    pred = model.predict(X)\n",
        "    print(f\"\\n===== {title} =====\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_true, pred))\n",
        "    print(\"Macro F1:\", f1_score(y_true, pred, average=\"macro\"))\n",
        "    print(classification_report(y_true, pred, target_names=le.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pqtuVQLk6DZ"
      },
      "outputs": [],
      "source": [
        "X_mr, y_mr = prepare_xy(mr_cs)\n",
        "y_mr_enc   = le.transform(y_mr)\n",
        "\n",
        "X_amg, y_amg = prepare_xy(amg_cs)\n",
        "y_amg_enc    = le.transform(y_amg)\n",
        "\n",
        "X_mr_vec  = tfidf.transform(X_mr)\n",
        "X_amg_vec = tfidf.transform(X_amg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VAJKvBTk7pW"
      },
      "outputs": [],
      "source": [
        "evaluate(lr, X_test_vec, y_test_enc, \"LR — EESA Test\")\n",
        "evaluate(lr, X_mr_vec,  y_mr_enc,   \"LR — MR-CS Test\")\n",
        "evaluate(lr, X_amg_vec, y_amg_enc,  \"LR — AMG-CS Test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0N8sG8xlAKf"
      },
      "outputs": [],
      "source": [
        "evaluate(svm, X_test_vec, y_test_enc, \"SVM — EESA Test\")\n",
        "evaluate(svm, X_mr_vec,  y_mr_enc,   \"SVM — MR-CS Test\")\n",
        "evaluate(svm, X_amg_vec, y_amg_enc,  \"SVM — AMG-CS Test\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhvpDyoXmOsg"
      },
      "source": [
        "## **NEURAL BASELINES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgsV2jb8lKtn"
      },
      "outputs": [],
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.ar.300.bin.gz\n",
        "!gunzip cc.ar.300.bin.gz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwhXZqaimhCp"
      },
      "outputs": [],
      "source": [
        "!pip install fasttext\n",
        "import fasttext\n",
        "ft = fasttext.load_model(\"cc.ar.300.bin\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDwgRaLTn1fJ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/cs-senti/repo/data/cleaned\"\n",
        "\n",
        "def load_jsonl(path):\n",
        "    return [json.loads(l) for l in open(path, \"r\", encoding=\"utf-8\")]\n",
        "\n",
        "eesa_train = load_jsonl(f\"{BASE}/eesa_train_clean.jsonl\")\n",
        "eesa_dev   = load_jsonl(f\"{BASE}/eesa_dev_clean.jsonl\")\n",
        "eesa_test  = load_jsonl(f\"{BASE}/eesa_test_clean.jsonl\")\n",
        "mr_cs      = load_jsonl(f\"{BASE}/mr_cs_clean.jsonl\")\n",
        "amg_cs     = load_jsonl(f\"{BASE}/amg_cs_clean.jsonl\")\n",
        "\n",
        "train_data = eesa_train + mr_cs + amg_cs\n",
        "dev_data   = eesa_dev\n",
        "test_data  = eesa_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WQOClyw-tSS"
      },
      "outputs": [],
      "source": [
        "def fix_labels(ds):\n",
        "    fixed = []\n",
        "    for row in ds:\n",
        "        lbl = row[\"label\"]\n",
        "\n",
        "        # if label is list like [\"neg\"]\n",
        "        if isinstance(lbl, list):\n",
        "            lbl = lbl[0]\n",
        "\n",
        "        row[\"label\"] = lbl\n",
        "        fixed.append(row)\n",
        "    return fixed\n",
        "\n",
        "final_train = fix_labels(final_train)\n",
        "final_dev   = fix_labels(final_dev)\n",
        "final_test_eesa = fix_labels(final_test_eesa)\n",
        "final_test_amg  = fix_labels(final_test_amg)\n",
        "final_test_mr   = fix_labels(final_test_mr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVe-O7qj-40-"
      },
      "outputs": [],
      "source": [
        "valid = {\"neg\",\"neu\",\"pos\"}\n",
        "\n",
        "for row in final_train:\n",
        "    if row[\"label\"] not in valid:\n",
        "        print(\"Unexpected:\", row)\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4r7f2rRoISG"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "texts = [d[\"text\"] for d in train_data]\n",
        "tokenizer = Tokenizer(num_words=50000, lower=True)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "max_len = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "426pOJX-oRcX"
      },
      "outputs": [],
      "source": [
        "def to_seq(data):\n",
        "    X = tokenizer.texts_to_sequences([d[\"text\"] for d in data])\n",
        "    X = pad_sequences(X, maxlen=max_len, padding=\"post\")\n",
        "    y = [d[\"label\"] for d in data]\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = to_seq(train_data)\n",
        "X_dev, y_dev = to_seq(dev_data)\n",
        "X_test, y_test = to_seq(test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTDfCdYSov26"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y_train = le.fit_transform(y_train)\n",
        "y_dev   = le.transform(y_dev)\n",
        "y_test  = le.transform(y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wA4g2so6oyDf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "vocab_size = min(50000, len(tokenizer.word_index) + 1)\n",
        "embedding_dim = 300\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i >= vocab_size:\n",
        "        continue\n",
        "    embedding_matrix[i] = ft.get_word_vector(word)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evLxY-ZTpmEA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "model_bilstm = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=False),\n",
        "    Bidirectional(LSTM(128, return_sequences=False)),\n",
        "    Dropout(0.3),\n",
        "    Dense(3, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model_bilstm.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                     optimizer=\"adam\",\n",
        "                     metrics=[\"accuracy\"])\n",
        "model_bilstm.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9piHpX6qpqza"
      },
      "outputs": [],
      "source": [
        "history_bilstm = model_bilstm.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_dev, y_dev),\n",
        "    batch_size=64,\n",
        "    epochs=8\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jfF2aacp3cd"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
        "\n",
        "model_cnn_lstm = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=False),\n",
        "    Conv1D(128, 5, activation=\"relu\"),\n",
        "    MaxPooling1D(2),\n",
        "    LSTM(128),\n",
        "    Dropout(0.3),\n",
        "    Dense(3, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model_cnn_lstm.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                       optimizer=\"adam\",\n",
        "                       metrics=[\"accuracy\"])\n",
        "model_cnn_lstm.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oobn8NMEp6kQ"
      },
      "outputs": [],
      "source": [
        "history_cnn_lstm = model_cnn_lstm.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_dev, y_dev),\n",
        "    batch_size=64,\n",
        "    epochs=8\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aO1HL4xup_XF"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class Attention(Layer):\n",
        "    def call(self, inputs):\n",
        "        score = tf.nn.softmax(tf.matmul(inputs, inputs, transpose_b=True), axis=-1)\n",
        "        context = tf.matmul(score, inputs)\n",
        "        return tf.reduce_mean(context, axis=1)\n",
        "\n",
        "model_attn = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], trainable=False),\n",
        "    Bidirectional(LSTM(128, return_sequences=True)),\n",
        "    Attention(),\n",
        "    Dropout(0.3),\n",
        "    Dense(3, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model_attn.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                   optimizer=\"adam\",\n",
        "                   metrics=[\"accuracy\"])\n",
        "model_attn.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVbhKJQwqBWs"
      },
      "outputs": [],
      "source": [
        "history_attn = model_attn.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_dev, y_dev),\n",
        "    batch_size=64,\n",
        "    epochs=8\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0Eue9apqHnS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "def eval_model(model, X, y, title):\n",
        "    pred = np.argmax(model.predict(X), axis=1)\n",
        "    print(f\"\\n===== {title} =====\")\n",
        "    print(\"Macro F1:\", f1_score(y, pred, average=\"macro\"))\n",
        "    print(classification_report(y, pred, target_names=le.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PmFAMYIqJMk"
      },
      "outputs": [],
      "source": [
        "eval_model(model_bilstm,    X_test, y_test, \"BiLSTM — EESA Test\")\n",
        "eval_model(model_cnn_lstm, X_test, y_test, \"CNN-LSTM — EESA Test\")\n",
        "eval_model(model_attn,     X_test, y_test, \"BiLSTM-Attn — EESA Test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EeBf3T6qOKR"
      },
      "outputs": [],
      "source": [
        "X_mr, y_mr = to_seq(mr_cs)\n",
        "y_mr = le.transform(y_mr)\n",
        "\n",
        "eval_model(model_bilstm, X_mr, y_mr, \"BiLSTM — MR-CS Test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZIpmAe4qPpr"
      },
      "outputs": [],
      "source": [
        "X_amg, y_amg = to_seq(amg_cs)\n",
        "y_amg = le.transform(y_amg)\n",
        "\n",
        "eval_model(model_bilstm, X_amg, y_amg, \"BiLSTM — AMG-CS Test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhh84pCr6Mal"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    \"Model\": [\n",
        "        \"Logistic Regression\", \"SVM\",\n",
        "        \"BiLSTM\", \"CNN-LSTM\", \"BiLSTM-Attn\",\n",
        "        \"XLM-R\", \"AraBERTv2\", \"MARBERTv2\"\n",
        "    ],\n",
        "    \"EESA_F1\": [\n",
        "        0.776, 0.774,\n",
        "        0.701, 0.670, 0.725,\n",
        "        0.816, 0.818, 0.883\n",
        "    ],\n",
        "    \"MRCS_F1\": [\n",
        "        0.957, 0.980,\n",
        "        0.671, None, None,\n",
        "        0.810, 0.808, 0.950\n",
        "    ],\n",
        "    \"AMGCS_F1\": [\n",
        "        0.949, 0.980,\n",
        "        0.627, None, None,\n",
        "        0.748, 0.769, 0.933\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuAhkZpH6Uki"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "print(tabulate(df, headers='keys', tablefmt='github'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZnob8Hx6Rvt"
      },
      "outputs": [],
      "source": [
        "df.to_csv(\"pre_augmentation_results.csv\", index=False)\n",
        "print(\"Saved as pre_augmentation_results.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsoOwO3qa7GA"
      },
      "source": [
        "# **post aug deterministic**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWHig7G9bGA9"
      },
      "outputs": [],
      "source": [
        "DET_PATH = \"/content/drive/MyDrive/cs-senti/data/ling/host_train_switched_lex_llm.jsonl\"\n",
        "\n",
        "det_rows = []\n",
        "with open(DET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            det_rows.append(json.loads(line))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "print(\"Loaded deterministic:\", len(det_rows))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-JnxGKOgK0C"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "DET_PATH = \"/content/drive/MyDrive/cs-senti/data/ling/host_train_switched_lex_llm.jsonl\"\n",
        "\n",
        "print(\"=== RAW FILE INSPECTION ===\")\n",
        "with open(DET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for i, line in enumerate(f):\n",
        "        print(f\"\\n--- LINE {i+1} ---\")\n",
        "        print(line)\n",
        "        if i >= 4:  # show only first 5 lines\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XI0zfQIMd3-W"
      },
      "outputs": [],
      "source": [
        "def show_aug_samples(rows, n=10):\n",
        "    print(\"\\n===== SAMPLE AUGMENTED SENTENCES =====\\n\")\n",
        "    rows = random.sample(rows, min(n, len(rows)))\n",
        "\n",
        "    for r in rows:\n",
        "        orig = r.get(\"orig_text\", \"\")\n",
        "        aug  = r.get(\"switched_text\", \"\")\n",
        "        lab  = r.get(\"label\", \"\")\n",
        "\n",
        "        print(\"ORIG :\", orig)\n",
        "        print(\"AUG  :\", aug)\n",
        "        print(\"LABEL:\", lab)\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "\n",
        "show_aug_samples(det_rows)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SctN6OU5hPgZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# -----------------------------\n",
        "# Semantic similarity\n",
        "# -----------------------------\n",
        "def semantic_similarity(a, b, model):\n",
        "    ea = model.encode(a, convert_to_tensor=True)\n",
        "    eb = model.encode(b, convert_to_tensor=True)\n",
        "    return float(F.cosine_similarity(ea, eb, dim=0).item())\n",
        "\n",
        "# -----------------------------\n",
        "# Switch rate based on JSON\n",
        "# -----------------------------\n",
        "def switch_rate(row):\n",
        "    switches = row.get(\"switches\", [])\n",
        "    total_tokens = len(row[\"orig_text\"].split())\n",
        "    return len(switches) / max(1, total_tokens)\n",
        "\n",
        "# -----------------------------\n",
        "# Sentiment consistency check\n",
        "# (using your MARBERT reward model)\n",
        "# -----------------------------\n",
        "idx_map = {\"neg\":0, \"neu\":1, \"pos\":2}\n",
        "\n",
        "def sentiment_match(text, gold_label, clf, tok):\n",
        "    inp = tok(text, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
        "    pred = clf(**inp).logits.argmax(-1).item()\n",
        "    return pred == idx_map[gold_label]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J01ZCPZBmrxi"
      },
      "outputs": [],
      "source": [
        "def sentiment_flip(orig_text, aug_text, gold_label, clf, tok, margin=0.15):\n",
        "    \"\"\"\n",
        "    Returns True ONLY if:\n",
        "    - original prediction matches gold label\n",
        "    - augmented prediction is a different sentiment class\n",
        "    - AND difference is confident enough (margin)\n",
        "    \"\"\"\n",
        "\n",
        "    # Encode both using same model\n",
        "    with torch.no_grad():\n",
        "        o = clf(**tok(orig_text, return_tensors=\"pt\", truncation=True).to(\"cuda\")).logits\n",
        "        a = clf(**tok(aug_text, return_tensors=\"pt\", truncation=True).to(\"cuda\")).logits\n",
        "\n",
        "    o_soft = o.softmax(-1)\n",
        "    a_soft = a.softmax(-1)\n",
        "\n",
        "    orig_pred = o_soft.argmax(-1).item()\n",
        "    aug_pred  = a_soft.argmax(-1).item()\n",
        "    gold_idx  = idx_map[gold_label]\n",
        "\n",
        "    # If model already misclassified original → ignore it\n",
        "    if orig_pred != gold_idx:\n",
        "        return False  # not a flip; model is wrong on original\n",
        "\n",
        "    # If class didn't change → not a flip\n",
        "    if orig_pred == aug_pred:\n",
        "        return False\n",
        "\n",
        "    # Apply confidence margin to avoid false flips\n",
        "    drop = o_soft[0, gold_idx] - a_soft[0, gold_idx]\n",
        "    if drop < margin:\n",
        "        return False  # too weak to be a real flip\n",
        "\n",
        "    return True  # true sentiment flip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYUZ7FldhRC5"
      },
      "outputs": [],
      "source": [
        "def filter_augmented(\n",
        "    rows,\n",
        "    sim_model,\n",
        "    clf,\n",
        "    clf_tok,\n",
        "    min_sim=0.75,\n",
        "    max_switch=0.40,   # your new threshold\n",
        "    margin=0.15\n",
        "):\n",
        "    kept = []\n",
        "    rejected = []\n",
        "\n",
        "    for r in rows:\n",
        "        orig = r[\"orig_text\"]\n",
        "        aug  = r[\"switched_text\"]\n",
        "        lab  = r[\"label\"]\n",
        "\n",
        "        # 1. semantic similarity\n",
        "        sim = semantic_similarity(orig, aug, sim_model)\n",
        "        if sim < min_sim:\n",
        "            r[\"reason\"] = f\"low similarity ({sim:.2f})\"\n",
        "            rejected.append(r)\n",
        "            continue\n",
        "\n",
        "        # 2. switch rate\n",
        "        sr = switch_rate(r)\n",
        "        if sr > max_switch:\n",
        "            r[\"reason\"] = f\"over-switching ({sr:.2f})\"\n",
        "            rejected.append(r)\n",
        "            continue\n",
        "\n",
        "        # 3. sentiment flip (new logic)\n",
        "        if sentiment_flip(orig, aug, lab, clf, clf_tok, margin):\n",
        "            r['reason'] = \"sentiment flip\"\n",
        "            rejected.append(r)\n",
        "            continue\n",
        "\n",
        "        kept.append(r)\n",
        "\n",
        "    return kept, rejected\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b94J0NKthTnU"
      },
      "outputs": [],
      "source": [
        "kept, rejected = filter_augmented(\n",
        "    det_rows,\n",
        "    sim_model=similarity_model,\n",
        "    clf=reward_model,\n",
        "    clf_tok=reward_tokenizer,\n",
        "    min_sim=0.75,\n",
        "    max_switch=0.40\n",
        ")\n",
        "\n",
        "print(\"========== FILTERING SUMMARY ==========\")\n",
        "print(f\"Total samples        : {len(det_rows)}\")\n",
        "print(f\"Kept after filtering : {len(kept)}\")\n",
        "print(f\"Rejected             : {len(rejected)}\")\n",
        "print(f\"Retention rate       : {len(kept)/len(det_rows):.2%}\")\n",
        "print(\"========================================\\n\")\n",
        "\n",
        "# Show examples of kept data\n",
        "print(\"===== KEPT SAMPLES =====\")\n",
        "for r in kept[:5]:\n",
        "    print(\"\\nORIG:\", r[\"orig_text\"])\n",
        "    print(\"AUG :\", r[\"switched_text\"])\n",
        "    print(\"LABEL:\", r[\"label\"])\n",
        "    print(\"----\")\n",
        "\n",
        "# Show examples of rejected data\n",
        "print(\"\\n===== REJECTED SAMPLES =====\")\n",
        "for r in rejected[:5]:\n",
        "    print(\"\\nORIG:\", r[\"orig_text\"])\n",
        "    print(\"AUG :\", r[\"switched_text\"])\n",
        "    print(\"LABEL:\", r[\"label\"])\n",
        "    print(\"REASON:\", r.get(\"reason\", \"\"))\n",
        "    print(\"----\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5iWWbqokZ6R"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Count reasons among rejected rows\n",
        "reason_counts = Counter([r[\"reason\"] for r in rejected])\n",
        "\n",
        "total_rej = sum(reason_counts.values())\n",
        "\n",
        "print(\"===== REJECTION BREAKDOWN =====\")\n",
        "for reason, count in reason_counts.items():\n",
        "    print(f\"{reason:20} : {count:4d} ({count/total_rej*100:.2f}%)\")\n",
        "\n",
        "print(\"\\nTotal rejected:\", total_rej)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wqww2LhEFkE"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "save_path = \"/content/drive/MyDrive/cs-senti/repo/data/filtered_aug.jsonl\"\n",
        "\n",
        "with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for r in kept:\n",
        "        # Save only the transformed sentence + label\n",
        "        f.write(json.dumps({\n",
        "            \"orig_text\": r[\"orig_text\"],\n",
        "            \"switched_text\": r[\"switched_text\"],\n",
        "            \"label\": r[\"label\"],\n",
        "            \"domain\": r.get(\"domain\", \"\")\n",
        "        }, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"Saved filtered augmented dataset:\", len(kept))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CLItUFkEQ4A"
      },
      "outputs": [],
      "source": [
        "reject_path = \"/content/drive/MyDrive/cs-senti/repo/data/rejected_aug.jsonl\"\n",
        "\n",
        "with open(reject_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for r in rejected:\n",
        "        f.write(json.dumps({\n",
        "            \"orig_text\": r[\"orig_text\"],\n",
        "            \"switched_text\": r[\"switched_text\"],\n",
        "            \"label\": r[\"label\"],\n",
        "            \"reason\": r[\"reason\"],\n",
        "            \"domain\": r.get(\"domain\", \"\")\n",
        "        }, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"Saved rejected samples:\", len(rejected))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wO07hYxREa2I"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "AUG_PATH = \"/content/drive/MyDrive/cs-senti/repo/data/filtered_aug.jsonl\"\n",
        "\n",
        "aug_rows = []\n",
        "with open(AUG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        aug_rows.append(json.loads(line))\n",
        "\n",
        "print(\"Loaded augmented samples:\", len(aug_rows))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9tgVC8LEfLW"
      },
      "outputs": [],
      "source": [
        "aug_training_format = []\n",
        "\n",
        "for r in aug_rows:\n",
        "    aug_training_format.append({\n",
        "        \"text\": r[\"switched_text\"],\n",
        "        \"label\": r[\"label\"],\n",
        "        \"domain\": \"augmented\"\n",
        "    })\n",
        "\n",
        "print(\"Formatted augmented samples:\", len(aug_training_format))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ysueq7gFakD"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# load_jsonl again (if missing)\n",
        "def load_jsonl(path):\n",
        "    return [json.loads(l) for l in open(path, \"r\", encoding=\"utf-8\")]\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/cs-senti/repo/data/cleaned\")\n",
        "\n",
        "eesa_train = load_jsonl(BASE/\"eesa_train_clean.jsonl\")\n",
        "eesa_dev   = load_jsonl(BASE/\"eesa_dev_clean.jsonl\")\n",
        "eesa_test  = load_jsonl(BASE/\"eesa_test_clean.jsonl\")\n",
        "\n",
        "mr_cs  = load_jsonl(BASE/\"mr_cs_clean.jsonl\")\n",
        "amg_cs = load_jsonl(BASE/\"amg_cs_clean.jsonl\")\n",
        "\n",
        "train_data = eesa_train + mr_cs + amg_cs\n",
        "dev_data   = eesa_dev\n",
        "test_data  = eesa_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNMQNdhqElg6"
      },
      "outputs": [],
      "source": [
        "post_aug_train = eesa_train + mr_cs + amg_cs + aug_training_format\n",
        "print(\"Original train:\", len(eesa_train + mr_cs + amg_cs))\n",
        "print(\"Post-aug train:\", len(post_aug_train))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r__ewzPaudD_"
      },
      "outputs": [],
      "source": [
        "SAVE_PATH = \"/content/drive/MyDrive/cs-senti/repo/data/post_aug_train.jsonl\"\n",
        "\n",
        "with open(SAVE_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    for row in post_aug_train:\n",
        "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(\"Saved post_aug_train to:\", SAVE_PATH)\n",
        "print(\"Total samples saved:\", len(post_aug_train))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hw_u2ezxLBCB"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "PATH = \"/content/drive/MyDrive/cs-senti/repo/data/post_aug_train.jsonl\"\n",
        "\n",
        "rows = []\n",
        "with open(PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        rows.append(json.loads(line))\n",
        "\n",
        "print(\"✅ Loaded samples:\", len(rows))\n",
        "print(\"\\n🔍 Example row:\")\n",
        "print(rows[0])\n",
        "\n",
        "# Show label distribution if labels exist\n",
        "if \"label\" in rows[0]:\n",
        "    labels = [r[\"label\"] for r in rows]\n",
        "    print(\"\\n📊 Label distribution:\")\n",
        "    print(Counter(labels))\n",
        "else:\n",
        "    print(\"\\n⚠ No 'label' field found in the rows!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lp4dX94XNeFZ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_global_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # Ensures reproducible results for cuDNN (LSTMs, CNNs)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_global_seed(42)\n",
        "print(\"Global SEED set to 42 ✓\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ked7TJpANiWm"
      },
      "outputs": [],
      "source": [
        "from transformers import set_seed\n",
        "set_seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wi23MU0dFDB-"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.shuffle(post_aug_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tNOvgpoFiCO"
      },
      "outputs": [],
      "source": [
        "label2id = {\"neg\": 0, \"neu\": 1, \"pos\": 2}\n",
        "id2label = {v: k for k, v in label2id.items()}\n",
        "\n",
        "def encode_label(example):\n",
        "    example[\"label\"] = label2id[example[\"label\"]]\n",
        "    return example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQ1uKAngFkD3"
      },
      "outputs": [],
      "source": [
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_list(train_data).map(encode_label),\n",
        "    \"dev\":   Dataset.from_list(dev_data).map(encode_label),\n",
        "    \"test\":  Dataset.from_list(test_data).map(encode_label),\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86A6uamLFl1q"
      },
      "outputs": [],
      "source": [
        "post_aug_dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_list(post_aug_train).map(encode_label),\n",
        "    \"dev\":   dataset[\"dev\"],\n",
        "    \"test\":  dataset[\"test\"],\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWmBnAISFzvz"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"UBC-NLP/MARBERTv2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "tokenized_post = post_aug_dataset.map(tokenize, batched=True)\n",
        "\n",
        "# remove unused columns\n",
        "for col in [\"text\", \"id\"]:\n",
        "    if col in tokenized_post[\"train\"].column_names:\n",
        "        tokenized_post = tokenized_post.remove_columns(col)\n",
        "\n",
        "tokenized_post = tokenized_post.rename_column(\"label\", \"labels\")\n",
        "tokenized_post.set_format(\"torch\")\n",
        "\n",
        "print(\"✓ Tokenization complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLJqL_l-F4nm"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    logits, labels = pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    f1 = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")\n",
        "    return {\"macro_f1\": f1[\"f1\"]}\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=3,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/cs-senti/post_aug_marbert\",\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    learning_rate=2e-5,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    report_to=[]\n",
        ")\n",
        "\n",
        "trainer_post = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=tokenized_post[\"train\"],\n",
        "    eval_dataset=tokenized_post[\"dev\"],\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer_post.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4yB9LzcJ2Uh"
      },
      "outputs": [],
      "source": [
        "print(\"\\n=== POST-AUG MARBERT TEST RESULTS ===\")\n",
        "results = trainer_post.evaluate(tokenized_post[\"test\"])\n",
        "print(results)\n",
        "\n",
        "preds = trainer_post.predict(tokenized_post[\"test\"]).predictions\n",
        "pred_labels = np.argmax(preds, axis=1)\n",
        "true_labels = tokenized_post[\"test\"][\"labels\"]\n",
        "\n",
        "print(classification_report(\n",
        "    true_labels,\n",
        "    pred_labels,\n",
        "    target_names=[\"neg\", \"neu\", \"pos\"]\n",
        "))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Vit02XENFnF"
      },
      "outputs": [],
      "source": [
        "mr_cs_data = load_jsonl(BASE/\"mr_cs_clean.jsonl\")\n",
        "amg_cs_data = load_jsonl(BASE/\"amg_cs_clean.jsonl\")\n",
        "mr_dataset = Dataset.from_list(mr_cs_data).map(encode_label)\n",
        "amg_dataset = Dataset.from_list(amg_cs_data).map(encode_label)\n",
        "mr_tok = mr_dataset.map(tokenize, batched=True)\n",
        "amg_tok = amg_dataset.map(tokenize, batched=True)\n",
        "\n",
        "for ds in [mr_tok, amg_tok]:\n",
        "    if \"text\" in ds.column_names:\n",
        "        ds = ds.remove_columns(\"text\")\n",
        "\n",
        "mr_tok = mr_tok.rename_column(\"label\", \"labels\")\n",
        "amg_tok = amg_tok.rename_column(\"label\", \"labels\")\n",
        "\n",
        "mr_tok.set_format(\"torch\")\n",
        "amg_tok.set_format(\"torch\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEEQob4oNLd-"
      },
      "outputs": [],
      "source": [
        "print(\"\\n=== POST-AUG → MR-CS GENERALIZATION ===\")\n",
        "mr_results = trainer_post.evaluate(mr_tok)\n",
        "print(mr_results)\n",
        "\n",
        "mr_preds = trainer_post.predict(mr_tok).predictions\n",
        "mr_pred_labels = np.argmax(mr_preds, axis=1)\n",
        "mr_true_labels = mr_tok[\"labels\"]\n",
        "\n",
        "print(classification_report(\n",
        "    mr_true_labels,\n",
        "    mr_pred_labels,\n",
        "    target_names=[\"neg\", \"neu\", \"pos\"]\n",
        "))\n",
        "\n",
        "\n",
        "print(\"\\n=== POST-AUG → AMG-CS GENERALIZATION ===\")\n",
        "amg_results = trainer_post.evaluate(amg_tok)\n",
        "print(amg_results)\n",
        "\n",
        "amg_preds = trainer_post.predict(amg_tok).predictions\n",
        "amg_pred_labels = np.argmax(amg_preds, axis=1)\n",
        "amg_true_labels = amg_tok[\"labels\"]\n",
        "\n",
        "print(classification_report(\n",
        "    amg_true_labels,\n",
        "    amg_pred_labels,\n",
        "    target_names=[\"neg\", \"neu\", \"pos\"]\n",
        "))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMzPi-OqQaJc"
      },
      "outputs": [],
      "source": [
        "def train_transformer_post(model_name, output_dir):\n",
        "    print(f\"\\n=== Training POST-AUG {model_name} ===\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "\n",
        "    def tokenize(batch):\n",
        "        return tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "    tokenized = post_aug_dataset.map(tokenize, batched=True)\n",
        "    tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
        "    tokenized = tokenized.remove_columns([\"text\", \"id\"]) if \"id\" in tokenized[\"train\"].column_names else tokenized\n",
        "    tokenized.set_format(\"torch\")\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=3,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    )\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=4,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        learning_rate=2e-5,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_strategy=\"epoch\",\n",
        "        report_to=[]\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=args,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=tokenized[\"train\"],\n",
        "        eval_dataset=tokenized[\"dev\"],\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate on EESA test\n",
        "    eval_result = trainer.evaluate(tokenized[\"test\"])\n",
        "    print(f\"\\n=== POST-AUG → EESA TEST ({model_name}) ===\")\n",
        "    print(eval_result)\n",
        "\n",
        "    # Full report\n",
        "    preds = trainer.predict(tokenized[\"test\"]).predictions.argmax(axis=1)\n",
        "    print(classification_report(tokenized[\"test\"][\"labels\"], preds, target_names=[\"neg\",\"neu\",\"pos\"]))\n",
        "\n",
        "    return trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WwR0j7_QeQH"
      },
      "outputs": [],
      "source": [
        "trainer_xlmr = train_transformer_post(\"xlm-roberta-base\", \"/content/drive/MyDrive/cs-senti/xlmr_post_aug\")\n",
        "trainer_arabert = train_transformer_post(\"aubmindlab/bert-base-arabertv2\", \"/content/drive/MyDrive/cs-senti/arabert_post_aug\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3eKyx508arlU"
      },
      "outputs": [],
      "source": [
        "label2id = {\"neg\": 0, \"neu\": 1, \"pos\": 2}\n",
        "\n",
        "# POST-AUG TRAIN SET\n",
        "train_texts  = [x[\"text\"] for x in post_aug_train]\n",
        "train_labels = [label2id[x[\"label\"]] for x in post_aug_train]\n",
        "\n",
        "# ORIGINAL TEST SET (EESA)\n",
        "eesa_test_dataset = dataset[\"test\"]\n",
        "\n",
        "test_texts  = [x[\"text\"] for x in eesa_test_dataset]\n",
        "test_labels = [x[\"label\"] for x in eesa_test_dataset]\n",
        "\n",
        "print(\"Train label set:\", set(train_labels))\n",
        "print(\"Test label set:\", set(test_labels))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qlwlyH_sD1T"
      },
      "source": [
        "classical classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDdZ0mqCa_OD"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# --------------------------\n",
        "# TRAIN SET (post-augmented)\n",
        "# --------------------------\n",
        "train_texts  = [x[\"text\"] for x in post_aug_train]\n",
        "train_labels = [label2id[x[\"label\"]] for x in post_aug_train]\n",
        "\n",
        "# --------------------------\n",
        "# TEST SET (EESA test)\n",
        "# --------------------------\n",
        "eesa_test_dataset = dataset[\"test\"]\n",
        "test_texts  = [x[\"text\"] for x in eesa_test_dataset]\n",
        "test_labels = [x[\"label\"] for x in eesa_test_dataset]   # already numeric\n",
        "\n",
        "# --------------------------\n",
        "# TF-IDF\n",
        "# --------------------------\n",
        "tfidf = TfidfVectorizer(max_features=30000, ngram_range=(1,2))\n",
        "X_train = tfidf.fit_transform(train_texts)\n",
        "X_test  = tfidf.transform(test_texts)\n",
        "\n",
        "# --------------------------\n",
        "# Logistic Regression\n",
        "# --------------------------\n",
        "logreg = LogisticRegression(max_iter=4000)\n",
        "logreg.fit(X_train, train_labels)\n",
        "logreg_preds = logreg.predict(X_test)\n",
        "\n",
        "print(\"\\n=== POST-AUG Logistic Regression (EESA Test) ===\")\n",
        "print(classification_report(test_labels, logreg_preds, target_names=[\"neg\",\"neu\",\"pos\"]))\n",
        "\n",
        "# --------------------------\n",
        "# Linear SVM\n",
        "# --------------------------\n",
        "svm = LinearSVC()\n",
        "svm.fit(X_train, train_labels)\n",
        "svm_preds = svm.predict(X_test)\n",
        "\n",
        "print(\"\\n=== POST-AUG SVM (EESA Test) ===\")\n",
        "print(classification_report(test_labels, svm_preds, target_names=[\"neg\",\"neu\",\"pos\"]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS4NkSGe6BhA"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "\n",
        "# --------------------------\n",
        "# TRAIN SET (post-augmented)\n",
        "# --------------------------\n",
        "train_texts  = [x[\"text\"] for x in post_aug_train]\n",
        "train_labels = [x[\"label\"] for x in post_aug_train]\n",
        "\n",
        "# --------------------------\n",
        "# TEST SET (EESA test)\n",
        "# --------------------------\n",
        "eesa_test_dataset = dataset[\"test\"]\n",
        "test_texts  = [x[\"text\"] for x in eesa_test_dataset]\n",
        "test_labels = [x[\"label\"] for x in eesa_test_dataset]\n",
        "\n",
        "# --------------------------\n",
        "# TF-IDF\n",
        "# --------------------------\n",
        "tfidf = TfidfVectorizer(max_features=30000, ngram_range=(1,2))\n",
        "X_train = tfidf.fit_transform(train_texts)\n",
        "X_test  = tfidf.transform(test_texts)\n",
        "\n",
        "# --------------------------\n",
        "# Logistic Regression\n",
        "# --------------------------\n",
        "logreg = LogisticRegression(max_iter=4000)\n",
        "logreg.fit(X_train, train_labels)\n",
        "logreg_preds = logreg.predict(X_test)\n",
        "\n",
        "print(\"\\n=== POST-AUG Logistic Regression (EESA Test) ===\")\n",
        "print(classification_report(test_labels, logreg_preds, target_names=[\"neg\",\"neu\",\"pos\"]))\n",
        "eesa_f1_logreg = f1_score(test_labels, logreg_preds, average=\"macro\")\n",
        "print(\"Macro F1:\", eesa_f1_logreg)\n",
        "\n",
        "# --------------------------\n",
        "# Linear SVM\n",
        "# --------------------------\n",
        "svm = LinearSVC()\n",
        "svm.fit(X_train, train_labels)\n",
        "svm_preds = svm.predict(X_test)\n",
        "\n",
        "print(\"\\n=== POST-AUG SVM (EESA Test) ===\")\n",
        "print(classification_report(test_labels, svm_preds, target_names=[\"neg\",\"neu\",\"pos\"]))\n",
        "eesa_f1_svm = f1_score(test_labels, svm_preds, average=\"macro\")\n",
        "print(\"Macro F1:\", eesa_f1_svm)\n",
        "\n",
        "\n",
        "# ====================================================\n",
        "#        GENERALIZATION TESTS (MR-CS and AMG-CS)\n",
        "# ====================================================\n",
        "\n",
        "# --------------------------\n",
        "# MR-CS\n",
        "# --------------------------\n",
        "mr_texts  = [x[\"text\"] for x in mr_cs]\n",
        "mr_labels = [[x[\"label\"]] for x in mr_cs]\n",
        "\n",
        "X_mr = tfidf.transform(mr_texts)\n",
        "\n",
        "mr_logreg_preds = logreg.predict(X_mr)\n",
        "mr_svm_preds    = svm.predict(X_mr)\n",
        "\n",
        "print(\"\\n=== POST-AUG Logistic Regression → MR-CS ===\")\n",
        "print(classification_report(mr_labels, mr_logreg_preds, target_names=[\"neg\",\"neu\",\"pos\"]))\n",
        "mr_f1_logreg = f1_score(mr_labels, mr_logreg_preds, average=\"macro\")\n",
        "print(\"Macro F1:\", mr_f1_logreg)\n",
        "\n",
        "print(\"\\n=== POST-AUG SVM → MR-CS ===\")\n",
        "print(classification_report(mr_labels, mr_svm_preds, target_names=[\"neg\",\"neu\",\"pos\"]))\n",
        "mr_f1_svm = f1_score(mr_labels, mr_svm_preds, average=\"macro\")\n",
        "print(\"Macro F1:\", mr_f1_svm)\n",
        "\n",
        "\n",
        "# --------------------------\n",
        "# AMG-CS\n",
        "# --------------------------\n",
        "amg_texts  = [x[\"text\"] for x in amg_cs]\n",
        "amg_labels = [[x[\"label\"]] for x in amg_cs]\n",
        "\n",
        "X_amg = tfidf.transform(amg_texts)\n",
        "\n",
        "amg_logreg_preds = logreg.predict(X_amg)\n",
        "amg_svm_preds    = svm.predict(X_amg)\n",
        "\n",
        "print(\"\\n=== POST-AUG Logistic Regression → AMG-CS ===\")\n",
        "print(classification_report(amg_labels, amg_logreg_preds, target_names=[\"neg\",\"neu\",\"pos\"]))\n",
        "amg_f1_logreg = f1_score(amg_labels, amg_logreg_preds, average=\"macro\")\n",
        "print(\"Macro F1:\", amg_f1_logreg)\n",
        "\n",
        "print(\"\\n=== POST-AUG SVM → AMG-CS ===\")\n",
        "print(classification_report(amg_labels, amg_svm_preds, target_names=[\"neg\",\"neu\",\"pos\"]))\n",
        "amg_f1_svm = f1_score(amg_labels, amg_svm_preds, average=\"macro\")\n",
        "print(\"Macro F1:\", amg_f1_svm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc7mbMhnsIu4"
      },
      "source": [
        "# neural classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4zX5938sLLn"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision transformers -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJoYGz8isuv9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.metrics import f1_score, classification_report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIg_fZNc0G3N"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/cs-senti/repo/data/cleaned\")\n",
        "\n",
        "def load_jsonl(path):\n",
        "    return [json.loads(l) for l in open(path, \"r\", encoding=\"utf-8\")]\n",
        "\n",
        "# Load original datasets\n",
        "eesa_train = load_jsonl(BASE/\"eesa_train_clean.jsonl\")\n",
        "eesa_dev   = load_jsonl(BASE/\"eesa_dev_clean.jsonl\")\n",
        "eesa_test  = load_jsonl(BASE/\"eesa_test_clean.jsonl\")\n",
        "\n",
        "mr_cs  = load_jsonl(BASE/\"mr_cs_clean.jsonl\")\n",
        "amg_cs = load_jsonl(BASE/\"amg_cs_clean.jsonl\")\n",
        "\n",
        "print(\n",
        "    len(eesa_train), len(eesa_dev), len(eesa_test),\n",
        "    len(mr_cs), len(amg_cs)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBBvBi0G0KGd"
      },
      "outputs": [],
      "source": [
        "AUG_PATH = \"/content/drive/MyDrive/cs-senti/repo/data/filtered_aug.jsonl\"\n",
        "\n",
        "aug_rows = [json.loads(l) for l in open(AUG_PATH, \"r\", encoding=\"utf-8\")]\n",
        "aug_training_format = [\n",
        "    {\"text\": r[\"switched_text\"], \"label\": r[\"label\"], \"domain\": \"augmented\"}\n",
        "    for r in aug_rows\n",
        "]\n",
        "print(\"Aug samples:\", len(aug_training_format))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JBckXa10OL5"
      },
      "outputs": [],
      "source": [
        "label2id = {\"neg\": 0, \"neu\": 1, \"pos\": 2}\n",
        "\n",
        "def encode_label(row):\n",
        "    row[\"label\"] = label2id[row[\"label\"]]\n",
        "    return row\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd9XrqHc0QQ-"
      },
      "outputs": [],
      "source": [
        "eesa_train = [encode_label(x) for x in eesa_train]\n",
        "eesa_dev   = [encode_label(x) for x in eesa_dev]\n",
        "eesa_test  = [encode_label(x) for x in eesa_test]\n",
        "\n",
        "mr_cs = [encode_label(x) for x in mr_cs]\n",
        "amg_cs = [encode_label(x) for x in amg_cs]\n",
        "aug_training_format = [encode_label(x) for x in aug_training_format]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5V3woX30S0z"
      },
      "outputs": [],
      "source": [
        "post_aug_train = eesa_train + mr_cs + amg_cs + aug_training_format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZvP2HhW0V19"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "MAX_LEN = 128\n",
        "\n",
        "def encode_batch(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_LEN\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49-Fpe6v0Z0S"
      },
      "outputs": [],
      "source": [
        "post_aug_hf = Dataset.from_list(post_aug_train)\n",
        "eesa_test_hf = Dataset.from_list(eesa_test)\n",
        "mr_hf = Dataset.from_list(mr_cs)\n",
        "amg_hf = Dataset.from_list(amg_cs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wK1ZRDxd0btR"
      },
      "outputs": [],
      "source": [
        "tokenized_train = post_aug_hf.map(\n",
        "    encode_batch,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\", \"label\"]\n",
        ")\n",
        "\n",
        "tokenized_test = eesa_test_hf.map(\n",
        "    encode_batch,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\", \"label\"]\n",
        ")\n",
        "\n",
        "tokenized_mr = mr_hf.map(\n",
        "    encode_batch,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\", \"label\"]\n",
        ")\n",
        "\n",
        "tokenized_amg = amg_hf.map(\n",
        "    encode_batch,\n",
        "    batched=True,\n",
        "    remove_columns=[\"text\", \"label\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_l6D9IE0s0P"
      },
      "outputs": [],
      "source": [
        "tokenized_train = tokenized_train.add_column(\"labels\", [x[\"label\"] for x in post_aug_train])\n",
        "tokenized_test  = tokenized_test.add_column(\"labels\",  [x[\"label\"] for x in eesa_test])\n",
        "tokenized_mr    = tokenized_mr.add_column(\"labels\",    [x[\"label\"] for x in mr_cs])\n",
        "tokenized_amg   = tokenized_amg.add_column(\"labels\",   [x[\"label\"] for x in amg_cs])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGUTkJAc0vJu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class HFTextDataset(Dataset):\n",
        "    def __init__(self, ds):\n",
        "        self.ds = ds\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.ds[idx]\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(item[\"input_ids\"], dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(item[\"attention_mask\"], dtype=torch.long),\n",
        "            \"labels\": torch.tensor(item[\"labels\"], dtype=torch.long),\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCFoWYo90w33"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(HFTextDataset(tokenized_train), batch_size=32, shuffle=True)\n",
        "test_loader  = DataLoader(HFTextDataset(tokenized_test),  batch_size=32)\n",
        "mr_loader    = DataLoader(HFTextDataset(tokenized_mr),    batch_size=32)\n",
        "amg_loader   = DataLoader(HFTextDataset(tokenized_amg),   batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mz9tdk8-09xQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BiLSTM_MHAttn(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=128, lstm_dim=128, num_heads=4, num_classes=3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            embed_dim,\n",
        "            lstm_dim,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "\n",
        "        # Multi-Head Self-Attention\n",
        "        self.attn = nn.MultiheadAttention(\n",
        "            embed_dim=2*lstm_dim,\n",
        "            num_heads=num_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc = nn.Linear(2*lstm_dim, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        x = self.embedding(input_ids)\n",
        "\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Multi-head self-attention\n",
        "        attn_out, _ = self.attn(lstm_out, lstm_out, lstm_out,\n",
        "                                key_padding_mask=(attention_mask == 0))\n",
        "\n",
        "        # mean pooling\n",
        "        pooled = attn_out.mean(dim=1)\n",
        "\n",
        "        logits = self.fc(self.dropout(pooled))\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8hsHxQt1AF4"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class HFWrapper(Dataset):\n",
        "    def __init__(self, hf_ds):\n",
        "        self.ds = hf_ds\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.ds[idx]\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(item[\"input_ids\"], dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(item[\"attention_mask\"], dtype=torch.long),\n",
        "            \"labels\": torch.tensor(item[\"labels\"], dtype=torch.long)\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Ggai2Uu1CPx"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "VOCAB_SIZE = tokenizer.vocab_size\n",
        "model = BiLSTM_MHAttn(VOCAB_SIZE).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJxnTQdn1e1t"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    return {\n",
        "        \"input_ids\": torch.stack([x[\"input_ids\"] for x in batch]),\n",
        "        \"attention_mask\": torch.stack([x[\"attention_mask\"] for x in batch]),\n",
        "        \"labels\": torch.stack([x[\"labels\"] for x in batch])\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lz4QyvUg1Dxj"
      },
      "outputs": [],
      "source": [
        "train_ds = HFWrapper(tokenized_train)\n",
        "test_ds  = HFWrapper(tokenized_test)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader  = DataLoader(test_ds, batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXvUaDxf1FW4"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 6\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        ids = batch[\"input_ids\"].to(device)\n",
        "        mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(ids, mask)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss = {total_loss/len(train_loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-P69BRR1m3f"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_loader(model, loader):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    trues = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            ids = batch[\"input_ids\"].to(device)\n",
        "            mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].cpu().numpy()\n",
        "\n",
        "            logits = model(ids, mask)\n",
        "            pred = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "            preds.extend(pred)\n",
        "            trues.extend(labels)\n",
        "\n",
        "    print(classification_report(trues, preds, target_names=[\"neg\",\"neu\",\"pos\"]))\n",
        "    macro_f1 = f1_score(trues, preds, average=\"macro\")\n",
        "    print(\"Macro F1:\", macro_f1)\n",
        "\n",
        "    return macro_f1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uON9SLwv1osf"
      },
      "outputs": [],
      "source": [
        "print(\"\\n=== BiLSTM + Multi-Head Self-Attention (EESA Test) ===\")\n",
        "evaluate_loader(model, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaKgQXeE1p9p"
      },
      "outputs": [],
      "source": [
        "mr_loader  = DataLoader(HFWrapper(tokenized_mr), batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "amg_loader = DataLoader(HFWrapper(tokenized_amg), batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(\"\\n=== → MR-CS GENERALIZATION ===\")\n",
        "evaluate_loader(model, mr_loader)\n",
        "\n",
        "print(\"\\n=== → AMG-CS GENERALIZATION ===\")\n",
        "evaluate_loader(model, amg_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaODMCY-xkwk"
      },
      "source": [
        "summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdW_hK2B2gz7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "results = {\n",
        "    \"Model\": [\n",
        "        \"Logistic Regression\",\n",
        "        \"SVM\",\n",
        "        \"BiLSTM-Attention\",\n",
        "        \"XLM-R Base\",\n",
        "        \"AraBERTv2\",\n",
        "        \"MARBERTv2\"\n",
        "    ],\n",
        "\n",
        "    # ------------------------------\n",
        "    # EESA Test (in-domain performance)\n",
        "    # ------------------------------\n",
        "    \"EESA_F1\": [\n",
        "        0.776,   # logistic (pre-aug baseline)\n",
        "        0.778,   # svm (pre-aug baseline)\n",
        "        0.727,   # BiLSTM-Attn\n",
        "        0.840,   # xlm-r\n",
        "        0.835,   # arabert\n",
        "        0.886    # marbert\n",
        "    ],\n",
        "\n",
        "    # ------------------------------\n",
        "    # MR-CS Generalization (post-augmentation)\n",
        "    # ------------------------------\n",
        "    \"MRCS_F1\": [\n",
        "        0.926,     # Logistic Regression POST-AUG\n",
        "        0.993,     # SVM POST-AUG\n",
        "        0.937,     # BiLSTM-Attn\n",
        "        None,      # xlm-r not computed\n",
        "        \"ERROR\",   # arabert crashed\n",
        "        0.888      # marbert\n",
        "    ],\n",
        "\n",
        "    # ------------------------------\n",
        "    # AMG-CS Generalization (post-augmentation)\n",
        "    # ------------------------------\n",
        "    \"AMGCS_F1\": [\n",
        "        0.902,     # Logistic Regression POST-AUG\n",
        "        0.978,     # SVM POST-AUG\n",
        "        0.941,     # BiLSTM-Attn\n",
        "        None,      # xlm-r not computed\n",
        "        \"ERROR\",   # arabert\n",
        "        0.824      # marbert\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDOAzefrUz7j"
      },
      "source": [
        "|    | Model               |   EESA_F1 |   MRCS_F1 |   AMGCS_F1 |\n",
        "|----|---------------------|-----------|-----------|------------|\n",
        "|  0 | Logistic Regression |     0.776 |     0.957 |      0.949 |\n",
        "|  1 | SVM                 |     0.774 |     0.98  |      0.98  |\n",
        "|  2 | BiLSTM              |     0.701 |     0.671 |      0.627 |\n",
        "|  3 | CNN-LSTM            |     0.67  |   nan     |    nan     |\n",
        "|  4 | BiLSTM-Attn         |     0.725 |   nan     |    nan     |\n",
        "|  5 | XLM-R               |     0.816 |     0.81  |      0.748 |\n",
        "|  6 | AraBERTv2           |     0.818 |     0.808 |      0.769 |\n",
        "|  7 | MARBERTv2           |     0.883 |     0.95  |      0.933 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbPHckzMqX_N"
      },
      "source": [
        "## **Transformer Baselines**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0VF3OP6CPee"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/cs-senti/repo/data/cleaned\")\n",
        "\n",
        "def load_jsonl(path):\n",
        "    return [json.loads(l) for l in open(path, \"r\", encoding=\"utf-8\")]\n",
        "\n",
        "eesa_train = load_jsonl(BASE/\"eesa_train_clean_fixed.jsonl\")\n",
        "eesa_dev   = load_jsonl(BASE/\"eesa_dev_clean_fixed.jsonl\")\n",
        "eesa_test  = load_jsonl(BASE/\"eesa_test_clean_fixed.jsonl\")\n",
        "mr_cs      = load_jsonl(BASE/\"mr_cs_clean_fixed.jsonl\")\n",
        "amg_cs     = load_jsonl(BASE/\"amg_cs_clean_fixed.jsonl\")\n",
        "\n",
        "# Unified train split\n",
        "final_train = eesa_train + mr_cs + amg_cs\n",
        "final_dev   = eesa_dev\n",
        "\n",
        "# For reporting\n",
        "final_test_eesa = eesa_test\n",
        "final_test_mr   = mr_cs\n",
        "final_test_amg  = amg_cs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzBN0v95CTaF"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_ds = Dataset.from_list(final_train)\n",
        "dev_ds   = Dataset.from_list(final_dev)\n",
        "\n",
        "test_eesa_ds = Dataset.from_list(final_test_eesa)\n",
        "test_mr_ds   = Dataset.from_list(final_test_mr)\n",
        "test_amg_ds  = Dataset.from_list(final_test_amg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lp14fOsF_o1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "def check_labels(ds, name):\n",
        "    print(f\"\\n=== Checking {name} ===\")\n",
        "    bad = []\n",
        "    for i, row in enumerate(ds):\n",
        "        if not isinstance(row[\"label\"], str):\n",
        "            bad.append((i, row[\"label\"]))\n",
        "    print(\"Total samples:\", len(ds))\n",
        "    print(\"Bad labels:\", len(bad))\n",
        "    if bad:\n",
        "        print(\"Examples:\")\n",
        "        print(bad[:5])\n",
        "\n",
        "check_labels(final_train, \"TRAIN\")\n",
        "check_labels(final_dev,   \"DEV\")\n",
        "check_labels(final_test_eesa, \"TEST EESA\")\n",
        "check_labels(final_test_amg,  \"TEST AMG\")\n",
        "check_labels(final_test_mr,   \"TEST MR\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "By9giUeVCVMB"
      },
      "outputs": [],
      "source": [
        "label2id = {\"neg\":0, \"neu\":1, \"pos\":2}\n",
        "id2label = {v:k for k,v in label2id.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSzeoyunCXCt"
      },
      "outputs": [],
      "source": [
        "def make_tokenize_fn(tokenizer):\n",
        "    def tokenize(batch):\n",
        "        enc = tokenizer(\n",
        "            batch[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=128\n",
        "        )\n",
        "        enc[\"labels\"] = [label2id[l] for l in batch[\"label\"]]\n",
        "        return enc\n",
        "    return tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-KG-HilCaJw"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "def make_args(output_dir):\n",
        "    return TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=3,\n",
        "        logging_steps=50,\n",
        "\n",
        "        save_steps=200,\n",
        "        eval_steps=200,\n",
        "        save_total_limit=2,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"macro_f1\",\n",
        "        greater_is_better=True,\n",
        "\n",
        "        report_to=\"none\",   # disable wandb\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyf9qK6gCb4p"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate\n",
        "import evaluate\n",
        "\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = logits.argmax(-1)\n",
        "    return {\n",
        "        \"macro_f1\": f1_metric.compute(\n",
        "            predictions=preds,\n",
        "            references=labels,\n",
        "            average=\"macro\"\n",
        "        )[\"f1\"]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_deYUa6hC8Tc"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "\n",
        "# -------------------------\n",
        "# LABEL MAP\n",
        "# -------------------------\n",
        "label2id = {\"neg\":0, \"neu\":1, \"pos\":2}\n",
        "id2label = {v:k for k,v in label2id.items()}\n",
        "\n",
        "# -------------------------\n",
        "# METRIC\n",
        "# -------------------------\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = logits.argmax(-1)\n",
        "    return {\n",
        "        \"f1\": f1_metric.compute(\n",
        "            predictions=preds,\n",
        "            references=labels,\n",
        "            average=\"macro\"\n",
        "        )[\"f1\"]\n",
        "    }\n",
        "\n",
        "# -------------------------\n",
        "# TOKENIZATION FN\n",
        "# -------------------------\n",
        "def make_tokenize_fn(tokenizer):\n",
        "    def tokenize(batch):\n",
        "        enc = tokenizer(\n",
        "            batch[\"text\"],\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "        )\n",
        "        enc[\"labels\"] = [label2id[l] for l in batch[\"label\"]]\n",
        "        return enc\n",
        "    return tokenize\n",
        "\n",
        "# -------------------------\n",
        "# MAIN TRAINER\n",
        "# -------------------------\n",
        "def train_transformer(model_name, output_dir):\n",
        "\n",
        "    print(f\"\\n============== Training {model_name} ==============\\n\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    tokenize_fn = make_tokenize_fn(tokenizer)\n",
        "\n",
        "    # BUILD DATASETS\n",
        "    train_ds = Dataset.from_list(final_train).map(tokenize_fn, batched=True)\n",
        "    dev_ds   = Dataset.from_list(final_dev).map(tokenize_fn, batched=True)\n",
        "    test_ds  = Dataset.from_list(final_test_eesa).map(tokenize_fn, batched=True)\n",
        "\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=3,\n",
        "        id2label=id2label,\n",
        "        label2id=label2id\n",
        "    )\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=3,\n",
        "\n",
        "        # 4.57.2-friendly params\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "\n",
        "        learning_rate=2e-5,\n",
        "        weight_decay=0.01,\n",
        "\n",
        "        logging_steps=100,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        args=args,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=dev_ds,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    return trainer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obNszvMxfJRJ"
      },
      "source": [
        "## **reward module**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCaI7CIhcaBc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "def load_jsonl(path):\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            rows.append(json.loads(line))\n",
        "    return Dataset.from_list(rows)\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/cs-senti/repo/data/cleaned\"\n",
        "\n",
        "eesa_train = load_jsonl(f\"{BASE}/eesa_train_clean.jsonl\")\n",
        "eesa_dev   = load_jsonl(f\"{BASE}/eesa_dev_clean.jsonl\")\n",
        "\n",
        "mr_cs = load_jsonl(f\"{BASE}/mr_cs_clean.jsonl\")\n",
        "amg_cs = load_jsonl(f\"{BASE}/amg_cs_clean.jsonl\")\n",
        "\n",
        "print(eesa_train, eesa_dev, mr_cs, amg_cs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKtDx0ied9Qd"
      },
      "outputs": [],
      "source": [
        "train_data = Dataset.from_list(\n",
        "    eesa_train.to_list() + mr_cs.to_list() + amg_cs.to_list()\n",
        ")\n",
        "\n",
        "reward_dataset = DatasetDict({\n",
        "    \"train\": train_data,\n",
        "    \"validation\": eesa_dev\n",
        "})\n",
        "\n",
        "reward_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpHFeRZkd_QT"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "reward_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "def tokenize_fn(batch):\n",
        "    return reward_tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "tokenized_reward = reward_dataset.map(tokenize_fn, batched=True)\n",
        "\n",
        "label2id = {\"neg\": 0, \"neu\": 1, \"pos\": 2}\n",
        "id2label = {v:k for k,v in label2id.items()}\n",
        "\n",
        "tokenized_reward = tokenized_reward.map(\n",
        "    lambda x: {\"labels\": label2id[x[\"label\"]]}\n",
        ")\n",
        "\n",
        "tokenized_reward = tokenized_reward.remove_columns([\"text\", \"label\"])\n",
        "tokenized_reward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2NpH6QQeLS4"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"xlm-roberta-base\",\n",
        "    num_labels=3,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwqbFAi6eM_e"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/cs-senti/reward_xlmr_v1\",\n",
        "\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    gradient_accumulation_steps=1,\n",
        "\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "\n",
        "    # MUST MATCH!\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"macro_f1\",\n",
        "    greater_is_better=True,\n",
        "\n",
        "    logging_dir=\"/content/drive/MyDrive/cs-senti/logs_reward\",\n",
        "    logging_steps=50,\n",
        "\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\"   # disable wandb\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FoakGK4gQXM"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "narY5GurgR-a"
      },
      "outputs": [],
      "source": [
        "from transformers.training_args import TrainingArguments\n",
        "help(TrainingArguments)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}